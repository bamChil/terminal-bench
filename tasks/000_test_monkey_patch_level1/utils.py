import json
import os
import tempfile
import shutil
import pandas as pd
from typing import Dict, Any, Optional
from contextlib import nullcontext
from openhands.events.action import CmdRunAction
from openhands.events.action.files import FileReadAction
from openhands.events.action.commands import CmdRunAction

try:
    from filelock import FileLock  # å¯é€‰
except Exception:
    FileLock = None

def upsert_instance_status_jsonl(
    dir_path: str,
    record: Dict[str, Any],
    filename: str = "instance_status.json",
    id_field: str = "instance_id",
    use_lock: bool = True,
) -> str:
    assert id_field in record and record[id_field] is not None, f"record å¿…é¡»åŒ…å«æœ‰æ•ˆçš„ {id_field}"
    file_path = os.path.join(dir_path, filename)
    os.makedirs(dir_path, exist_ok=True)

    lock: Optional[Any] = FileLock(file_path + ".lock") if (use_lock and FileLock is not None) else None
    ctx = lock or nullcontext()

    def _write_json_line(fp, obj):
        fp.write(json.dumps(obj, ensure_ascii=False))
        fp.write("\n")

    with ctx:
        replaced = False
        # å…³é”®ä¿®æ”¹ï¼šåœ¨ **åŒç›®å½•** ä¸‹å»ºä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…è·¨è®¾å¤‡é‡å‘½å
        with tempfile.NamedTemporaryFile(
            "w", delete=False, encoding="utf-8", dir=dir_path  # <â€”â€” è¿™é‡ŒæŒ‡å®š dir
        ) as tmp:
            tmp_path = tmp.name
            if os.path.exists(file_path):
                with open(file_path, "r", encoding="utf-8") as src:
                    for line in src:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            obj = json.loads(line)
                        except json.JSONDecodeError:
                            tmp.write(line + "\n")  # ä¿ç•™æ— æ³•è§£æçš„è¡Œ
                            continue
                        if isinstance(obj, dict) and obj.get(id_field) == record[id_field]:
                            _write_json_line(tmp, record)  # æ›¿æ¢
                            replaced = True
                        else:
                            _write_json_line(tmp, obj)
            if not replaced:
                _write_json_line(tmp, record)

        # åŸå­æ›¿æ¢ï¼ˆåŒä¸€æ–‡ä»¶ç³»ç»Ÿå†…ï¼‰
        try:
            os.replace(tmp_path, file_path)
        except OSError:
            # æç«¯æƒ…å†µä¸‹çš„å…œåº•ï¼ˆä¸ä¿è¯åŸå­æ€§ï¼Œä½†é¿å…å¤±è´¥ï¼‰
            shutil.move(tmp_path, file_path)

    return file_path

def extract_instance_metrics(structured_logs_dir: str, instance_id: str, logger=None) -> dict:
    """
    ä»ç»“æ„åŒ–æ—¥å¿—ä¸­æå–å®ä¾‹çš„ tokenã€æˆæœ¬å’Œè¿è¡Œæ—¶é—´ä¿¡æ¯ã€‚
    """
    log_file = os.path.join(structured_logs_dir, f"{instance_id}.jsonl")

    if not os.path.exists(log_file):
        return {
            'total_tokens': 0,
            'total_cost': 0.0,
            'runtime_seconds': 0.0
        }

    # ä»æœ€åä¸€ä¸ªè®°å½•è·å–ä¿¡æ¯
    accumulated_token_usage = None
    accumulated_cost = None
    start_time = None
    end_time = None

    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # æŸ¥æ‰¾æœ€åçš„accumulated_costï¼ˆç³»ç»Ÿè®°å½•çš„çœŸå®æˆæœ¬ï¼‰
        import re
        cost_matches = re.findall(r'"accumulated_cost":\s*([\d.]+)', content)
        if cost_matches:
            accumulated_cost = float(cost_matches[-1])

        # æŸ¥æ‰¾æ‰€æœ‰çš„accumulated_token_usage
        token_usage_matches = re.findall(r'"accumulated_token_usage":\s*{[^}]*}', content)
        if token_usage_matches:
            # è·å–æœ€åä¸€ä¸ªtoken usageä¿¡æ¯
            last_usage_str = token_usage_matches[-1]
            # æå–å®Œæ•´çš„JSONå¯¹è±¡
            token_usage_json = '{' + last_usage_str + '}'
            try:
                token_data = json.loads(token_usage_json)
                accumulated_token_usage = token_data.get('accumulated_token_usage')
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
                prompt_match = re.search(r'"prompt_tokens":\s*(\d+)', last_usage_str)
                completion_match = re.search(r'"completion_tokens":\s*(\d+)', last_usage_str)
                if prompt_match and completion_match:
                    accumulated_token_usage = {
                        'prompt_tokens': int(prompt_match.group(1)),
                        'completion_tokens': int(completion_match.group(1))
                    }

        # æŸ¥æ‰¾å¼€å§‹å’Œç»“æŸæ—¶é—´æˆ³
        time_matches = re.findall(r'"timestamp":\s*"([^"]+)"', content)
        if time_matches:
            start_time = time_matches[0]
            end_time = time_matches[-1]

    except Exception as e:
        logger.warning(f"Error parsing structured log for {instance_id}: {e}")

    # è®¡ç®—æ€» token æ•°
    total_tokens = 0
    if accumulated_token_usage:
        total_tokens = (
            accumulated_token_usage.get('prompt_tokens', 0) +
            accumulated_token_usage.get('completion_tokens', 0)
        )

    # ä½¿ç”¨ç³»ç»Ÿè®°å½•çš„çœŸå®æˆæœ¬ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°ä¼°ç®—
    total_cost = 0.0
    if accumulated_cost is not None:
        total_cost = accumulated_cost
    elif accumulated_token_usage:
        # å¤‡é€‰ï¼šä½¿ç”¨ç®€å•ä¼°ç®—
        prompt_tokens = accumulated_token_usage.get('prompt_tokens', 0)
        completion_tokens = accumulated_token_usage.get('completion_tokens', 0)
        total_cost = (prompt_tokens * 3 / 1_000_000) + (completion_tokens * 15 / 1_000_000)

    # è®¡ç®—è¿è¡Œæ—¶é—´
    runtime_seconds = 0.0
    if start_time and end_time:
        try:
            from datetime import datetime
            start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
            end_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
            runtime_seconds = (end_dt - start_dt).total_seconds()
        except Exception:
            runtime_seconds = 0.0

    return {
        'total_tokens': total_tokens,
        'total_cost': total_cost,
        'runtime_seconds': runtime_seconds
    }

def detect_cheating_in_logs(structured_logs_dir: str, instance_id: str, logger=None) -> Dict[str, Any]:
    """
    æ£€æµ‹ç»“æ„åŒ–æ—¥å¿—ä¸­çš„ä½œå¼Šè¡Œä¸º

    Args:
        structured_logs_dir: ç»“æ„åŒ–æ—¥å¿—ç›®å½•
        instance_id: å®ä¾‹ID
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        DictåŒ…å«ä½œå¼Šæ£€æµ‹ç»“æœ:
        {
            'is_cheating': bool,
            'cheating_details': list,  # ä½œå¼Šè¡Œä¸ºè¯¦æƒ…åˆ—è¡¨
            'total_violations': int    # è¿è§„æ€»æ•°
        }
    """
    log_file = os.path.join(structured_logs_dir, f"{instance_id}.jsonl")

    if not os.path.exists(log_file):
        return {
            'is_cheating': False,
            'cheating_details': [],
            'total_violations': 0
        }

    cheating_details = []

    # å®šä¹‰ä½œå¼Šè§„åˆ™
    CHEATING_RULES = {
        'forbidden_system_access': [
            # r'"message".*find /usr/local/lib/python\d+\.\d+',
            # r'"command".*find /usr/local/lib/python\d+\.\d+',
            # r'running command.*find /usr/local/lib/python\d+\.\d+',
            r'"message".*cat /usr/local/lib/python\d+\.\d+',
            r'"command".*cat /usr/local/lib/python\d+\.\d+',
            r'"message".*reading file: /usr/local/lib/python\d+\.\d+',
            r'"message".*reading /usr/local/lib/python\d+\.\d+',
            # r'"message".*ls /usr/local/lib/python\d+\.\d+',
            # r'"command".*ls /usr/local/lib/python\d+\.\d+',
            # r'"path".*/usr/local/lib/python\d+\.\d+/',
        ],
    }

    try:
        # å…ˆå°è¯•è¯»å–æ•´ä¸ªæ–‡ä»¶å†…å®¹
        with open(log_file, 'r', encoding='utf-8') as f:
            file_content = f.read()

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æœç´¢æ–‡ä»¶å†…å®¹ä¸­çš„è¿è§„æ¨¡å¼
        import re
        line_number = 0

        # æŒ‰è¡Œåˆ†å‰²æ–‡ä»¶å†…å®¹è¿›è¡Œæ£€æŸ¥
        for line_num, line in enumerate(file_content.split('\n'), 1):
            if not line.strip():
                continue

            line_content = line.lower()

            # æ£€æŸ¥æ‰€æœ‰è§„åˆ™
            for rule_category, patterns in CHEATING_RULES.items():
                for pattern in patterns:
                    if re.search(pattern, line_content):
                        cheating_details.append({
                            'rule_category': rule_category,
                            'pattern': pattern,
                            'line_number': line_num,
                            'matched_content': line[:200],  # æˆªå–å‰200å­—ç¬¦é¿å…å¤ªé•¿
                            'violation_type': f'åŒ¹é…åˆ°ç¦æ­¢æ¨¡å¼: {pattern}'
                        })

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿è§„ï¼Œä¹Ÿå°è¯•JSONLè§£ææ–¹å¼ä½œä¸ºå¤‡ä»½
        if not cheating_details:
            with open(log_file, 'r', encoding='utf-8') as f:
                line_number = 0
                for line in f:
                    line_number += 1
                    try:
                        log_entry = json.loads(line.strip())
                        if not isinstance(log_entry, dict):
                            continue

                        # æ£€æŸ¥JSONå†…å®¹
                        content = log_entry.get('content', '')
                        message = log_entry.get('message', '')
                        command = log_entry.get('command', '')
                        path = log_entry.get('path', '')

                        # åˆå¹¶æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„æ–‡æœ¬å†…å®¹
                        text_to_check = f"{content} {message} {command} {path}".lower()

                        # æ£€æŸ¥ç¦æ­¢å‘½ä»¤å’Œç³»ç»Ÿè®¿é—®
                        for rule_category, patterns in CHEATING_RULES.items():
                            for pattern in patterns:
                                if re.search(pattern, text_to_check):
                                    cheating_details.append({
                                        'rule_category': rule_category,
                                        'pattern': pattern,
                                        'line_number': line_number,
                                        'matched_content': content[:200],  # æˆªå–å‰200å­—ç¬¦é¿å…å¤ªé•¿
                                        'violation_type': f'åŒ¹é…åˆ°ç¦æ­¢æ¨¡å¼: {pattern}'
                                    })

                        # ç‰¹æ®Šæ£€æŸ¥ï¼šæ–‡ä»¶è¯»å–æ“ä½œ
                        if log_entry.get('action') == 'read' or 'read_file' in text_to_check:
                            file_path = ''
                            # å°è¯•ä»ä¸åŒå­—æ®µæå–æ–‡ä»¶è·¯å¾„
                            if 'file' in log_entry:
                                file_path = str(log_entry['file']).lower()
                            elif 'path' in log_entry:
                                file_path = str(log_entry['path']).lower()
                            elif 'filePath' in log_entry:
                                file_path = str(log_entry['filePath']).lower()

                            if file_path:
                                for pattern in CHEATING_RULES['forbidden_file_access']:
                                    if re.search(pattern, file_path):
                                        cheating_details.append({
                                            'rule_category': 'forbidden_file_access',
                                            'pattern': pattern,
                                            'line_number': line_number,
                                            'matched_content': file_path,
                                            'violation_type': f'å°è¯•è¯»å–ç¦æ­¢çš„æ–‡ä»¶: {file_path}'
                                        })

                    except json.JSONDecodeError:
                        continue
                    except Exception as e:
                        logger.warning(f"Error processing line {line_number} in {instance_id}: {e}")
                        continue

    except Exception as e:
        logger.error(f"Error reading structured log for {instance_id}: {e}")
        return {
            'is_cheating': False,
            'cheating_details': [],
            'total_violations': 0
        }

    total_violations = len(cheating_details)
    is_cheating = total_violations > 0

    if is_cheating and logger:
        logger.warning(f"Detected {total_violations} potential cheating violations in {instance_id}")

    return {
        'is_cheating': is_cheating,
        'cheating_details': cheating_details,
        'total_violations': total_violations
    }


def generate_evaluation_summary_report(
    eval_output_dir: str,
    dataset: pd.DataFrame,
    submitted_instances_count: Optional[int] = None,
    logger=None
) -> None:
    """
    ç”Ÿæˆè¯„ä¼°æ‘˜è¦æŠ¥å‘Š
    Args:
        eval_output_dir: è¯„ä¼°è¾“å‡ºç›®å½•è·¯å¾„
        dataset: æ•°æ®é›†DataFrame
        submitted_instances_count: æäº¤è¯„ä¼°çš„å®ä¾‹æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨dataseté•¿åº¦
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¯é€‰
    """
    if logger:
        logger.info('Generating evaluation summary report...')

    instance_status_path = os.path.join(eval_output_dir, 'instance_status.txt')
    repos_dir = os.path.join(eval_output_dir, 'repos')
    structured_logs_dir = os.path.join(eval_output_dir, 'structured_logs')
    summary_md_path = os.path.join(eval_output_dir, 'report.md')

    if not os.path.exists(instance_status_path):
        if logger:
            logger.warning(f"Instance status file not found at {instance_status_path}, skipping summary generation.")
        return

    # æ”¶é›†æ‰€æœ‰å®ä¾‹çš„ç»“æœ
    instance_results = {}
    resolved_count = 0
    cheating_count = 0

    # 1. ä» instance_status.txt è¯»å–å¹¶åˆ†ç±»æ‰€æœ‰å¤„ç†è¿‡çš„å®ä¾‹
    with open(instance_status_path, 'r') as f:
        for line in f:
            try:
                record = json.loads(line.strip())
                instance_id = record.get("instance_id")
                if not instance_id:
                    continue

                # æå– tokenã€æˆæœ¬å’Œè¿è¡Œæ—¶é—´ä¿¡æ¯
                metrics = extract_instance_metrics(structured_logs_dir, instance_id, logger)

                # æ£€æµ‹ä½œå¼Šè¡Œä¸º
                cheating_result = detect_cheating_in_logs(structured_logs_dir, instance_id, logger)

                if record.get("status") == "success" and not cheating_result['is_cheating']:
                    repo_path = record.get("repo_json_path")
                    if repo_path and os.path.exists(repo_path):
                        with open(repo_path, 'r') as repo_f:
                            # è¯»å– repo.json çš„å†…å®¹
                            repo_content = json.load(repo_f)

                            # è§£ææµ‹è¯•ç»“æœ
                            summary = repo_content.get('summary', {})
                            total_tests = summary.get('total', 0)
                            passed_tests = summary.get('passed', 0)
                            failed_tests = summary.get('failed', 0)
                            error_tests = summary.get('error', 0)

                            # è®¡ç®—é€šè¿‡ç‡
                            test_pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

                            # åˆ¤æ–­æ˜¯å¦è§£å†³ï¼ˆæ ¹æ®æ˜¯å¦æœ‰å¤±è´¥æˆ–é”™è¯¯çš„æµ‹è¯•æ¥åˆ¤æ–­ï¼‰
                            resolved = failed_tests == 0 and error_tests == 0 and total_tests > 0

                            if resolved:
                                resolved_count += 1

                            instance_results[instance_id] = {
                                'resolved': resolved,
                                'total_tests': total_tests,
                                'passed_tests': passed_tests,
                                'failed_tests': failed_tests,
                                'error_tests': error_tests,
                                'test_pass_rate': test_pass_rate,
                                'test_file': 'test suite',
                                'repo_content': repo_content,
                                'total_tokens': metrics['total_tokens'],
                                'total_cost': metrics['total_cost'],
                                'runtime_seconds': metrics['runtime_seconds'],
                                'is_cheating': False,
                                'cheating_details': []
                            }
                    else:
                        instance_results[instance_id] = {
                            'resolved': False,
                            'total_tests': 0,
                            'passed_tests': 0,
                            'failed_tests': 0,
                            'error_tests': 0,
                            'test_pass_rate': 0,
                            'test_file': 'repo.json not found',
                            'repo_content': None,
                            'total_tokens': metrics['total_tokens'],
                            'total_cost': metrics['total_cost'],
                            'runtime_seconds': metrics['runtime_seconds'],
                            'is_cheating': False,
                            'cheating_details': []
                        }
                elif cheating_result['is_cheating']:
                    # å‘ç°ä½œå¼Šè¡Œä¸ºï¼Œä½†ä»ç„¶è¦æ˜¾ç¤ºæµ‹è¯•ç»“æœæ•°æ®
                    cheating_count += 1

                    # å°è¯•è·å–æµ‹è¯•ç»“æœæ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    repo_path = record.get("repo_json_path")
                    if repo_path and os.path.exists(repo_path):
                        try:
                            with open(repo_path, 'r') as repo_f:
                                repo_content = json.load(repo_f)

                                # è§£ææµ‹è¯•ç»“æœ
                                summary = repo_content.get('summary', {})
                                total_tests = summary.get('total', 0)
                                passed_tests = summary.get('passed', 0)
                                failed_tests = summary.get('failed', 0)
                                error_tests = summary.get('error', 0)

                                # è®¡ç®—é€šè¿‡ç‡
                                test_pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

                                instance_results[instance_id] = {
                                    'resolved': False,  # ä½œå¼Šä¸ç®—è§£å†³
                                    'total_tests': total_tests,
                                    'passed_tests': passed_tests,
                                    'failed_tests': failed_tests,
                                    'error_tests': error_tests,
                                    'test_pass_rate': test_pass_rate,
                                    'test_file': 'test suite',
                                    'repo_content': repo_content,
                                    'total_tokens': metrics['total_tokens'],
                                    'total_cost': metrics['total_cost'],
                                    'runtime_seconds': metrics['runtime_seconds'],
                                    'is_cheating': True,
                                    'cheating_details': cheating_result['cheating_details']
                                }
                        except:
                            # å¦‚æœæ— æ³•è¯»å–æµ‹è¯•ç»“æœï¼Œè®¾ç½®ä¸ºé»˜è®¤å€¼
                            instance_results[instance_id] = {
                                'resolved': False,
                                'total_tests': 0,
                                'passed_tests': 0,
                                'failed_tests': 0,
                                'error_tests': 0,
                                'test_pass_rate': 0,
                                'test_file': 'repo.json not found',
                                'repo_content': None,
                                'total_tokens': metrics['total_tokens'],
                                'total_cost': metrics['total_cost'],
                                'runtime_seconds': metrics['runtime_seconds'],
                                'is_cheating': True,
                                'cheating_details': cheating_result['cheating_details']
                            }
                    else:
                        # æ²¡æœ‰æµ‹è¯•ç»“æœæ–‡ä»¶
                        instance_results[instance_id] = {
                            'resolved': False,
                            'total_tests': 0,
                            'passed_tests': 0,
                            'failed_tests': 0,
                            'error_tests': 0,
                            'test_pass_rate': 0,
                            'test_file': 'repo.json not found',
                            'repo_content': None,
                            'total_tokens': metrics['total_tokens'],
                            'total_cost': metrics['total_cost'],
                            'runtime_seconds': metrics['runtime_seconds'],
                            'is_cheating': True,
                            'cheating_details': cheating_result['cheating_details']
                        }
                else:
                    instance_results[instance_id] = {
                        'resolved': False,
                        'total_tests': 0,
                        'passed_tests': 0,
                        'failed_tests': 0,
                        'error_tests': 0,
                        'test_pass_rate': 0,
                        'test_file': record.get("error", "Unknown error"),
                        'repo_content': None,
                        'total_tokens': metrics['total_tokens'],
                        'total_cost': metrics['total_cost'],
                        'runtime_seconds': metrics['runtime_seconds'],
                        'is_cheating': False,
                        'cheating_details': []
                    }
            except json.JSONDecodeError:
                continue

    # 2. è®¡ç®—æ•´ä½“ç»Ÿè®¡
    total_instances = len(dataset) if submitted_instances_count is None else submitted_instances_count
    failed_count = len([v for v in instance_results.values() if not v.get('resolved', False) and not v.get('is_cheating', False)])
    resolution_rate = (resolved_count / total_instances * 100) if total_instances > 0 else 0
    failure_rate = (failed_count / total_instances * 100) if total_instances > 0 else 0
    cheating_rate = (cheating_count / total_instances * 100) if total_instances > 0 else 0

    # 3. ç”ŸæˆmarkdownæŠ¥å‘Š
    from datetime import datetime

    md_content = f"""# Programmer Bench Real World æµ‹è¯•ç»“æœæŠ¥å‘Š

æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ•´ä½“ç»Ÿè®¡

- **æ€»å®ä¾‹æ•°**: {total_instances}
- **å·²è§£å†³å®ä¾‹æ•°**: {resolved_count}
- **å¤±è´¥å®ä¾‹æ•°**: {failed_count}
- **ä½œå¼Šå®ä¾‹æ•°**: {cheating_count}
- **è§£å†³ç‡**: {resolution_rate:.1f}%
- **å¤±è´¥ç‡**: {failure_rate:.1f}%
- **ä½œå¼Šç‡**: {cheating_rate:.1f}%

## è¯¦ç»†ç»“æœ

| Instance ID | çŠ¶æ€ | è§£å†³ | Pass | Fail | Error | Total | Passç‡ | Tokens | Cost (Â¥) | æ—¶é—´ (s) |
|-------------|------|------|------|------|-------|-------|--------|---------|----------|----------|"""

    # æŒ‰ instance_id æ’åºæ˜¾ç¤ºç»“æœ
    all_instance_ids = [row['instance_id'] for _, row in dataset.iterrows()]
    for instance_id in sorted(all_instance_ids):
        result = instance_results.get(instance_id, {
            'resolved': False,
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'error_tests': 0,
            'test_pass_rate': 0,
            'total_tokens': 0,
            'total_cost': 0.0,
            'runtime_seconds': 0.0,
            'is_cheating': False,
            'cheating_details': []
        })

        # çŠ¶æ€åˆ—æŒ‰åŸæ¥çš„é€»è¾‘ï¼šæ˜¯å¦æœ‰æµ‹è¯•æ•°æ®
        status = 'âœ…' if result.get('total_tests', 0) > 0 else 'âŒ'

        # è§£å†³åˆ—ï¼šæ ¹æ®æ˜¯å¦ä½œå¼Šæ˜¾ç¤ºä¸åŒçš„æ ‡è¯†
        if result.get('is_cheating', False):
            resolved = 'ğŸš«'  # ä½œå¼Šæ ‡è¯†
        else:
            resolved = 'ğŸŸ¢' if result.get('resolved', False) else 'ğŸ”´'

        pass_rate = f"{result.get('test_pass_rate', 0):.1f}%"
        tokens = result.get('total_tokens', 0)
        cost = f"{result.get('total_cost', 0.0):.4f}"
        runtime = f"{result.get('runtime_seconds', 0.0):.1f}"

        md_content += f"\n| {instance_id} | {status} | {resolved} | {result.get('passed_tests', 0)} | {result.get('failed_tests', 0)} | {result.get('error_tests', 0)} | {result.get('total_tests', 0)} | {pass_rate} | {tokens} | {cost} | {runtime} |"

    md_content += f"""

### å›¾ä¾‹
- **çŠ¶æ€**: âœ… æœ‰æµ‹è¯•æ•°æ® | âŒ æ— æµ‹è¯•æ•°æ®
- **è§£å†³**: ğŸŸ¢ å·²è§£å†³ (æ— å¤±è´¥æ— é”™è¯¯) | ğŸ”´ æœªè§£å†³ | ğŸš« ä½œå¼Šè¿è§„

"""

    # æ·»åŠ ä½œå¼Šè¡Œä¸ºè¯¦æƒ…
    cheating_instances = {k: v for k, v in instance_results.items() if v.get('is_cheating', False)}

    if cheating_instances:
        md_content += """## ä½œå¼Šè¡Œä¸ºè¯¦æƒ…

ä»¥ä¸‹å®ä¾‹æ£€æµ‹åˆ°ä½œå¼Šè¡Œä¸ºï¼Œå·²è¢«è‡ªåŠ¨æ ‡è®°ä¸ºè¿è§„ï¼š

"""
        for instance_id, result in cheating_instances.items():
            md_content += f"### {instance_id}\n\n"
            cheating_details = result.get('cheating_details', [])

            if cheating_details:
                md_content += f"**æ£€æµ‹åˆ° {len(cheating_details)} é¡¹è¿è§„è¡Œä¸º**ï¼š\n\n"

                # æŒ‰è§„åˆ™ç±»å‹åˆ†ç»„æ˜¾ç¤º
                violations_by_category = {}
                for detail in cheating_details:
                    category = detail['rule_category']
                    if category not in violations_by_category:
                        violations_by_category[category] = []
                    violations_by_category[category].append(detail)

                for category, violations in violations_by_category.items():
                    category_names = {
                        'forbidden_system_access': 'ğŸš¨ ç¦æ­¢ç³»ç»Ÿè®¿é—®',
                        'forbidden_commands': 'ğŸš« ç¦æ­¢å‘½ä»¤',
                        'forbidden_file_access': 'ğŸ“„ ç¦æ­¢æ–‡ä»¶è®¿é—®',
                        'suspicious_patterns': 'âš ï¸ å¯ç–‘è¡Œä¸ºæ¨¡å¼'
                    }
                    md_content += f"#### {category_names.get(category, category)}\n\n"

                    for violation in violations:
                        md_content += f"- **ç¬¬{violation['line_number']}è¡Œ**: {violation['violation_type']}\n"
                        md_content += f"  - åŒ¹é…æ¨¡å¼: `{violation['pattern']}`\n"
                        md_content += f"  - æ£€æµ‹å†…å®¹: `{violation['matched_content'][:100]}{'...' if len(violation['matched_content']) > 100 else ''}`\n\n"

            md_content += "---\n\n"

    # æ·»åŠ å¤±è´¥æµ‹è¯•çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    failed_instances = {k: v for k, v in instance_results.items()
                       if (v.get('failed_tests', 0) > 0 or v.get('error_tests', 0) > 0) and not v.get('is_cheating', False)}

    if failed_instances:
        md_content += """## å¤±è´¥æµ‹è¯•è¯¦æƒ…

"""
        for instance_id, result in failed_instances.items():
            md_content += f"### {instance_id}\n\n"

            # ä» repo_content ä¸­è·å–å¤±è´¥çš„æµ‹è¯•è¯¦æƒ…
            repo_content = result.get('repo_content')
            if repo_content:
                test_cases = repo_content.get('test_cases', {})
                failures = repo_content.get('failures', {})

                # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•ï¼Œç”¨ä»£ç å—åŒ…å›´
                failed_tests = [test_name for test_name, status in test_cases.items() if status == 'failed']
                if failed_tests:
                    md_content += "**å¤±è´¥çš„æµ‹è¯•**:\n```\n"
                    for test_name in failed_tests:
                        error_msg = failures.get(test_name, 'æ— é”™è¯¯ä¿¡æ¯')
                        md_content += f"- {test_name}: {error_msg}\n"
                    md_content += "```\n\n"

    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    with open(summary_md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)

    if logger:
        logger.info(f"Report generated: {summary_md_path}")
        logger.info(f"Overall statistics: {resolved_count}/{total_instances} instances resolved ({resolution_rate:.1f}%)")
        if cheating_count > 0:
            logger.warning(f"Cheating detection: {cheating_count}/{total_instances} instances detected as cheating ({cheating_rate:.1f}%)")


def _build_test_command(base_cmd: str, test_path: str, timeout: int, output_file: str = None, json_report_file: str = None) -> str:
    """
    æ ¹æ®åŸºç¡€æµ‹è¯•å‘½ä»¤æ„å»ºå®Œæ•´çš„æµ‹è¯•å‘½ä»¤

    Args:
        base_cmd: åŸºç¡€æµ‹è¯•å‘½ä»¤ï¼Œå¦‚ 'pytest -vs' æˆ– 'python tests/runtests.py --verbosity 2'
        test_path: æµ‹è¯•è·¯å¾„
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        json_report_file: JSONæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        str: å®Œæ•´çš„æµ‹è¯•å‘½ä»¤
    """
    # æ£€æŸ¥æ˜¯å¦æ˜¯ pytest å‘½ä»¤ æˆ–è€… Django å‘½ä»¤
    is_pytest = 'pytest' in base_cmd
    is_django_runtests = 'runtests.py' in base_cmd

    # æ„å»ºå‘½ä»¤ - ç®€å•å­—ç¬¦ä¸²æ‹¼æ¥
    if is_pytest:
        # pytest å‘½ä»¤ï¼šæ·»åŠ å¿…è¦å‚æ•°
        cmd = f"{base_cmd} {test_path} --color=no"

        if '--timeout' not in base_cmd:
            cmd += f" --timeout={timeout}"

        if json_report_file:
            cmd += f" --json-report --json-report-file={json_report_file}"
        elif output_file:
            cmd += f" > {output_file} 2>&1"

    elif is_django_runtests:
        # Django runtestsï¼šä¿æŒåŸæœ‰å‘½ä»¤
        cmd = base_cmd

        # å¯¹äº Djangoï¼Œå¦‚æœæä¾›äº†æµ‹è¯•è·¯å¾„ï¼Œé€šå¸¸æ˜¯æµ‹è¯•æ¨¡å—åï¼Œç›´æ¥æ·»åŠ 
        if test_path:
            # FIXME: è¿™é‡Œåº”è¯¥æ˜¯ç‚¹åˆ†å½¢å¼æ‰å¯¹, ä¹‹åä¿®
            cmd += f" {test_path}"

        # æ·»åŠ è¾“å‡ºé‡å®šå‘
        if output_file:
            cmd += f" > {output_file} 2>&1"

    else:
        raise ValueError(f"è¯„æµ‹ pipeline è¿˜æœªæ”¯æŒçš„æµ‹è¯•å‘½ä»¤: {base_cmd}")

    return cmd


def run_pytest_and_evaluate(runtime, instance, work_dir: str = 'workspace', test_time: int = 3600, logger=None, task_level=None, repo_name=None):
    """
    è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¯„ä¼°ç»“æœçš„é€šç”¨å‡½æ•°

    Args:
        runtime: OpenHands runtime å®ä¾‹
        instance: åŒ…å« instance_idã€timeout å’Œ test_cmd çš„å®ä¾‹å¯¹è±¡
        work_dir: å·¥ä½œç›®å½•åç§°ï¼Œé»˜è®¤ä¸º 'workspace'
        test_time: ç¡¬è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º 3600 ç§’
        logger: æ—¥å¿—è®°å½•å™¨
        task_level: ä»»åŠ¡çº§åˆ«
        repo_name: ä»“åº“åç§°

    Returns:
        dict: åŒ…å«æ‰§è¡Œç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
        {
            'status': 'success' | 'failure',
            'error': str | None,
            'instance_id': str,
            'detail': str,
        }

    Notes:
        - æ”¯æŒè‡ªå®šä¹‰æµ‹è¯•å‘½ä»¤é€šè¿‡ instance.test_cmdï¼Œé»˜è®¤ä¸º 'pytest -vs'
        - å¯¹äº pytest å‘½ä»¤ä¼šè‡ªåŠ¨æ·»åŠ  --timeoutã€--color=no ç­‰å‚æ•°
        - å¯¹äº Django runtests.py ç­‰å‘½ä»¤ä¼šæ™ºèƒ½å¤„ç†ï¼Œä¸æ·»åŠ ä¸æ”¯æŒçš„å‚æ•°
    """

    detail = ''
    is_level_1 = (task_level == 1 or task_level == '1')

    # è·å–æµ‹è¯•å‘½ä»¤ï¼Œé»˜è®¤ä¸º 'pytest -vs'
    test_cmd_raw = getattr(instance, 'test_cmd', instance.get('test_cmd') if hasattr(instance, 'get') else None)

    # å¤„ç†å„ç§æ— æ•ˆå€¼æƒ…å†µï¼šNone, NaN, ç©ºå­—ç¬¦ä¸²ç­‰
    if (test_cmd_raw is None or
        test_cmd_raw == '' or
        (isinstance(test_cmd_raw, float) and (str(test_cmd_raw).lower() == 'nan' or pd.isna(test_cmd_raw)))):
        test_cmd = 'pytest -vs'
    elif isinstance(test_cmd_raw, str):
        test_cmd = test_cmd_raw.strip() or 'pytest -vs'  # å¤„ç†åªæœ‰ç©ºç™½å­—ç¬¦çš„æƒ…å†µ
    else:
        # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        test_cmd = str(test_cmd_raw)

    logger.info(f"Using test command: {test_cmd}")

    # 1. æŠŠæµ‹è¯•ç›®å½•æŒ‚è½½åˆ° runtime é‡Œ
    sandbox_app_path = f'/{work_dir}'
    test_host_path = getattr(instance, 'test_path', instance.get('test_path') if hasattr(instance, 'get') else None)
    if test_host_path and os.path.exists(test_host_path):
        runtime.copy_to(test_host_path, sandbox_app_path, recursive=True)
        logger.info(f"Copied test from '{test_host_path}' to sandbox at '{sandbox_app_path}'")
    else:
        error_msg = f'Test path not provided or not found: {test_host_path}'
        logger.warning(error_msg)
        return {
            'status': 'failure',
            'error': error_msg,
            'instance_id': getattr(instance, 'instance_id', str(instance)),
            'detail': detail
        }

    # 2.1 è¯»å– WORK_DIR/test/path2test.txt, é‡Œé¢çš„ py è·¯å¾„è®°ä½œ test_path, è½¬ä¸ºç»å¯¹è·¯å¾„
    sandbox_test_path = os.path.join(sandbox_app_path, 'test')
    path2test_file = os.path.join(sandbox_test_path, 'path2test.txt')

    # 2.2 æ£€æŸ¥ path2test.txt æ˜¯å¦å­˜åœ¨
    action = CmdRunAction(command=f'test -f {path2test_file}')
    obs = runtime.run_action(action)
    if obs.exit_code != 0:
        error_msg = f"path2test.txt not found at {path2test_file}"
        logger.error(error_msg)
        return {
            'status': 'failure',
            'error': error_msg,
            'instance_id': getattr(instance, 'instance_id', str(instance)),
            'detail': detail
        }

    # 2.3 è¯»å– path2test.txt è·å–æµ‹è¯•è·¯å¾„
    action = CmdRunAction(command=f'cat {path2test_file}')
    obs = runtime.run_action(action)
    if obs.exit_code != 0:
        error_msg = f"Failed to read path2test.txt: {obs.content}"
        logger.error(error_msg)
        return {
            'status': 'failure',
            'error': error_msg,
            'instance_id': getattr(instance, 'instance_id', str(instance)),
            'detail': detail
        }

    # æ”¯æŒ path2test.txt å¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ªæµ‹è¯•è·¯å¾„
    test_path_lines = [line.strip() for line in obs.content.splitlines() if line.strip()]
    if not test_path_lines:
        error_msg = "path2test.txt is empty"
        logger.error(error_msg)
        return {
            'status': 'failure',
            'error': error_msg,
            'instance_id': getattr(instance, 'instance_id', str(instance)),
            'detail': detail
        }
    # test_path_list: æ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªæµ‹è¯•è·¯å¾„
    test_path_list = test_path_lines

    # å¯¹äº level 3, éœ€è¦å…ˆæŠŠ /testbed ä¸‹çš„ä¸œè¥¿æ¸…ç©º, ç„¶åæŠŠ sandbox_test_path/repo_name/* é‡Œé¢çš„å†…å®¹ mv åˆ° /testbed ä¸‹, ç„¶ååˆ é™¤ç©ºæ–‡ä»¶å¤¹ sandbox_test_path/repo_name
    if not is_level_1:  # level 3
        logger.info("[LV3] Starting level 3 specific processing")

        # 1. æ¸…ç©º /testbed ä¸‹çš„æ‰€æœ‰å†…å®¹
        clear_action = CmdRunAction(command='rm -rf /testbed/*')
        clear_obs = runtime.run_action(clear_action)
        if clear_obs.exit_code == 0:
            logger.info("[LV3] Successfully cleared /testbed directory")
        else:
            logger.warning(f"[LV3] Failed to clear /testbed directory: {clear_obs.content}")

        # 2. å°† sandbox_test_path/repo_name/* çš„å†…å®¹ç§»åŠ¨åˆ° /testbed ä¸‹
        repo_source_path = os.path.join(sandbox_test_path, repo_name)
        move_action = CmdRunAction(command=f'mv {repo_source_path}/* /testbed/')
        move_obs = runtime.run_action(move_action)
        if move_obs.exit_code == 0:
            logger.info(f"[LV3] Successfully moved contents from {repo_source_path} to /testbed")
        else:
            logger.warning(f"[LV3] Failed to move contents from {repo_source_path} to /testbed: {move_obs.content}")

        # 4. åˆ é™¤ sandbox_test_path/repo_name æ–‡ä»¶å¤¹
        rm_action = CmdRunAction(command=f'rm -rf {repo_source_path}')
        rm_obs = runtime.run_action(rm_action)
        if rm_obs.exit_code == 0:
            logger.info(f"[LV3] Successfully removed empty directory {repo_source_path}")
        else:
            logger.warning(f"[LV3] Failed to remove directory {repo_source_path}: {rm_obs.content}")

    # å¯¹äº level 1, éœ€è¦å¯¹æ¯ä¸ª test_path è¿›è¡Œç»å¯¹è·¯å¾„è½¬æ¢å’Œç§»åŠ¨
    if is_level_1:
        # level=1: test_path å½¢å¦‚ repo_name/../.., éœ€è¦æ›¿æ¢ä¸º testbed/...
        # 1. å¯¹æ¯ä¸ª test_path è¿›è¡Œè·¯å¾„è½¬æ¢å’Œç§»åŠ¨
        abs_test_path_list = []
        for test_path in test_path_list:
            test_name = os.path.basename(test_path) # æ‹¿åˆ°æµ‹è¯•æ–‡ä»¶è‡ªå·±çš„åå­—
            source_path = f'/{work_dir}/test/{test_name}'
            # å°† test_path å¤´éƒ¨çš„ repo_name æ›¿æ¢ä¸º testbed
            modified_test_path = test_path.replace(repo_name + '/', 'testbed/', 1)
            # target_test_path å°±æ˜¯ /{modified_test_path}
            target_test_path = f'/{modified_test_path}'
            action = CmdRunAction(command=f'mv {source_path} {target_test_path}')
            obs_mv = runtime.run_action(action)
            if obs_mv.exit_code != 0:
                logger.warning(f"Failed to move test file from {source_path} to {target_test_path}: {obs_mv.content}")
            abs_test_path_list.append(target_test_path)
        test_path_list = abs_test_path_list
    else:
        # level=3: ç°åœ¨ä»“åº“å†…å®¹å·²ç»ç§»åŠ¨åˆ° /testbed ä¸‹
        # éœ€è¦æŠŠ test_path è½¬æ¢ä¸º /testbed ä¸‹çš„ç»å¯¹è·¯å¾„
        abs_test_path_list = []
        for test_path in test_path_list:
            # å¯¹äºç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦ä» test_path ä¸­å»æ‰ repo_name å‰ç¼€ï¼Œç„¶åæ‹¼æ¥åˆ° /testbed ä¸‹
            relative_path = test_path[len(repo_name) + 1:]
            # æ‹¼æ¥åˆ° /testbed/ ä¸‹
            abs_test_path = os.path.join('/testbed', relative_path)
            abs_test_path_list.append(abs_test_path)
        test_path_list = abs_test_path_list

    logger.info(f"Test paths from path2test.txt: {test_path_list}")

    # æ‰§è¡Œ /work_dir/test ä¸‹çš„ wrap_imports_with_try.py
    wrap_target_path = '/testbed'
    action = CmdRunAction(
        command=f'python {sandbox_test_path}/wrap_imports_with_try.py {wrap_target_path} -r --no-backup',
        blocking=True,  # è®¾ç½®é˜»å¡æ¨¡å¼ä¿è¯å‘½ä»¤æ‰§è¡Œå®Œæˆåå†ç»§ç»­
    )
    action.set_hard_timeout(test_time)
    obs = runtime.run_action(action)
    if obs.exit_code != 0:
        detail += '  ' + obs.message
        print(obs.message)
    logger.info(f"Successfully ran wrap_imports_with_try.py on {wrap_target_path}")

    # è·å– timeout å€¼
    if hasattr(instance, 'timeout'):
        timeout = instance.timeout
    elif hasattr(instance, 'get') and 'timeout' in instance:
        timeout = instance['timeout']
    else:
        raise ValueError(f"Instance {getattr(instance, 'instance_id', str(instance))} does not have timeout attribute")

    # 2.4 é’ˆå¯¹æ¯ä¸ªæµ‹è¯•æ–‡ä»¶åˆ†åˆ«æ‰§è¡Œ pytestï¼Œç»“æœåˆ†åˆ«ä¿å­˜
    work_dir_path = '/testbed'

    repos_dir = os.path.join(sandbox_test_path, 'repos')
    raw_output_dir = os.path.join(sandbox_test_path, 'raw_output')
    # åˆ›å»ºç»“æœç›®å½•
    runtime.run_action(CmdRunAction(command=f'mkdir -p {repos_dir}'))
    runtime.run_action(CmdRunAction(command=f'mkdir -p {raw_output_dir}'))

    first_repo_json_path = None
    for idx, test_path in enumerate(test_path_list):
        test_name = os.path.basename(test_path)
        raw_output_file = os.path.join(raw_output_dir, f'{test_name}.txt')
        repo_tmp = os.path.join(repos_dir, f'{test_name}.json')

        # 1. æ‰§è¡Œæµ‹è¯•å¹¶è¾“å‡ºåˆ°æ–‡ä»¶
        test_command1 = _build_test_command(test_cmd, test_path, timeout, output_file=raw_output_file)
        action1 = CmdRunAction(
            command=f'cd {work_dir_path} && {test_command1}',
            blocking=True,
        )
        action1.set_hard_timeout(test_time)
        obs1 = runtime.run_action(action1)
        if obs1.exit_code == -1:
            detail += obs1.message
            print(obs1.message)
        logger.info(f"Test for {test_name} completed, output saved to {raw_output_file}")

        # æ£€æŸ¥ raw_output_file æ˜¯å¦å­˜åœ¨, è‹¥ä¸å­˜åœ¨, continue
        check_action = CmdRunAction(command=f'test -f {raw_output_file}')
        check_obs = runtime.run_action(check_action)
        if check_obs.exit_code != 0:
            error_msg = f"Raw output file {raw_output_file} was not created"
            logger.error(error_msg)
            continue

        # 2. æ‰§è¡Œæµ‹è¯•ç”Ÿæˆ JSON æŠ¥å‘Šï¼ˆä»…å¯¹æ”¯æŒçš„å‘½ä»¤ï¼‰ï¼Œç”Ÿæˆä¹‹å‰å…ˆè¿›è¡Œä¸€æ­¥é™é»˜åˆ é™¤ repo_tmp.json
        runtime.run_action(CmdRunAction(command=f'rm -f {os.path.join(sandbox_test_path, "repo_tmp.json")}'))
        repo_tmp_local = os.path.join(sandbox_test_path, 'repo_tmp.json')

        # æ£€æŸ¥æ˜¯å¦æ”¯æŒ JSON æŠ¥å‘Šï¼ˆä¸»è¦æ˜¯ pytestï¼‰
        if 'pytest' in test_cmd:
            test_command2 = _build_test_command(test_cmd, test_path, timeout, json_report_file=repo_tmp_local)
            action2 = CmdRunAction(
                command=f'cd {work_dir_path} && {test_command2}',
                blocking=True,
            )
            action2.set_hard_timeout(test_time)
            obs2 = runtime.run_action(action2)
            if obs2.exit_code != 0:
                detail += f" {obs2.message}"
                logger.warning(f"Test json command failed for {test_name}: exit_code={obs2.exit_code}, content={obs2.content}")
                print(obs2.message)
            logger.info(f"Test json for {test_name} completed")
        # è¿è¡Œåˆ°è¿™é‡Œçš„å°±æ˜¯ Django
        else:
            # å¯¹äºä¸æ”¯æŒ JSON æŠ¥å‘Šçš„å‘½ä»¤ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ JSON æ–‡ä»¶æˆ–è·³è¿‡
            logger.warning(f"Test command '{test_cmd}' does not support JSON reporting, skipping JSON report generation")
            # åˆ›å»ºä¸€ä¸ªæœ€å°çš„ JSON æŠ¥å‘Šç»“æ„ï¼Œä½¿ç”¨æ²™ç›’å‘½ä»¤
            minimal_report_content = '''{
    "summary": {"total": 0, "passed": 0, "failed": 0, "error": 0},
    "test_cases": {},
    "failures": {},
    "note": "JSON reporting not supported for this test command"
}'''
            create_json_action = CmdRunAction(command=f'echo \'{minimal_report_content}\' > {repo_tmp_local}')
            runtime.run_action(create_json_action)

        # æ£€æŸ¥ repo_tmp.json æ–‡ä»¶æ˜¯å¦å­˜åœ¨, è‹¥ä¸å­˜åœ¨, continue
        check_action2 = CmdRunAction(command=f'test -f {repo_tmp_local}')
        check_obs2 = runtime.run_action(check_action2)
        if check_obs2.exit_code != 0:
            error_msg = f"JSON report file {repo_tmp_local} was not created"
            logger.error(error_msg)
            continue

        # FIXME: è¿™é‡Œåº”è¯¥æ¢ä¸ºè§£æå™¨åˆ°æ—¶å€™
        # 3. æ‰§è¡Œ eval_code.pyï¼Œç”Ÿæˆ repo.json, ç”Ÿæˆä¹‹å‰å…ˆè¿›è¡Œä¸€æ­¥é™é»˜åˆ é™¤ repo.json
        runtime.run_action(CmdRunAction(command=f'rm -f {os.path.join(sandbox_test_path, "repo.json")}'))
        action_eval = CmdRunAction(
            command=f'python {sandbox_test_path}/eval_code.py',
            blocking=True,
        )
        action_eval.set_hard_timeout(test_time)
        obs_eval = runtime.run_action(action_eval)
        if obs_eval.exit_code != 0:
            detail += '  ' + obs_eval.message
            print(obs_eval.message)
        logger.info(f"Successfully ran eval_code.py for {test_name}")

        # æ£€æŸ¥ repo.json æ˜¯å¦ç”Ÿæˆ
        repo_json_path = os.path.join(sandbox_test_path, 'repo.json')
        check_action3 = CmdRunAction(command=f'test -f {repo_json_path}')
        check_obs3 = runtime.run_action(check_action3)
        if check_obs3.exit_code != 0:
            error_msg = f"repo.json was not created for {test_name}"
            logger.error(error_msg)
            continue

        # 4. ç§»åŠ¨ repo.json åˆ° repos/<test_name>.json
        move_action = CmdRunAction(command=f'mv {repo_json_path} {repo_tmp}')
        move_obs = runtime.run_action(move_action)
        if move_obs.exit_code != 0:
            logger.warning(f"Failed to move repo.json to {repo_tmp}: {move_obs.content}")

        # è®°å½•ç¬¬ä¸€ä¸ªæµ‹è¯•çš„ repo.json è·¯å¾„
        if idx == 0:
            first_repo_json_path = repo_tmp

    # å…¼å®¹ï¼šå°†ç¬¬ä¸€ä¸ªæµ‹è¯•çš„ç»“æœæ‹·è´ä¸º /work_dir/test/repo.json
    if first_repo_json_path:
        repo_json_link = os.path.join(sandbox_test_path, 'repo.json')
        runtime.run_action(CmdRunAction(command=f'cp {first_repo_json_path} {repo_json_link}'))

    return {
        'status': 'success',
        'error': None,
        'instance_id': getattr(instance, 'instance_id', str(instance)),
        'detail': detail,
    }


def extract_repo_json_and_cleanup(runtime, work_dir: str = 'workspace', logger=None):
    """
    æå– repo.json æ–‡ä»¶å†…å®¹å¹¶åˆ é™¤ test ç›®å½•

    Args:
        runtime: OpenHands runtime å®ä¾‹
        work_dir: å·¥ä½œç›®å½•åç§°ï¼Œé»˜è®¤ä¸º 'workspace'
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        str: repo.json æ–‡ä»¶çš„å†…å®¹ï¼Œå¦‚æœå‡ºé”™åˆ™è¿”å›é”™è¯¯ä¿¡æ¯
    """
    repo_json_content = ""
    ok = 0
    try:
        # 1. è¯» repo.json æ–‡ä»¶
        repo_json_path = f"/{work_dir}/test/repo.json"
        read_action = FileReadAction(path=repo_json_path)
        obs = runtime.run_action(read_action)

        if hasattr(obs, 'content') and obs.content:
            repo_json_content = obs.content
            ok += 1
            logger.info(f"æˆåŠŸè¯»å– repo.json æ–‡ä»¶ï¼Œå†…å®¹é•¿åº¦: {len(repo_json_content)}")
        else:
            logger.warning(f"æ— æ³•è¯»å– repo.json æ–‡ä»¶: {obs}")
            repo_json_content = "Unable to read repo.json file content"

        # 2. åˆ é™¤ /work_dir/test ç›®å½•
        delete_action = CmdRunAction(command=f"rm -rf /{work_dir}/test")
        delete_obs = runtime.run_action(delete_action)
        if delete_obs.exit_code == 0:
            ok += 1
            logger.info(f"æˆåŠŸåˆ é™¤ /{work_dir}/test ç›®å½•")
        else:
            logger.warning(f"åˆ é™¤ /{work_dir}/test ç›®å½•å¤±è´¥: {delete_obs.content}")

    except Exception as e:
        logger.warning(f"å¤„ç† repo.json æˆ–åˆ é™¤ç›®å½•æ—¶å‡ºé”™: {e}")
        repo_json_content = f"Error during processing: {str(e)}"

    return repo_json_content, (ok == 2)


def cleanup_test_directory(runtime, work_dir: str = 'workspace'):
    """
    é™é»˜åˆ é™¤ test ç›®å½•è¿›è¡Œæ¸…ç†

    Args:
        runtime: OpenHands runtime å®ä¾‹
        work_dir: å·¥ä½œç›®å½•åç§°ï¼Œé»˜è®¤ä¸º 'workspace'

    Raises:
        Exception: å½“ç›®å½•å­˜åœ¨ä½†åˆ é™¤å¤±è´¥æ—¶
    """
    # å…ˆæ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    check_action = CmdRunAction(command=f"test -d /{work_dir}/test")
    check_obs = runtime.run_action(check_action)

    # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if check_obs.exit_code != 0:
        return

    # ç›®å½•å­˜åœ¨ï¼Œå°è¯•åˆ é™¤
    delete_action = CmdRunAction(command=f"rm -rf /{work_dir}/test")
    delete_obs = runtime.run_action(delete_action)

    # å¦‚æœåˆ é™¤å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    if delete_obs.exit_code != 0:
        raise RuntimeError(f"Failed to delete /{work_dir}/test directory: {delete_obs.content}")
