base_image: pb-python310_nvidia-base_a48d8454
black_links:
- https://github.com/scikit-learn/scikit-learn/
commit: null
docker_specs:
  run_args:
    cap add: []
    cuda_visible_devices: 0,1
install: python -m pip install --upgrade pip setuptools wheel && python -m pip install
  build meson-python>=0.16.0 Cython>=3.1.2 ninja pkgconfig patchelf>=0.11.0 && python
  -m pip install 'numpy>=2.0.0' 'scipy>=1.8.0' && pip install -e . --no-build-isolation
  -v && echo '环境设置完成'
instance_image: pb-instance_dd6f1d1f
library_name: scikit-learn
pip_packages:
- build
- meson-python>=0.16.0
- Cython>=3.1.2
- numpy>=2.0.0
- scipy>=1.8.0
- pytest-timeout
- setuptools>=64.0.0
- wheel
- ninja
- pkgconfig
- patchelf>=0.11.0
- pytest>=7.1.2
- pytest-cov>=2.9.0
- pytest-xdist
- matplotlib>=3.5.0
- pandas>=1.4.0
- threadpoolctl>=3.1.0
- joblib>=1.2.0
- pillow
- pooch
python: '3.10'
repo_name: scikit-learn
repository: scikit-learn/scikit-learn
task_level: 3
task_name: scikit_learn_monotonic_constraints
task_statement: '## Task: Implement Histogram-Based Gradient Boosting Models


  **Core Functionality:**

  Build fast gradient boosting estimators for regression and classification using
  histogram-based tree growing, optimized for large datasets (10,000+ samples) with
  native missing value support.


  **Main Features & Requirements:**

  - **Dual Models**: Implement both regressor and classifier with configurable loss
  functions (squared_error, log_loss, etc.)

  - **Efficient Tree Growing**: Use histogram-based splitting with binned features
  for speed optimization

  - **Missing Value Handling**: Native support for NaN values with learned split directions

  - **Advanced Constraints**: Support monotonic constraints, interaction constraints,
  and categorical features

  - **Early Stopping**: Implement validation-based early stopping with configurable
  scoring metrics

  - **Prediction Methods**: Provide standard prediction plus staged prediction for
  monitoring training progress


  **Key Challenges:**

  - **Performance Optimization**: Balance speed gains from binning while maintaining
  prediction accuracy

  - **Constraint Enforcement**: Properly handle monotonic and interaction constraints
  during tree construction

  - **Memory Management**: Efficiently manage histogram computations and tree structures
  for large datasets

  - **Parameter Validation**: Ensure robust parameter validation across numerous hyperparameters
  and constraint combinations

  - **Multi-class Handling**: Scale appropriately from binary to multi-class classification
  (multiple trees per iteration)'
technical_docs: []
test_cmd: pytest --no-header -rA --tb=short -p no:cacheprovider --timeout=20
test_code1: 'from agent_code.sklearn.ensemble import HistGradientBoostingClassifier

  from agent_code.sklearn.ensemble import HistGradientBoostingRegressor

  from agent_code.sklearn.ensemble._hist_gradient_boosting.grower import TreeGrower'
test_code_example: from agent_code.sklearn.ensemble import HistGradientBoostingClassifier
test_code_example_obj: HistGradientBoostingClassifier
test_code_example_path: /testbed/agent_code/sklearn/ensemble.py
test_description1: Below is **Test Description 1**
timeout: 20
interface_description1: 'Below is **Interface Description 1** for file: sklearn-ensemble-_hist_gradient_boosting-gradient_boosting.py


  This file contains 2 top-level interface(s) that need to be implemented.

  '
interface_code1: "class HistGradientBoostingRegressor(RegressorMixin, BaseHistGradientBoosting):\n\
  \    \"\"\"\n    Histogram-based Gradient Boosting Regression Tree.\n    \n    \
  \    This estimator is much faster than\n        :class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\n\
  \        for big datasets (n_samples >= 10 000).\n    \n        This estimator has\
  \ native support for missing values (NaNs). During\n        training, the tree grower\
  \ learns at each split point whether samples\n        with missing values should\
  \ go to the left or right child, based on the\n        potential gain. When predicting,\
  \ samples with missing values are\n        assigned to the left or right child consequently.\
  \ If no missing values\n        were encountered for a given feature during training,\
  \ then samples with\n        missing values are mapped to whichever child has the\
  \ most samples.\n        See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`\
  \ for a\n        usecase example of this feature.\n    \n        This implementation\
  \ is inspired by\n        `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\
  \    \n        Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\
  \    \n        .. versionadded:: 0.21\n    \n        Parameters\n        ----------\n\
  \        loss : {'squared_error', 'absolute_error', 'gamma', 'poisson', 'quantile'},\
  \             default='squared_error'\n            The loss function to use in the\
  \ boosting process. Note that the\n            \"squared error\", \"gamma\" and\
  \ \"poisson\" losses actually implement\n            \"half least squares loss\"\
  , \"half gamma deviance\" and \"half poisson\n            deviance\" to simplify\
  \ the computation of the gradient. Furthermore,\n            \"gamma\" and \"poisson\"\
  \ losses internally use a log-link, \"gamma\"\n            requires ``y > 0`` and\
  \ \"poisson\" requires ``y >= 0``.\n            \"quantile\" uses the pinball loss.\n\
  \    \n            .. versionchanged:: 0.23\n               Added option 'poisson'.\n\
  \    \n            .. versionchanged:: 1.1\n               Added option 'quantile'.\n\
  \    \n            .. versionchanged:: 1.3\n               Added option 'gamma'.\n\
  \    \n        quantile : float, default=None\n            If loss is \"quantile\"\
  , this parameter specifies which quantile to be estimated\n            and must\
  \ be between 0 and 1.\n        learning_rate : float, default=0.1\n            The\
  \ learning rate, also known as *shrinkage*. This is used as a\n            multiplicative\
  \ factor for the leaves values. Use ``1`` for no\n            shrinkage.\n     \
  \   max_iter : int, default=100\n            The maximum number of iterations of\
  \ the boosting process, i.e. the\n            maximum number of trees.\n       \
  \ max_leaf_nodes : int or None, default=31\n            The maximum number of leaves\
  \ for each tree. Must be strictly greater\n            than 1. If None, there is\
  \ no maximum limit.\n        max_depth : int or None, default=None\n           \
  \ The maximum depth of each tree. The depth of a tree is the number of\n       \
  \     edges to go from the root to the deepest leaf.\n            Depth isn't constrained\
  \ by default.\n        min_samples_leaf : int, default=20\n            The minimum\
  \ number of samples per leaf. For small datasets with less\n            than a few\
  \ hundred samples, it is recommended to lower this value\n            since only\
  \ very shallow trees would be built.\n        l2_regularization : float, default=0\n\
  \            The L2 regularization parameter penalizing leaves with small hessians.\n\
  \            Use ``0`` for no regularization (default).\n        max_features :\
  \ float, default=1.0\n            Proportion of randomly chosen features in each\
  \ and every node split.\n            This is a form of regularization, smaller values\
  \ make the trees weaker\n            learners and might prevent overfitting.\n \
  \           If interaction constraints from `interaction_cst` are present, only\
  \ allowed\n            features are taken into account for the subsampling.\n  \
  \  \n            .. versionadded:: 1.4\n    \n        max_bins : int, default=255\n\
  \            The maximum number of bins to use for non-missing values. Before\n\
  \            training, each feature of the input array `X` is binned into\n    \
  \        integer-valued bins, which allows for a much faster training stage.\n \
  \           Features with a small number of unique values may use less than\n  \
  \          ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n\
  \            is always reserved for missing values. Must be no larger than 255.\n\
  \        categorical_features : array-like of {bool, int, str} of shape (n_features)\
  \             or shape (n_categorical_features,), default='from_dtype'\n       \
  \     Indicates the categorical features.\n    \n            - None : no feature\
  \ will be considered categorical.\n            - boolean array-like : boolean mask\
  \ indicating categorical features.\n            - integer array-like : integer indices\
  \ indicating categorical\n              features.\n            - str array-like:\
  \ names of categorical features (assuming the training\n              data has feature\
  \ names).\n            - `\"from_dtype\"`: dataframe columns with dtype \"category\"\
  \ are\n              considered to be categorical features. The input must be an\
  \ object\n              exposing a ``__dataframe__`` method such as pandas or polars\n\
  \              DataFrames to use this feature.\n    \n            For each categorical\
  \ feature, there must be at most `max_bins` unique\n            categories. Negative\
  \ values for categorical features encoded as numeric\n            dtypes are treated\
  \ as missing values. All categorical values are\n            converted to floating\
  \ point numbers. This means that categorical values\n            of 1.0 and 1 are\
  \ treated as the same category.\n    \n            Read more in the :ref:`User Guide\
  \ <categorical_support_gbdt>` and\n            :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`.\n\
  \    \n            .. versionadded:: 0.24\n    \n            .. versionchanged::\
  \ 1.2\n               Added support for feature names.\n    \n            .. versionchanged::\
  \ 1.4\n               Added `\"from_dtype\"` option.\n    \n            .. versionchanged::\
  \ 1.6\n               The default value changed from `None` to `\"from_dtype\"`.\n\
  \    \n        monotonic_cst : array-like of int of shape (n_features) or dict,\
  \ default=None\n            Monotonic constraint to enforce on each feature are\
  \ specified using the\n            following integer values:\n    \n           \
  \ - 1: monotonic increase\n            - 0: no constraint\n            - -1: monotonic\
  \ decrease\n    \n            If a dict with str keys, map feature to monotonic\
  \ constraints by name.\n            If an array, the features are mapped to constraints\
  \ by position. See\n            :ref:`monotonic_cst_features_names` for a usage\
  \ example.\n    \n            Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\
  \    \n            .. versionadded:: 0.23\n    \n            .. versionchanged::\
  \ 1.2\n               Accept dict of constraints with feature names as keys.\n \
  \   \n        interaction_cst : {\"pairwise\", \"no_interactions\"} or sequence\
  \ of lists/tuples/sets             of int, default=None\n            Specify interaction\
  \ constraints, the sets of features which can\n            interact with each other\
  \ in child node splits.\n    \n            Each item specifies the set of feature\
  \ indices that are allowed\n            to interact with each other. If there are\
  \ more features than\n            specified in these constraints, they are treated\
  \ as if they were\n            specified as an additional set.\n    \n         \
  \   The strings \"pairwise\" and \"no_interactions\" are shorthands for\n      \
  \      allowing only pairwise or no interactions, respectively.\n    \n        \
  \    For instance, with 5 features in total, `interaction_cst=[{0, 1}]`\n      \
  \      is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,\n            and\
  \ specifies that each branch of a tree will either only split\n            on features\
  \ 0 and 1 or only split on features 2, 3 and 4.\n    \n            See :ref:`this\
  \ example<ice-vs-pdp>` on how to use `interaction_cst`.\n    \n            .. versionadded::\
  \ 1.2\n    \n        warm_start : bool, default=False\n            When set to ``True``,\
  \ reuse the solution of the previous call to fit\n            and add more estimators\
  \ to the ensemble. For results to be valid, the\n            estimator should be\
  \ re-trained on the same data only.\n            See :term:`the Glossary <warm_start>`.\n\
  \        early_stopping : 'auto' or bool, default='auto'\n            If 'auto',\
  \ early stopping is enabled if the sample size is larger than\n            10000\
  \ or if `X_val` and `y_val` are passed to `fit`. If True, early stopping\n     \
  \       is enabled, otherwise early stopping is disabled.\n    \n            ..\
  \ versionadded:: 0.23\n    \n        scoring : str or callable or None, default='loss'\n\
  \            Scoring method to use for early stopping. Only used if `early_stopping`\n\
  \            is enabled. Options:\n    \n            - str: see :ref:`scoring_string_names`\
  \ for options.\n            - callable: a scorer callable object (e.g., function)\
  \ with signature\n              ``scorer(estimator, X, y)``. See :ref:`scoring_callable`\
  \ for details.\n            - `None`: the :ref:`coefficient of determination <r2_score>`\n\
  \              (:math:`R^2`) is used.\n            - 'loss': early stopping is checked\
  \ w.r.t the loss value.\n    \n        validation_fraction : int or float or None,\
  \ default=0.1\n            Proportion (or absolute size) of training data to set\
  \ aside as\n            validation data for early stopping. If None, early stopping\
  \ is done on\n            the training data.\n            The value is ignored if\
  \ either early stopping is not performed, e.g.\n            `early_stopping=False`,\
  \ or if `X_val` and `y_val` are passed to fit.\n        n_iter_no_change : int,\
  \ default=10\n            Used to determine when to \"early stop\". The fitting\
  \ process is\n            stopped when none of the last ``n_iter_no_change`` scores\
  \ are better\n            than the ``n_iter_no_change - 1`` -th-to-last one, up\
  \ to some\n            tolerance. Only used if early stopping is performed.\n  \
  \      tol : float, default=1e-7\n            The absolute tolerance to use when\
  \ comparing scores during early\n            stopping. The higher the tolerance,\
  \ the more likely we are to early\n            stop: higher tolerance means that\
  \ it will be harder for subsequent\n            iterations to be considered an improvement\
  \ upon the reference score.\n        verbose : int, default=0\n            The verbosity\
  \ level. If not zero, print some information about the\n            fitting process.\
  \ ``1`` prints only summary info, ``2`` prints info per\n            iteration.\n\
  \        random_state : int, RandomState instance or None, default=None\n      \
  \      Pseudo-random number generator to control the subsampling in the\n      \
  \      binning process, and the train/validation data split if early stopping\n\
  \            is enabled.\n            Pass an int for reproducible output across\
  \ multiple function calls.\n            See :term:`Glossary <random_state>`.\n \
  \   \n        Attributes\n        ----------\n        do_early_stopping_ : bool\n\
  \            Indicates whether early stopping is used during training.\n       \
  \ n_iter_ : int\n            The number of iterations as selected by early stopping,\
  \ depending on\n            the `early_stopping` parameter. Otherwise it corresponds\
  \ to max_iter.\n        n_trees_per_iteration_ : int\n            The number of\
  \ tree that are built at each iteration. For regressors,\n            this is always\
  \ 1.\n        train_score_ : ndarray, shape (n_iter_+1,)\n            The scores\
  \ at each iteration on the training data. The first entry\n            is the score\
  \ of the ensemble before the first iteration. Scores are\n            computed according\
  \ to the ``scoring`` parameter. If ``scoring`` is\n            not 'loss', scores\
  \ are computed on a subset of at most 10 000\n            samples. Empty if no early\
  \ stopping.\n        validation_score_ : ndarray, shape (n_iter_+1,)\n         \
  \   The scores at each iteration on the held-out validation data. The\n        \
  \    first entry is the score of the ensemble before the first iteration.\n    \
  \        Scores are computed according to the ``scoring`` parameter. Empty if\n\
  \            no early stopping or if ``validation_fraction`` is None.\n        is_categorical_\
  \ : ndarray, shape (n_features, ) or None\n            Boolean mask for the categorical\
  \ features. ``None`` if there are no\n            categorical features.\n      \
  \  n_features_in_ : int\n            Number of features seen during :term:`fit`.\n\
  \    \n            .. versionadded:: 0.24\n        feature_names_in_ : ndarray of\
  \ shape (`n_features_in_`,)\n            Names of features seen during :term:`fit`.\
  \ Defined only when `X`\n            has feature names that are all strings.\n \
  \   \n            .. versionadded:: 1.0\n    \n        See Also\n        --------\n\
  \        GradientBoostingRegressor : Exact gradient boosting method that does not\n\
  \            scale as good on datasets with a large number of samples.\n       \
  \ sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n        RandomForestRegressor\
  \ : A meta-estimator that fits a number of decision\n            tree regressors\
  \ on various sub-samples of the dataset and uses\n            averaging to improve\
  \ the statistical performance and control\n            over-fitting.\n        AdaBoostRegressor\
  \ : A meta-estimator that begins by fitting a regressor\n            on the original\
  \ dataset and then fits additional copies of the\n            regressor on the same\
  \ dataset but where the weights of instances are\n            adjusted according\
  \ to the error of the current prediction. As such,\n            subsequent regressors\
  \ focus more on difficult cases.\n    \n        Examples\n        --------\n   \
  \     >>> from sklearn.ensemble import HistGradientBoostingRegressor\n        >>>\
  \ from sklearn.datasets import load_diabetes\n        >>> X, y = load_diabetes(return_X_y=True)\n\
  \        >>> est = HistGradientBoostingRegressor().fit(X, y)\n        >>> est.score(X,\
  \ y)\n        0.92...\n        \n    \"\"\"\n\n    _parameter_constraints = \"{**BaseHistGradientBoosting._parameter_constraints,\
  \ 'loss': [StrOptions({'squared_error', 'absolute_error', 'poisson', 'gamma', 'quantile'}),\
  \ BaseLoss], 'quantile': [Interval(Real, 0, 1, closed='both'), None]}\"\n\n    def\
  \ __init__(\n        self,\n        loss = 'squared_error'\n    ):\n        \"\"\
  \"\n        Initialize a histogram-based gradient boosting estimator.\n\n      \
  \  This constructor sets up the base parameters for histogram-based gradient boosting\n\
  \        models used in both regression and classification tasks.\n\n        Parameters\n\
  \        ----------\n        loss : str or BaseLoss instance, default='squared_error'\n\
  \            The loss function to use in the boosting process. For regression tasks,\n\
  \            options include 'squared_error', 'absolute_error', 'gamma', 'poisson',\
  \ \n            and 'quantile'. For classification tasks, 'log_loss' is typically\
  \ used.\n            Can also be a custom BaseLoss instance for advanced use cases.\n\
  \n        Notes\n        -----\n        This is an abstract base class constructor\
  \ that should not be called directly.\n        Use HistGradientBoostingRegressor\
  \ or HistGradientBoostingClassifier instead.\n\n        The loss parameter is validated\
  \ during the fitting process, and the actual\n        loss object is created based\
  \ on the specific requirements of the task\n        (regression vs classification)\
  \ and any additional parameters like sample\n        weights or quantile values.\n\
  \        \"\"\"\n        <your code>\n\n    def predict(self, X):\n        \"\"\"\
  \n        Predict values for X.\n\n                Parameters\n                ----------\n\
  \                X : array-like, shape (n_samples, n_features)\n               \
  \     The input samples.\n\n                Returns\n                -------\n \
  \               y : ndarray, shape (n_samples,)\n                    The predicted\
  \ values.\n\n        \"\"\"\n        <your code>\n\n    def staged_predict(self,\
  \ X):\n        \"\"\"\n        Predict regression target for each iteration.\n\n\
  \                This method allows monitoring (i.e. determine error on testing\
  \ set)\n                after each stage.\n\n                .. versionadded:: 0.24\n\
  \n                Parameters\n                ----------\n                X : array-like\
  \ of shape (n_samples, n_features)\n                    The input samples.\n\n \
  \               Yields\n                ------\n                y : generator of\
  \ ndarray of shape (n_samples,)\n                    The predicted values of the\
  \ input samples, for each iteration.\n\n        \"\"\"\n        <your code>\n\n\
  \    def _encode_y(self, y):\n        \"\"\"\n        \"\"\"Encode target values\
  \ for training.\n\n        This method processes and validates the target values\
  \ `y` to prepare them for\n        the gradient boosting training process. It performs\
  \ necessary transformations\n        and sets up class-related attributes for the\
  \ estimator.\n\n        Parameters\n        ----------\n        y : array-like of\
  \ shape (n_samples,)\n            Target values to be encoded. For classification\
  \ tasks, these are the\n            class labels. For regression tasks, these are\
  \ the continuous target\n            values.\n\n        Returns\n        -------\n\
  \        encoded_y : ndarray of shape (n_samples,)\n            The encoded target\
  \ values. For regression, this is typically the input\n            array converted\
  \ to the appropriate dtype. For classification, this\n            contains the label-encoded\
  \ class indices.\n\n        Notes\n        -----\n        This is an abstract method\
  \ that must be implemented by subclasses. The\n        specific encoding behavior\
  \ depends on whether the estimator is used for\n        classification or regression:\n\
  \n        - For regression: Converts y to the expected dtype and validates constraints\n\
  \          specific to certain loss functions (e.g., gamma loss requires y > 0)\n\
  \        - For classification: Uses a LabelEncoder to convert class labels to integer\n\
  \          indices and sets up the classes_ attribute\n\n        The method also\
  \ sets the n_trees_per_iteration_ attribute, which determines\n        how many\
  \ trees are built at each boosting iteration (1 for regression and\n        binary\
  \ classification, n_classes for multiclass classification).\n\n        This method\
  \ is called during the fit process and is essential for preparing\n        the target\
  \ values in the format expected by the gradient boosting algorithm.\n        \"\"\
  \"\n        \"\"\"\n        <your code>\n\n    def _encode_y_val(self, y = None):\n\
  \        \"\"\"\n        Encode validation target values for internal use during\
  \ fitting.\n\n        This method transforms validation target values (y_val) to\
  \ the same encoded format\n        used internally by the gradient boosting algorithm.\
  \ For regressors, this typically\n        involves dtype conversion and validation.\
  \ For classifiers, this involves label\n        encoding to convert class labels\
  \ to integer indices.\n\n        Parameters\n        ----------\n        y : array-like\
  \ of shape (n_samples,), default=None\n            The validation target values\
  \ to encode. If None, no encoding is performed.\n            For regressors, these\
  \ should be continuous values. For classifiers, these\n            should be class\
  \ labels that were seen during the initial fit.\n\n        Returns\n        -------\n\
  \        encoded_y : ndarray of shape (n_samples,) or None\n            The encoded\
  \ validation target values with appropriate dtype for internal\n            computations.\
  \ Returns None if input y is None. For regressors, returns\n            y converted\
  \ to Y_DTYPE. For classifiers, returns integer-encoded labels\n            using\
  \ the same label encoder fitted during training.\n\n        Notes\n        -----\n\
  \        This method is called internally during fit when validation data is provided\n\
  \        via X_val and y_val parameters. It ensures that validation targets are\
  \ in\n        the same format as training targets for consistent evaluation during\
  \ early\n        stopping.\n\n        For classifiers, the validation labels must\
  \ contain only classes that were\n        seen during training, otherwise a ValueError\
  \ will be raised by the internal\n        label encoder.\n\n        For regressors\
  \ with specific loss functions (e.g., 'gamma', 'poisson'), the\n        same constraints\
  \ that apply to training targets also apply to validation\n        targets.\n  \
  \      \"\"\"\n        <your code>\n\n    def _get_loss(self, sample_weight):\n\
  \        \"\"\"\n        \"\"\"Get the appropriate loss function for the gradient\
  \ boosting model.\n\n        This method creates and returns a loss function instance\
  \ based on the model's\n        loss parameter and current configuration. The loss\
  \ function is used throughout\n        the boosting process to compute gradients\
  \ and hessians for tree fitting.\n\n        Parameters\n        ----------\n   \
  \     sample_weight : array-like of shape (n_samples,) or None\n            Sample\
  \ weights for the training data. If None, uniform weights are assumed.\n       \
  \     The loss function uses these weights to properly weight the contribution of\n\
  \            each sample during gradient and hessian computation.\n\n        Returns\n\
  \        -------\n        loss : BaseLoss\n            An instance of the appropriate\
  \ loss function class. The specific type depends\n            on the model's loss\
  \ parameter:\n\n            For HistGradientBoostingRegressor:\n            - Returns\
  \ PinballLoss for quantile regression (requires quantile parameter)\n          \
  \  - Returns corresponding loss class for other regression losses\n\n          \
  \  For HistGradientBoostingClassifier:\n            - Returns HalfBinomialLoss for\
  \ binary classification\n            - Returns HalfMultinomialLoss for multiclass\
  \ classification\n\n        Notes\n        -----\n        This is an abstract method\
  \ that must be implemented by subclasses. The method\n        is called during the\
  \ fit process to initialize the loss function with the\n        appropriate sample\
  \ weights.\n\n        The loss function instance is used to:\n        - Compute\
  \ initial baseline predictions\n        - Calculate gradients and hessians at each\
  \ boosting iteration\n        - Evaluate training and validation scores during early\
  \ stopping\n        - Transform raw predictions to final predictions\n\n       \
  \ For classification tasks, the number of trees per iteration (n_trees_per_iteration_)\n\
  \        must be set before calling this method, as it determines whether to use\
  \ binomial\n        or multinomial loss.\n\n        For regression tasks with quantile\
  \ loss, the quantile parameter must be properly\n        validated before calling\
  \ this method.\"\"\"\n        \"\"\"\n        <your code>\n\nclass HistGradientBoostingClassifier(ClassifierMixin,\
  \ BaseHistGradientBoosting):\n    \"\"\"\n    Histogram-based Gradient Boosting\
  \ Classification Tree.\n    \n        This estimator is much faster than\n     \
  \   :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n\
  \        for big datasets (n_samples >= 10 000).\n    \n        This estimator has\
  \ native support for missing values (NaNs). During\n        training, the tree grower\
  \ learns at each split point whether samples\n        with missing values should\
  \ go to the left or right child, based on the\n        potential gain. When predicting,\
  \ samples with missing values are\n        assigned to the left or right child consequently.\
  \ If no missing values\n        were encountered for a given feature during training,\
  \ then samples with\n        missing values are mapped to whichever child has the\
  \ most samples.\n    \n        This implementation is inspired by\n        `LightGBM\
  \ <https://github.com/Microsoft/LightGBM>`_.\n    \n        Read more in the :ref:`User\
  \ Guide <histogram_based_gradient_boosting>`.\n    \n        .. versionadded:: 0.21\n\
  \    \n        Parameters\n        ----------\n        loss : {'log_loss'}, default='log_loss'\n\
  \            The loss function to use in the boosting process.\n    \n         \
  \   For binary classification problems, 'log_loss' is also known as logistic loss,\n\
  \            binomial deviance or binary crossentropy. Internally, the model fits\
  \ one tree\n            per boosting iteration and uses the logistic sigmoid function\
  \ (expit) as\n            inverse link function to compute the predicted positive\
  \ class probability.\n    \n            For multiclass classification problems,\
  \ 'log_loss' is also known as multinomial\n            deviance or categorical crossentropy.\
  \ Internally, the model fits one tree per\n            boosting iteration and per\
  \ class and uses the softmax function as inverse link\n            function to compute\
  \ the predicted probabilities of the classes.\n    \n        learning_rate : float,\
  \ default=0.1\n            The learning rate, also known as *shrinkage*. This is\
  \ used as a\n            multiplicative factor for the leaves values. Use ``1``\
  \ for no\n            shrinkage.\n        max_iter : int, default=100\n        \
  \    The maximum number of iterations of the boosting process, i.e. the\n      \
  \      maximum number of trees for binary classification. For multiclass\n     \
  \       classification, `n_classes` trees per iteration are built.\n        max_leaf_nodes\
  \ : int or None, default=31\n            The maximum number of leaves for each tree.\
  \ Must be strictly greater\n            than 1. If None, there is no maximum limit.\n\
  \        max_depth : int or None, default=None\n            The maximum depth of\
  \ each tree. The depth of a tree is the number of\n            edges to go from\
  \ the root to the deepest leaf.\n            Depth isn't constrained by default.\n\
  \        min_samples_leaf : int, default=20\n            The minimum number of samples\
  \ per leaf. For small datasets with less\n            than a few hundred samples,\
  \ it is recommended to lower this value\n            since only very shallow trees\
  \ would be built.\n        l2_regularization : float, default=0\n            The\
  \ L2 regularization parameter penalizing leaves with small hessians.\n         \
  \   Use ``0`` for no regularization (default).\n        max_features : float, default=1.0\n\
  \            Proportion of randomly chosen features in each and every node split.\n\
  \            This is a form of regularization, smaller values make the trees weaker\n\
  \            learners and might prevent overfitting.\n            If interaction\
  \ constraints from `interaction_cst` are present, only allowed\n            features\
  \ are taken into account for the subsampling.\n    \n            .. versionadded::\
  \ 1.4\n    \n        max_bins : int, default=255\n            The maximum number\
  \ of bins to use for non-missing values. Before\n            training, each feature\
  \ of the input array `X` is binned into\n            integer-valued bins, which\
  \ allows for a much faster training stage.\n            Features with a small number\
  \ of unique values may use less than\n            ``max_bins`` bins. In addition\
  \ to the ``max_bins`` bins, one more bin\n            is always reserved for missing\
  \ values. Must be no larger than 255.\n        categorical_features : array-like\
  \ of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,),\
  \ default='from_dtype'\n            Indicates the categorical features.\n    \n\
  \            - None : no feature will be considered categorical.\n            -\
  \ boolean array-like : boolean mask indicating categorical features.\n         \
  \   - integer array-like : integer indices indicating categorical\n            \
  \  features.\n            - str array-like: names of categorical features (assuming\
  \ the training\n              data has feature names).\n            - `\"from_dtype\"\
  `: dataframe columns with dtype \"category\" are\n              considered to be\
  \ categorical features. The input must be an object\n              exposing a ``__dataframe__``\
  \ method such as pandas or polars\n              DataFrames to use this feature.\n\
  \    \n            For each categorical feature, there must be at most `max_bins`\
  \ unique\n            categories. Negative values for categorical features encoded\
  \ as numeric\n            dtypes are treated as missing values. All categorical\
  \ values are\n            converted to floating point numbers. This means that categorical\
  \ values\n            of 1.0 and 1 are treated as the same category.\n    \n   \
  \         Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n    \n\
  \            .. versionadded:: 0.24\n    \n            .. versionchanged:: 1.2\n\
  \               Added support for feature names.\n    \n            .. versionchanged::\
  \ 1.4\n               Added `\"from_dtype\"` option.\n    \n            .. versionchanged::\
  \ 1.6\n               The default value changed from `None` to `\"from_dtype\"`.\n\
  \    \n        monotonic_cst : array-like of int of shape (n_features) or dict,\
  \ default=None\n            Monotonic constraint to enforce on each feature are\
  \ specified using the\n            following integer values:\n    \n           \
  \ - 1: monotonic increase\n            - 0: no constraint\n            - -1: monotonic\
  \ decrease\n    \n            If a dict with str keys, map feature to monotonic\
  \ constraints by name.\n            If an array, the features are mapped to constraints\
  \ by position. See\n            :ref:`monotonic_cst_features_names` for a usage\
  \ example.\n    \n            The constraints are only valid for binary classifications\
  \ and hold\n            over the probability of the positive class.\n          \
  \  Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n    \n            ..\
  \ versionadded:: 0.23\n    \n            .. versionchanged:: 1.2\n             \
  \  Accept dict of constraints with feature names as keys.\n    \n        interaction_cst\
  \ : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets       \
  \      of int, default=None\n            Specify interaction constraints, the sets\
  \ of features which can\n            interact with each other in child node splits.\n\
  \    \n            Each item specifies the set of feature indices that are allowed\n\
  \            to interact with each other. If there are more features than\n    \
  \        specified in these constraints, they are treated as if they were\n    \
  \        specified as an additional set.\n    \n            The strings \"pairwise\"\
  \ and \"no_interactions\" are shorthands for\n            allowing only pairwise\
  \ or no interactions, respectively.\n    \n            For instance, with 5 features\
  \ in total, `interaction_cst=[{0, 1}]`\n            is equivalent to `interaction_cst=[{0,\
  \ 1}, {2, 3, 4}]`,\n            and specifies that each branch of a tree will either\
  \ only split\n            on features 0 and 1 or only split on features 2, 3 and\
  \ 4.\n    \n            See :ref:`this example<ice-vs-pdp>` on how to use `interaction_cst`.\n\
  \    \n            .. versionadded:: 1.2\n    \n        warm_start : bool, default=False\n\
  \            When set to ``True``, reuse the solution of the previous call to fit\n\
  \            and add more estimators to the ensemble. For results to be valid, the\n\
  \            estimator should be re-trained on the same data only.\n           \
  \ See :term:`the Glossary <warm_start>`.\n        early_stopping : 'auto' or bool,\
  \ default='auto'\n            If 'auto', early stopping is enabled if the sample\
  \ size is larger than\n            10000 or if `X_val` and `y_val` are passed to\
  \ `fit`. If True, early stopping\n            is enabled, otherwise early stopping\
  \ is disabled.\n    \n            .. versionadded:: 0.23\n    \n        scoring\
  \ : str or callable or None, default='loss'\n            Scoring method to use for\
  \ early stopping. Only used if `early_stopping`\n            is enabled. Options:\n\
  \    \n            - str: see :ref:`scoring_string_names` for options.\n       \
  \     - callable: a scorer callable object (e.g., function) with signature\n   \
  \           ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n\
  \            - `None`: :ref:`accuracy <accuracy_score>` is used.\n            -\
  \ 'loss': early stopping is checked w.r.t the loss value.\n    \n        validation_fraction\
  \ : int or float or None, default=0.1\n            Proportion (or absolute size)\
  \ of training data to set aside as\n            validation data for early stopping.\
  \ If None, early stopping is done on\n            the training data.\n         \
  \   The value is ignored if either early stopping is not performed, e.g.\n     \
  \       `early_stopping=False`, or if `X_val` and `y_val` are passed to fit.\n \
  \       n_iter_no_change : int, default=10\n            Used to determine when to\
  \ \"early stop\". The fitting process is\n            stopped when none of the last\
  \ ``n_iter_no_change`` scores are better\n            than the ``n_iter_no_change\
  \ - 1`` -th-to-last one, up to some\n            tolerance. Only used if early stopping\
  \ is performed.\n        tol : float, default=1e-7\n            The absolute tolerance\
  \ to use when comparing scores. The higher the\n            tolerance, the more\
  \ likely we are to early stop: higher tolerance\n            means that it will\
  \ be harder for subsequent iterations to be\n            considered an improvement\
  \ upon the reference score.\n        verbose : int, default=0\n            The verbosity\
  \ level. If not zero, print some information about the\n            fitting process.\
  \ ``1`` prints only summary info, ``2`` prints info per\n            iteration.\n\
  \        random_state : int, RandomState instance or None, default=None\n      \
  \      Pseudo-random number generator to control the subsampling in the\n      \
  \      binning process, and the train/validation data split if early stopping\n\
  \            is enabled.\n            Pass an int for reproducible output across\
  \ multiple function calls.\n            See :term:`Glossary <random_state>`.\n \
  \       class_weight : dict or 'balanced', default=None\n            Weights associated\
  \ with classes in the form `{class_label: weight}`.\n            If not given, all\
  \ classes are supposed to have weight one.\n            The \"balanced\" mode uses\
  \ the values of y to automatically adjust\n            weights inversely proportional\
  \ to class frequencies in the input data\n            as `n_samples / (n_classes\
  \ * np.bincount(y))`.\n            Note that these weights will be multiplied with\
  \ sample_weight (passed\n            through the fit method) if `sample_weight`\
  \ is specified.\n    \n            .. versionadded:: 1.2\n    \n        Attributes\n\
  \        ----------\n        classes_ : array, shape = (n_classes,)\n          \
  \  Class labels.\n        do_early_stopping_ : bool\n            Indicates whether\
  \ early stopping is used during training.\n        n_iter_ : int\n            The\
  \ number of iterations as selected by early stopping, depending on\n           \
  \ the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n      \
  \  n_trees_per_iteration_ : int\n            The number of tree that are built at\
  \ each iteration. This is equal to 1\n            for binary classification, and\
  \ to ``n_classes`` for multiclass\n            classification.\n        train_score_\
  \ : ndarray, shape (n_iter_+1,)\n            The scores at each iteration on the\
  \ training data. The first entry\n            is the score of the ensemble before\
  \ the first iteration. Scores are\n            computed according to the ``scoring``\
  \ parameter. If ``scoring`` is\n            not 'loss', scores are computed on a\
  \ subset of at most 10 000\n            samples. Empty if no early stopping.\n \
  \       validation_score_ : ndarray, shape (n_iter_+1,)\n            The scores\
  \ at each iteration on the held-out validation data. The\n            first entry\
  \ is the score of the ensemble before the first iteration.\n            Scores are\
  \ computed according to the ``scoring`` parameter. Empty if\n            no early\
  \ stopping or if ``validation_fraction`` is None.\n        is_categorical_ : ndarray,\
  \ shape (n_features, ) or None\n            Boolean mask for the categorical features.\
  \ ``None`` if there are no\n            categorical features.\n        n_features_in_\
  \ : int\n            Number of features seen during :term:`fit`.\n    \n       \
  \     .. versionadded:: 0.24\n        feature_names_in_ : ndarray of shape (`n_features_in_`,)\n\
  \            Names of features seen during :term:`fit`. Defined only when `X`\n\
  \            has feature names that are all strings.\n    \n            .. versionadded::\
  \ 1.0\n    \n        See Also\n        --------\n        GradientBoostingClassifier\
  \ : Exact gradient boosting method that does not\n            scale as good on datasets\
  \ with a large number of samples.\n        sklearn.tree.DecisionTreeClassifier :\
  \ A decision tree classifier.\n        RandomForestClassifier : A meta-estimator\
  \ that fits a number of decision\n            tree classifiers on various sub-samples\
  \ of the dataset and uses\n            averaging to improve the predictive accuracy\
  \ and control over-fitting.\n        AdaBoostClassifier : A meta-estimator that\
  \ begins by fitting a classifier\n            on the original dataset and then fits\
  \ additional copies of the\n            classifier on the same dataset where the\
  \ weights of incorrectly\n            classified instances are adjusted such that\
  \ subsequent classifiers\n            focus more on difficult cases.\n    \n   \
  \     Examples\n        --------\n        >>> from sklearn.ensemble import HistGradientBoostingClassifier\n\
  \        >>> from sklearn.datasets import load_iris\n        >>> X, y = load_iris(return_X_y=True)\n\
  \        >>> clf = HistGradientBoostingClassifier().fit(X, y)\n        >>> clf.score(X,\
  \ y)\n        1.0\n        \n    \"\"\"\n\n    _parameter_constraints = \"{**BaseHistGradientBoosting._parameter_constraints,\
  \ 'loss': [StrOptions({'log_loss'}), BaseLoss], 'class_weight': [dict, StrOptions({'balanced'}),\
  \ None]}\"\n\n    def __init__(self, loss = 'log_loss'):\n        \"\"\"\n     \
  \   Initialize a histogram-based gradient boosting classifier.\n\n        This constructor\
  \ sets up the basic parameters for the HistGradientBoostingClassifier,\n       \
  \ which implements a fast gradient boosting algorithm using histogram-based tree\
  \ growing.\n\n        Parameters\n        ----------\n        loss : {'log_loss'}\
  \ or BaseLoss instance, default='log_loss'\n            The loss function to use\
  \ in the boosting process. For binary classification,\n            'log_loss' uses\
  \ logistic loss (binomial deviance). For multiclass classification,\n          \
  \  'log_loss' uses multinomial deviance (categorical crossentropy). Custom loss\n\
  \            functions can be provided as BaseLoss instances.\n\n        learning_rate\
  \ : float, default=0.1\n            The learning rate (shrinkage factor) applied\
  \ to each tree's contribution.\n            Lower values make the model more robust\
  \ but require more iterations.\n            Must be greater than 0.\n\n        max_iter\
  \ : int, default=100\n            Maximum number of boosting iterations. For binary\
  \ classification, this is\n            the number of trees. For multiclass, n_classes\
  \ trees are built per iteration.\n            Must be at least 1.\n\n        max_leaf_nodes\
  \ : int or None, default=31\n            Maximum number of leaves per tree. Must\
  \ be greater than 1 if specified.\n            If None, no limit is imposed on the\
  \ number of leaves.\n\n        max_depth : int or None, default=None\n         \
  \   Maximum depth of each tree (number of edges from root to deepest leaf).\n  \
  \          If None, depth is not constrained.\n\n        min_samples_leaf : int,\
  \ default=20\n            Minimum number of samples required in each leaf node.\
  \ For small datasets,\n            consider lowering this value to allow deeper\
  \ trees.\n\n        l2_regularization : float, default=0.0\n            L2 regularization\
  \ parameter for leaf values. Higher values provide more\n            regularization.\
  \ Must be non-negative.\n\n        max_features : float, default=1.0\n         \
  \   Proportion of features to consider at each split (0 < max_features <= 1).\n\
  \            Provides regularization by introducing randomness in feature selection.\n\
  \n        max_bins : int, default=255\n            Maximum number of bins for discretizing\
  \ continuous features. Must be\n            between 2 and 255. One additional bin\
  \ is reserved for missing values.\n\n        categorical_features : array-like,\
  \ str or None, default='from_dtype'\n            Specification of categorical features.\
  \ Can be boolean mask, integer indices,\n            feature names, or 'from_dtype'\
  \ to auto-detect from pandas categorical columns.\n\n        monotonic_cst : array-like,\
  \ dict or None, default=None\n            Monotonic constraints for features. Values:\
  \ 1 (increasing), 0 (no constraint),\n            -1 (decreasing). Only supported\
  \ for binary classification.\n\n        interaction_cst : sequence, str or None,\
  \ default=None\n            Interaction constraints specifying which features can\
  \ interact in splits.\n            Can be 'pairwise', 'no_interactions', or custom\
  \ feature groupings.\n\n        warm_start : bool, default=False\n            Whether\
  \ to reuse the previous fit solution and add more estimators.\n            Requires\
  \ fitting on the same data.\n\n        early_stopping : 'auto' or bool, default='auto'\n\
  \            Whether to use early stopping. 'auto' enables it for large datasets\n\
  \            (n_samples > 10000) or when validation data is provided.\n\n      \
  \  scoring : str, callable or None, default='loss'\n            Scoring method for\
  \ early stopping. Can be a scorer name, callable,\n            None (uses accuracy),\
  \ or 'loss' (uses the loss function).\n\n        validation_fraction : float, int\
  \ or None, default=0.1\n            Fraction or absolute size of training data for\
  \ validation in early stopping.\n            Ignored if validation data is explicitly\
  \ provided or early stopping is disabled.\n\n        n_iter_no_change : int, default=10\n\
  \            Number of iterations with no improvement to trigger early stopping.\n\
  \            Must be at least 1.\n\n        tol : float, default=1e-7\n        \
  \    Tolerance for early stopping. Higher values make early stopping more likely.\n\
  \            Must be non-negative.\n\n        verbose : int, default=0\n       \
  \     Verbosity level. 0 for silent, 1 for summary info, 2 for detailed iteration\
  \ info.\n\n        random_state : int, RandomState or None, default=None\n     \
  \       Random state for reproducible results. Controls binning randomness and\n\
  \            train/validation splits.\n\n        class_weight : dict, 'balanced'\
  \ or None, default=None\n            Weights for classes. 'balanced' uses inverse\
  \ class frequencies.\n            Custom weights as {class_label: weight} dictionary.\n\
  \n        Notes\n        -----\n        This initializer only sets the parameters.\
  \ The actual model training occurs\n        when the fit method is called. The classifier\
  \ supports both binary and\n        multiclass classification automatically based\
  \ on the target variable.\n\n        For binary classification, one tree per iteration\
  \ is built. For multiclass\n        classification, one tree per class per iteration\
  \ is built.\n        \"\"\"\n        <your code>\n\n    def _finalize_sample_weight(self,\
  \ sample_weight, y):\n        \"\"\"\n        Adjust sample_weights with class_weights.\n\
  \        \"\"\"\n        <your code>\n\n    def predict(self, X):\n        \"\"\"\
  \n        Predict classes for X.\n\n                Parameters\n               \
  \ ----------\n                X : array-like, shape (n_samples, n_features)\n  \
  \                  The input samples.\n\n                Returns\n             \
  \   -------\n                y : ndarray, shape (n_samples,)\n                 \
  \   The predicted classes.\n\n        \"\"\"\n        <your code>\n\n    def staged_predict(self,\
  \ X):\n        \"\"\"\n        Predict classes at each iteration.\n\n          \
  \      This method allows monitoring (i.e. determine error on testing set)\n   \
  \             after each stage.\n\n                .. versionadded:: 0.24\n\n  \
  \              Parameters\n                ----------\n                X : array-like\
  \ of shape (n_samples, n_features)\n                    The input samples.\n\n \
  \               Yields\n                ------\n                y : generator of\
  \ ndarray of shape (n_samples,)\n                    The predicted classes of the\
  \ input samples, for each iteration.\n\n        \"\"\"\n        <your code>\n\n\
  \    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities\
  \ for X.\n\n                Parameters\n                ----------\n           \
  \     X : array-like, shape (n_samples, n_features)\n                    The input\
  \ samples.\n\n                Returns\n                -------\n               \
  \ p : ndarray, shape (n_samples, n_classes)\n                    The class probabilities\
  \ of the input samples.\n\n        \"\"\"\n        <your code>\n\n    def staged_predict_proba(self,\
  \ X):\n        \"\"\"\n        Predict class probabilities at each iteration.\n\n\
  \                This method allows monitoring (i.e. determine error on testing\
  \ set)\n                after each stage.\n\n                Parameters\n      \
  \          ----------\n                X : array-like of shape (n_samples, n_features)\n\
  \                    The input samples.\n\n                Yields\n            \
  \    ------\n                y : generator of ndarray of shape (n_samples,)\n  \
  \                  The predicted class probabilities of the input samples,\n   \
  \                 for each iteration.\n\n        \"\"\"\n        <your code>\n\n\
  \    def decision_function(self, X):\n        \"\"\"\n        Compute the decision\
  \ function of ``X``.\n\n                Parameters\n                ----------\n\
  \                X : array-like, shape (n_samples, n_features)\n               \
  \     The input samples.\n\n                Returns\n                -------\n \
  \               decision : ndarray, shape (n_samples,) or                 (n_samples,\
  \ n_trees_per_iteration)\n                    The raw predicted values (i.e. the\
  \ sum of the trees leaves) for\n                    each sample. n_trees_per_iteration\
  \ is equal to the number of\n                    classes in multiclass classification.\n\
  \n        \"\"\"\n        <your code>\n\n    def staged_decision_function(self,\
  \ X):\n        \"\"\"\n        Compute decision function of ``X`` for each iteration.\n\
  \n                This method allows monitoring (i.e. determine error on testing\
  \ set)\n                after each stage.\n\n                Parameters\n      \
  \          ----------\n                X : array-like of shape (n_samples, n_features)\n\
  \                    The input samples.\n\n                Yields\n            \
  \    ------\n                decision : generator of ndarray of shape (n_samples,)\
  \ or                 (n_samples, n_trees_per_iteration)\n                    The\
  \ decision function of the input samples, which corresponds to\n               \
  \     the raw values predicted from the trees of the ensemble . The\n          \
  \          classes corresponds to that in the attribute :term:`classes_`.\n\n  \
  \      \"\"\"\n        <your code>\n\n    def _encode_y(self, y):\n        \"\"\"\
  \n        Create self._label_encoder and encode y correspondingly.\n        \"\"\
  \"\n        <your code>\n\n    def _encode_y_val(self, y):\n        \"\"\"\n   \
  \     Encode validation target values for internal use during fitting.\n\n     \
  \   This method transforms validation target values (y_val) to the same encoded\
  \ format\n        used internally by the gradient boosting algorithm. For regressors,\
  \ this typically\n        involves dtype conversion and validation. For classifiers,\
  \ this involves label\n        encoding to convert class labels to integer indices.\n\
  \n        Parameters\n        ----------\n        y : array-like of shape (n_samples,),\
  \ default=None\n            The validation target values to encode. These should\
  \ be in the same format\n            as the original training targets passed to\
  \ fit().\n\n        Returns\n        -------\n        encoded_y : ndarray of shape\
  \ (n_samples,)\n            The encoded validation target values. For regressors,\
  \ this is typically\n            the input array converted to the appropriate dtype\
  \ (Y_DTYPE). For classifiers,\n            this is the label-encoded version where\
  \ class labels are converted to\n            integer indices corresponding to the\
  \ classes learned during training.\n\n        Notes\n        -----\n        This\
  \ method is called internally during the fit process when validation data\n    \
  \    is provided (either through validation_fraction parameter or explicit X_val/y_val\n\
  \        parameters). It ensures that validation targets are in the same format\
  \ as the\n        training targets for consistent evaluation during early stopping.\n\
  \n        For classifiers, this method uses the label encoder fitted on the training\
  \ data\n        to transform validation labels, ensuring consistency between training\
  \ and\n        validation label encoding.\n\n        For regressors, this method\
  \ applies the same validation and preprocessing\n        steps as _encode_y to ensure\
  \ validation targets meet the same requirements\n        as training targets (e.g.,\
  \ positivity constraints for gamma loss).\n        \"\"\"\n        <your code>\n\
  \n    def _get_loss(self, sample_weight):\n        \"\"\"\n        \"\"\"Get the\
  \ appropriate loss function for the gradient boosting model.\n\n        This method\
  \ creates and returns a loss function instance based on the model's\n        loss\
  \ parameter and current configuration. The loss function is used throughout\n  \
  \      the boosting process to compute gradients and hessians for tree fitting.\n\
  \n        Parameters\n        ----------\n        sample_weight : array-like of\
  \ shape (n_samples,) or None\n            Sample weights for the training data.\
  \ If None, uniform weights are assumed.\n            The loss function uses these\
  \ weights to properly weight the contribution of\n            each sample during\
  \ gradient and hessian computation.\n\n        Returns\n        -------\n      \
  \  loss : BaseLoss\n            An instance of the appropriate loss function class.\
  \ The specific type depends\n            on the model's loss parameter:\n\n    \
  \        - For regression: Returns instances like HalfSquaredError, HalfPoissonLoss,\n\
  \              HalfGammaLoss, or PinballLoss based on the loss string parameter\n\
  \            - For classification: Returns HalfBinomialLoss for binary classification\n\
  \              or HalfMultinomialLoss for multiclass classification\n\n        \
  \    The returned loss instance is configured with the provided sample_weight\n\
  \            and any additional parameters specific to the loss type (e.g., quantile\n\
  \            parameter for PinballLoss).\n\n        Notes\n        -----\n     \
  \   This is an abstract method that must be implemented by subclasses. The\n   \
  \     implementation varies between HistGradientBoostingRegressor and\n        HistGradientBoostingClassifier\
  \ to handle their respective loss functions\n        and requirements.\n\n     \
  \   The loss function instance is used internally during the boosting process\n\
  \        to compute gradients and hessians, evaluate training/validation scores,\n\
  \        and make predictions through the inverse link function.\n\n        For\
  \ custom loss functions passed as BaseLoss instances rather than strings,\n    \
  \    this method should return the loss instance directly after any necessary\n\
  \        configuration.\"\"\"\n        \"\"\"\n        <your code>\n"
interface_description2: 'Below is **Interface Description 2** for file: sklearn-ensemble-_hist_gradient_boosting-grower.py


  This file contains 1 top-level interface(s) that need to be implemented.

  '
interface_code2: "class TreeGrower:\n    \"\"\"\n    Tree grower class used to build\
  \ a tree.\n    \n        The tree is fitted to predict the values of a Newton-Raphson\
  \ step. The\n        splits are considered in a best-first fashion, and the quality\
  \ of a\n        split is defined in splitting._split_gain.\n    \n        Parameters\n\
  \        ----------\n        X_binned : ndarray of shape (n_samples, n_features),\
  \ dtype=np.uint8\n            The binned input samples. Must be Fortran-aligned.\n\
  \        gradients : ndarray of shape (n_samples,)\n            The gradients of\
  \ each training sample. Those are the gradients of the\n            loss w.r.t the\
  \ predictions, evaluated at iteration ``i - 1``.\n        hessians : ndarray of\
  \ shape (n_samples,)\n            The hessians of each training sample. Those are\
  \ the hessians of the\n            loss w.r.t the predictions, evaluated at iteration\
  \ ``i - 1``.\n        max_leaf_nodes : int, default=None\n            The maximum\
  \ number of leaves for each tree. If None, there is no\n            maximum limit.\n\
  \        max_depth : int, default=None\n            The maximum depth of each tree.\
  \ The depth of a tree is the number of\n            edges to go from the root to\
  \ the deepest leaf.\n            Depth isn't constrained by default.\n        min_samples_leaf\
  \ : int, default=20\n            The minimum number of samples per leaf.\n     \
  \   min_gain_to_split : float, default=0.\n            The minimum gain needed to\
  \ split a node. Splits with lower gain will\n            be ignored.\n        min_hessian_to_split\
  \ : float, default=1e-3\n            The minimum sum of hessians needed in each\
  \ node. Splits that result in\n            at least one child having a sum of hessians\
  \ less than\n            ``min_hessian_to_split`` are discarded.\n        n_bins\
  \ : int, default=256\n            The total number of bins, including the bin for\
  \ missing values. Used\n            to define the shape of the histograms.\n   \
  \     n_bins_non_missing : ndarray, dtype=np.uint32, default=None\n            For\
  \ each feature, gives the number of bins actually used for\n            non-missing\
  \ values. For features with a lot of unique values, this\n            is equal to\
  \ ``n_bins - 1``. If it's an int, all features are\n            considered to have\
  \ the same number of bins. If None, all features\n            are considered to\
  \ have ``n_bins - 1`` bins.\n        has_missing_values : bool or ndarray, dtype=bool,\
  \ default=False\n            Whether each feature contains missing values (in the\
  \ training data).\n            If it's a bool, the same value is used for all features.\n\
  \        is_categorical : ndarray of bool of shape (n_features,), default=None\n\
  \            Indicates categorical features.\n        monotonic_cst : array-like\
  \ of int of shape (n_features,), dtype=int, default=None\n            Indicates\
  \ the monotonic constraint to enforce on each feature.\n              - 1: monotonic\
  \ increase\n              - 0: no constraint\n              - -1: monotonic decrease\n\
  \    \n            Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n  \
  \      interaction_cst : list of sets of integers, default=None\n            List\
  \ of interaction constraints.\n        l2_regularization : float, default=0.\n \
  \           The L2 regularization parameter penalizing leaves with small hessians.\n\
  \            Use ``0`` for no regularization (default).\n        feature_fraction_per_split\
  \ : float, default=1\n            Proportion of randomly chosen features in each\
  \ and every node split.\n            This is a form of regularization, smaller values\
  \ make the trees weaker\n            learners and might prevent overfitting.\n \
  \       rng : Generator\n            Numpy random Generator used for feature subsampling.\n\
  \        shrinkage : float, default=1.\n            The shrinkage parameter to apply\
  \ to the leaves values, also known as\n            learning rate.\n        n_threads\
  \ : int, default=None\n            Number of OpenMP threads to use. `_openmp_effective_n_threads`\
  \ is called\n            to determine the effective number of threads use, which\
  \ takes cgroups CPU\n            quotes into account. See the docstring of `_openmp_effective_n_threads`\n\
  \            for details.\n    \n        Attributes\n        ----------\n      \
  \  histogram_builder : HistogramBuilder\n        splitter : Splitter\n        root\
  \ : TreeNode\n        finalized_leaves : list of TreeNode\n        splittable_nodes\
  \ : list of TreeNode\n        missing_values_bin_idx : int\n            Equals n_bins\
  \ - 1\n        n_categorical_splits : int\n        n_features : int\n        n_nodes\
  \ : int\n        total_find_split_time : float\n            Time spent finding the\
  \ best splits\n        total_compute_hist_time : float\n            Time spent computing\
  \ histograms\n        total_apply_split_time : float\n            Time spent splitting\
  \ nodes\n        with_monotonic_cst : bool\n            Whether there are monotonic\
  \ constraints that apply. False iff monotonic_cst is\n            None.\n      \
  \  \n    \"\"\"\n\n    def __init__(\n        self,\n        X_binned,\n       \
  \ gradients,\n        hessians,\n        max_leaf_nodes = None,\n        max_depth\
  \ = None,\n        min_samples_leaf = 20,\n        min_gain_to_split = 0.0,\n  \
  \      min_hessian_to_split = 0.001,\n        n_bins = 256,\n        n_bins_non_missing\
  \ = None,\n        has_missing_values = False,\n        is_categorical = None,\n\
  \        monotonic_cst = None,\n        interaction_cst = None,\n        l2_regularization\
  \ = 0.0,\n        feature_fraction_per_split = 1.0,\n        rng = np.random.default_rng(),\n\
  \        shrinkage = 1.0,\n        n_threads = None\n    ):\n        \"\"\"\n  \
  \      Initialize a TreeGrower instance for building gradient boosting regression\
  \ trees.\n\n        TreeGrower constructs regression trees by fitting Newton-Raphson\
  \ steps based on\n        gradients and hessians of training samples. It uses a\
  \ best-first splitting strategy\n        where split quality is determined by gain\
  \ calculations in the splitting module.\n\n        Parameters\n        ----------\n\
  \        X_binned : ndarray of shape (n_samples, n_features), dtype=np.uint8\n \
  \           The binned input training samples. Must be Fortran-aligned for optimal\n\
  \            performance. Each feature's values should be discretized into bins.\n\
  \        gradients : ndarray of shape (n_samples,)\n            The gradients of\
  \ the loss function with respect to predictions, evaluated\n            at the previous\
  \ boosting iteration (i-1). Used to determine split directions.\n        hessians\
  \ : ndarray of shape (n_samples,)\n            The second derivatives (hessians)\
  \ of the loss function with respect to\n            predictions, evaluated at the\
  \ previous boosting iteration (i-1). Used for\n            Newton-Raphson step calculations.\n\
  \        max_leaf_nodes : int, default=None\n            Maximum number of leaf\
  \ nodes allowed in the tree. If None, no limit is\n            imposed on the number\
  \ of leaves.\n        max_depth : int, default=None\n            Maximum depth of\
  \ the tree, measured as the number of edges from root to\n            the deepest\
  \ leaf. If None, depth is not constrained.\n        min_samples_leaf : int, default=20\n\
  \            Minimum number of samples required to form a leaf node. Splits resulting\n\
  \            in leaves with fewer samples are rejected.\n        min_gain_to_split\
  \ : float, default=0.0\n            Minimum improvement in loss reduction required\
  \ to make a split. Splits\n            with gain below this threshold are discarded.\n\
  \        min_hessian_to_split : float, default=0.001\n            Minimum sum of\
  \ hessians required in a node to consider splitting it.\n            Helps prevent\
  \ splits on nodes with very low confidence.\n        n_bins : int, default=256\n\
  \            Total number of bins used for feature discretization, including the\n\
  \            special bin reserved for missing values.\n        n_bins_non_missing\
  \ : ndarray of shape (n_features,) or int, default=None\n            Number of bins\
  \ actually used for non-missing values per feature. If int,\n            same value\
  \ applies to all features. If None, defaults to n_bins - 1 for\n            all\
  \ features.\n        has_missing_values : bool or ndarray of shape (n_features,),\
  \ default=False\n            Indicates whether each feature contains missing values\
  \ in training data.\n            If bool, same value applies to all features.\n\
  \        is_categorical : ndarray of shape (n_features,), dtype=bool, default=None\n\
  \            Boolean array indicating which features are categorical. If None, all\n\
  \            features are treated as numerical.\n        monotonic_cst : array-like\
  \ of shape (n_features,), dtype=int, default=None\n            Monotonic constraints\
  \ for each feature. Values: 1 (increasing), 0 (no\n            constraint), -1 (decreasing).\
  \ If None, no constraints are applied.\n        interaction_cst : list of sets of\
  \ int, default=None\n            Interaction constraints specifying which features\
  \ can be used together\n            in the same path from root to leaf. Each set\
  \ contains feature indices\n            that can interact.\n        l2_regularization\
  \ : float, default=0.0\n            L2 regularization parameter applied to leaf\
  \ values. Higher values\n            penalize leaves with small hessians, helping\
  \ prevent overfitting.\n        feature_fraction_per_split : float, default=1.0\n\
  \            Fraction of features to randomly consider at each split. Values less\n\
  \            than 1.0 provide regularization by making trees weaker learners.\n\
  \        rng : numpy.random.Generator, default=np.random.default_rng()\n       \
  \     Random number generator used for feature subsampling when\n            feature_fraction_per_split\
  \ < 1.0.\n        shrinkage : float, default=1.0\n            Learning rate applied\
  \ to leaf values. Also known as the shrinkage\n            parameter in gradient\
  \ boosting literature.\n        n_threads : int, default=None\n            Number\
  \ of OpenMP threads for parallel computation. If None, determined\n            automatically\
  \ considering system resources and cgroup constraints.\n\n        Raises\n     \
  \   ------\n        NotImplementedError\n            If X_binned is not of dtype\
  \ uint8.\n        ValueError\n            If X_binned is not Fortran-contiguous,\
  \ min_gain_to_split is negative,\n            min_hessian_to_split is negative,\
  \ or categorical features have monotonic\n            constraints.\n\n        Notes\n\
  \        -----\n        The tree growing process uses histogram-based splitting\
  \ for efficiency. The\n        algorithm maintains a priority queue of splittable\
  \ nodes, always splitting\n        the node with the highest potential gain first.\
  \ Categorical features are\n        handled using bitset representations for efficient\
  \ subset splitting.\n\n        When monotonic constraints are specified, child node\
  \ value bounds are\n        enforced to ensure the learned function respects the\
  \ constraints. Interaction\n        constraints limit which features can appear\
  \ together in any root-to-leaf path.\n        \"\"\"\n        <your code>\n\n  \
  \  def _validate_parameters(\n        self,\n        X_binned,\n        min_gain_to_split,\n\
  \        min_hessian_to_split\n    ):\n        \"\"\"\n        Validate parameters\
  \ passed to __init__.\n\n                Also validate parameters passed to splitter.\n\
  \n        \"\"\"\n        <your code>\n\n    def grow(self):\n        \"\"\"\n \
  \       Grow the tree, from root to leaves.\n        \"\"\"\n        <your code>\n\
  \n    def _apply_shrinkage(self):\n        \"\"\"\n        Multiply leaves values\
  \ by shrinkage parameter.\n\n                This must be done at the very end of\
  \ the growing process. If this were\n                done during the growing process\
  \ e.g. in finalize_leaf(), then a leaf\n                would be shrunk but its\
  \ sibling would potentially not be (if it's a\n                non-leaf), which\
  \ would lead to a wrong computation of the 'middle'\n                value needed\
  \ to enforce the monotonic constraints.\n\n        \"\"\"\n        <your code>\n\
  \n    def _initialize_root(self):\n        \"\"\"\n        Initialize root node\
  \ and finalize it if needed.\n        \"\"\"\n        <your code>\n\n    def _compute_best_split_and_push(self,\
  \ node):\n        \"\"\"\n        Compute the best possible split (SplitInfo) of\
  \ a given node.\n\n                Also push it in the heap of splittable nodes\
  \ if gain isn't zero.\n                The gain of a node is 0 if either all the\
  \ leaves are pure\n                (best gain = 0), or if no split would satisfy\
  \ the constraints,\n                (min_hessians_to_split, min_gain_to_split, min_samples_leaf)\n\
  \n        \"\"\"\n        <your code>\n\n    def split_next(self):\n        \"\"\
  \"\n        Split the node with highest potential gain.\n\n                Returns\n\
  \                -------\n                left : TreeNode\n                    The\
  \ resulting left child.\n                right : TreeNode\n                    The\
  \ resulting right child.\n\n        \"\"\"\n        <your code>\n\n    def _compute_interactions(self,\
  \ node):\n        \"\"\"\n        Compute features allowed by interactions to be\
  \ inherited by child nodes.\n\n                Example: Assume constraints [{0,\
  \ 1}, {1, 2}].\n                   1      <- Both constraint groups could be applied\
  \ from now on\n                  / \\\n                 1   2    <- Left split still\
  \ fulfills both constraint groups.\n                / \\ / \\      Right split at\
  \ feature 2 has only group {1, 2} from now on.\n\n                LightGBM uses\
  \ the same logic for overlapping groups. See\n                https://github.com/microsoft/LightGBM/issues/4481\
  \ for details.\n\n                Parameters:\n                ----------\n    \
  \            node : TreeNode\n                    A node that might have children.\
  \ Based on its feature_idx, the interaction\n                    constraints for\
  \ possible child nodes are computed.\n\n                Returns\n              \
  \  -------\n                allowed_features : ndarray, dtype=uint32\n         \
  \           Indices of features allowed to split for children.\n               \
  \ interaction_cst_indices : list of ints\n                    Indices of the interaction\
  \ sets that have to be applied on splits of\n                    child nodes. The\
  \ fewer sets the stronger the constraint as fewer sets\n                    contain\
  \ fewer features.\n\n        \"\"\"\n        <your code>\n\n    def _finalize_leaf(self,\
  \ node):\n        \"\"\"\n        Make node a leaf of the tree being grown.\n  \
  \      \"\"\"\n        <your code>\n\n    def _finalize_splittable_nodes(self):\n\
  \        \"\"\"\n        Transform all splittable nodes into leaves.\n\n       \
  \         Used when some constraint is met e.g. maximum number of leaves or\n  \
  \              maximum depth.\n        \"\"\"\n        <your code>\n\n    def make_predictor(self,\
  \ binning_thresholds):\n        \"\"\"\n        Make a TreePredictor object out\
  \ of the current tree.\n\n                Parameters\n                ----------\n\
  \                binning_thresholds : array-like of floats\n                   \
  \ Corresponds to the bin_thresholds_ attribute of the BinMapper.\n             \
  \       For each feature, this stores:\n\n                    - the bin frontiers\
  \ for continuous features\n                    - the unique raw category values\
  \ for categorical features\n\n                Returns\n                -------\n\
  \                A TreePredictor object.\n\n        \"\"\"\n        <your code>\n"
interface_code_example: "class HistGradientBoostingClassifier(ClassifierMixin, BaseHistGradientBoosting):\n\
  \    \"\"\"\n    Histogram-based Gradient Boosting Classification Tree.\n    \n\
  \        This estimator is much faster than\n        :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n\
  \        for big datasets (n_samples >= 10 000).\n    \n        This estimator has\
  \ native support for missing values (NaNs). During\n        training, the tree grower\
  \ learns at each split point whether samples\n        with missing values should\
  \ go to the left or right child, based on the\n        potential gain. When predicting,\
  \ samples with missing values are\n        assigned to the left or right child consequently.\
  \ If no missing values\n        were encountered for a given feature during training,\
  \ then samples with\n        missing values are mapped to whichever child has the\
  \ most samples.\n    \n        This implementation is inspired by\n        `LightGBM\
  \ <https://github.com/Microsoft/LightGBM>`_.\n    \n        Read more in the :ref:`User\
  \ Guide <histogram_based_gradient_boosting>`.\n    \n        .. versionadded:: 0.21\n\
  \    \n        Parameters\n        ----------\n        loss : {'log_loss'}, default='log_loss'\n\
  \            The loss function to use in the boosting process.\n    \n         \
  \   For binary classification problems, 'log_loss' is also known as logistic loss,\n\
  \            binomial deviance or binary crossentropy. Internally, the model fits\
  \ one tree\n            per boosting iteration and uses the logistic sigmoid function\
  \ (expit) as\n            inverse link function to compute the predicted positive\
  \ class probability.\n    \n            For multiclass classification problems,\
  \ 'log_loss' is also known as multinomial\n            deviance or categorical crossentropy.\
  \ Internally, the model fits one tree per\n            boosting iteration and per\
  \ class and uses the softmax function as inverse link\n            function to compute\
  \ the predicted probabilities of the classes.\n    \n        learning_rate : float,\
  \ default=0.1\n            The learning rate, also known as *shrinkage*. This is\
  \ used as a\n            multiplicative factor for the leaves values. Use ``1``\
  \ for no\n            shrinkage.\n        max_iter : int, default=100\n        \
  \    The maximum number of iterations of the boosting process, i.e. the\n      \
  \      maximum number of trees for binary classification. For multiclass\n     \
  \       classification, `n_classes` trees per iteration are built.\n        max_leaf_nodes\
  \ : int or None, default=31\n            The maximum number of leaves for each tree.\
  \ Must be strictly greater\n            than 1. If None, there is no maximum limit.\n\
  \        max_depth : int or None, default=None\n            The maximum depth of\
  \ each tree. The depth of a tree is the number of\n            edges to go from\
  \ the root to the deepest leaf.\n            Depth isn't constrained by default.\n\
  \        min_samples_leaf : int, default=20\n            The minimum number of samples\
  \ per leaf. For small datasets with less\n            than a few hundred samples,\
  \ it is recommended to lower this value\n            since only very shallow trees\
  \ would be built.\n        l2_regularization : float, default=0\n            The\
  \ L2 regularization parameter penalizing leaves with small hessians.\n         \
  \   Use ``0`` for no regularization (default).\n        max_features : float, default=1.0\n\
  \            Proportion of randomly chosen features in each and every node split.\n\
  \            This is a form of regularization, smaller values make the trees weaker\n\
  \            learners and might prevent overfitting.\n            If interaction\
  \ constraints from `interaction_cst` are present, only allowed\n            features\
  \ are taken into account for the subsampling.\n    \n            .. versionadded::\
  \ 1.4\n    \n        max_bins : int, default=255\n            The maximum number\
  \ of bins to use for non-missing values. Before\n            training, each feature\
  \ of the input array `X` is binned into\n            integer-valued bins, which\
  \ allows for a much faster training stage.\n            Features with a small number\
  \ of unique values may use less than\n            ``max_bins`` bins. In addition\
  \ to the ``max_bins`` bins, one more bin\n            is always reserved for missing\
  \ values. Must be no larger than 255.\n        categorical_features : array-like\
  \ of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,),\
  \ default='from_dtype'\n            Indicates the categorical features.\n    \n\
  \            - None : no feature will be considered categorical.\n            -\
  \ boolean array-like : boolean mask indicating categorical features.\n         \
  \   - integer array-like : integer indices indicating categorical\n            \
  \  features.\n            - str array-like: names of categorical features (assuming\
  \ the training\n              data has feature names).\n            - `\"from_dtype\"\
  `: dataframe columns with dtype \"category\" are\n              considered to be\
  \ categorical features. The input must be an object\n              exposing a ``__dataframe__``\
  \ method such as pandas or polars\n              DataFrames to use this feature.\n\
  \    \n            For each categorical feature, there must be at most `max_bins`\
  \ unique\n            categories. Negative values for categorical features encoded\
  \ as numeric\n            dtypes are treated as missing values. All categorical\
  \ values are\n            converted to floating point numbers. This means that categorical\
  \ values\n            of 1.0 and 1 are treated as the same category.\n    \n   \
  \         Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n    \n\
  \            .. versionadded:: 0.24\n    \n            .. versionchanged:: 1.2\n\
  \               Added support for feature names.\n    \n            .. versionchanged::\
  \ 1.4\n               Added `\"from_dtype\"` option.\n    \n            .. versionchanged::\
  \ 1.6\n               The default value changed from `None` to `\"from_dtype\"`.\n\
  \    \n        monotonic_cst : array-like of int of shape (n_features) or dict,\
  \ default=None\n            Monotonic constraint to enforce on each feature are\
  \ specified using the\n            following integer values:\n    \n           \
  \ - 1: monotonic increase\n            - 0: no constraint\n            - -1: monotonic\
  \ decrease\n    \n            If a dict with str keys, map feature to monotonic\
  \ constraints by name.\n            If an array, the features are mapped to constraints\
  \ by position. See\n            :ref:`monotonic_cst_features_names` for a usage\
  \ example.\n    \n            The constraints are only valid for binary classifications\
  \ and hold\n            over the probability of the positive class.\n          \
  \  Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n    \n            ..\
  \ versionadded:: 0.23\n    \n            .. versionchanged:: 1.2\n             \
  \  Accept dict of constraints with feature names as keys.\n    \n        interaction_cst\
  \ : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets       \
  \      of int, default=None\n            Specify interaction constraints, the sets\
  \ of features which can\n            interact with each other in child node splits.\n\
  \    \n            Each item specifies the set of feature indices that are allowed\n\
  \            to interact with each other. If there are more features than\n    \
  \        specified in these constraints, they are treated as if they were\n    \
  \        specified as an additional set.\n    \n            The strings \"pairwise\"\
  \ and \"no_interactions\" are shorthands for\n            allowing only pairwise\
  \ or no interactions, respectively.\n    \n            For instance, with 5 features\
  \ in total, `interaction_cst=[{0, 1}]`\n            is equivalent to `interaction_cst=[{0,\
  \ 1}, {2, 3, 4}]`,\n            and specifies that each branch of a tree will either\
  \ only split\n            on features 0 and 1 or only split on features 2, 3 and\
  \ 4.\n    \n            See :ref:`this example<ice-vs-pdp>` on how to use `interaction_cst`.\n\
  \    \n            .. versionadded:: 1.2\n    \n        warm_start : bool, default=False\n\
  \            When set to ``True``, reuse the solution of the previous call to fit\n\
  \            and add more estimators to the ensemble. For results to be valid, the\n\
  \            estimator should be re-trained on the same data only.\n           \
  \ See :term:`the Glossary <warm_start>`.\n        early_stopping : 'auto' or bool,\
  \ default='auto'\n            If 'auto', early stopping is enabled if the sample\
  \ size is larger than\n            10000 or if `X_val` and `y_val` are passed to\
  \ `fit`. If True, early stopping\n            is enabled, otherwise early stopping\
  \ is disabled.\n    \n            .. versionadded:: 0.23\n    \n        scoring\
  \ : str or callable or None, default='loss'\n            Scoring method to use for\
  \ early stopping. Only used if `early_stopping`\n            is enabled. Options:\n\
  \    \n            - str: see :ref:`scoring_string_names` for options.\n       \
  \     - callable: a scorer callable object (e.g., function) with signature\n   \
  \           ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n\
  \            - `None`: :ref:`accuracy <accuracy_score>` is used.\n            -\
  \ 'loss': early stopping is checked w.r.t the loss value.\n    \n        validation_fraction\
  \ : int or float or None, default=0.1\n            Proportion (or absolute size)\
  \ of training data to set aside as\n            validation data for early stopping.\
  \ If None, early stopping is done on\n            the training data.\n         \
  \   The value is ignored if either early stopping is not performed, e.g.\n     \
  \       `early_stopping=False`, or if `X_val` and `y_val` are passed to fit.\n \
  \       n_iter_no_change : int, default=10\n            Used to determine when to\
  \ \"early stop\". The fitting process is\n            stopped when none of the last\
  \ ``n_iter_no_change`` scores are better\n            than the ``n_iter_no_change\
  \ - 1`` -th-to-last one, up to some\n            tolerance. Only used if early stopping\
  \ is performed.\n        tol : float, default=1e-7\n            The absolute tolerance\
  \ to use when comparing scores. The higher the\n            tolerance, the more\
  \ likely we are to early stop: higher tolerance\n            means that it will\
  \ be harder for subsequent iterations to be\n            considered an improvement\
  \ upon the reference score.\n        verbose : int, default=0\n            The verbosity\
  \ level. If not zero, print some information about the\n            fitting process.\
  \ ``1`` prints only summary info, ``2`` prints info per\n            iteration.\n\
  \        random_state : int, RandomState instance or None, default=None\n      \
  \      Pseudo-random number generator to control the subsampling in the\n      \
  \      binning process, and the train/validation data split if early stopping\n\
  \            is enabled.\n            Pass an int for reproducible output across\
  \ multiple function calls.\n            See :term:`Glossary <random_state>`.\n \
  \       class_weight : dict or 'balanced', default=None\n            Weights associated\
  \ with classes in the form `{class_label: weight}`.\n            If not given, all\
  \ classes are supposed to have weight one.\n            The \"balanced\" mode uses\
  \ the values of y to automatically adjust\n            weights inversely proportional\
  \ to class frequencies in the input data\n            as `n_samples / (n_classes\
  \ * np.bincount(y))`.\n            Note that these weights will be multiplied with\
  \ sample_weight (passed\n            through the fit method) if `sample_weight`\
  \ is specified.\n    \n            .. versionadded:: 1.2\n    \n        Attributes\n\
  \        ----------\n        classes_ : array, shape = (n_classes,)\n          \
  \  Class labels.\n        do_early_stopping_ : bool\n            Indicates whether\
  \ early stopping is used during training.\n        n_iter_ : int\n            The\
  \ number of iterations as selected by early stopping, depending on\n           \
  \ the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n      \
  \  n_trees_per_iteration_ : int\n            The number of tree that are built at\
  \ each iteration. This is equal to 1\n            for binary classification, and\
  \ to ``n_classes`` for multiclass\n            classification.\n        train_score_\
  \ : ndarray, shape (n_iter_+1,)\n            The scores at each iteration on the\
  \ training data. The first entry\n            is the score of the ensemble before\
  \ the first iteration. Scores are\n            computed according to the ``scoring``\
  \ parameter. If ``scoring`` is\n            not 'loss', scores are computed on a\
  \ subset of at most 10 000\n            samples. Empty if no early stopping.\n \
  \       validation_score_ : ndarray, shape (n_iter_+1,)\n            The scores\
  \ at each iteration on the held-out validation data. The\n            first entry\
  \ is the score of the ensemble before the first iteration.\n            Scores are\
  \ computed according to the ``scoring`` parameter. Empty if\n            no early\
  \ stopping or if ``validation_fraction`` is None.\n        is_categorical_ : ndarray,\
  \ shape (n_features, ) or None\n            Boolean mask for the categorical features.\
  \ ``None`` if there are no\n            categorical features.\n        n_features_in_\
  \ : int\n            Number of features seen during :term:`fit`.\n    \n       \
  \     .. versionadded:: 0.24\n        feature_names_in_ : ndarray of shape (`n_features_in_`,)\n\
  \            Names of features seen during :term:`fit`. Defined only when `X`\n\
  \            has feature names that are all strings.\n    \n            .. versionadded::\
  \ 1.0\n    \n        See Also\n        --------\n        GradientBoostingClassifier\
  \ : Exact gradient boosting method that does not\n            scale as good on datasets\
  \ with a large number of samples.\n        sklearn.tree.DecisionTreeClassifier :\
  \ A decision tree classifier.\n        RandomForestClassifier : A meta-estimator\
  \ that fits a number of decision\n            tree classifiers on various sub-samples\
  \ of the dataset and uses\n            averaging to improve the predictive accuracy\
  \ and control over-fitting.\n        AdaBoostClassifier : A meta-estimator that\
  \ begins by fitting a classifier\n            on the original dataset and then fits\
  \ additional copies of the\n            classifier on the same dataset where the\
  \ weights of incorrectly\n            classified instances are adjusted such that\
  \ subsequent classifiers\n            focus more on difficult cases.\n    \n   \
  \     Examples\n        --------\n        >>> from sklearn.ensemble import HistGradientBoostingClassifier\n\
  \        >>> from sklearn.datasets import load_iris\n        >>> X, y = load_iris(return_X_y=True)\n\
  \        >>> clf = HistGradientBoostingClassifier().fit(X, y)\n        >>> clf.score(X,\
  \ y)\n        1.0\n        \n    \"\"\"\n\n    _parameter_constraints = \"{**BaseHistGradientBoosting._parameter_constraints,\
  \ 'loss': [StrOptions({'log_loss'}), BaseLoss], 'class_weight': [dict, StrOptions({'balanced'}),\
  \ None]}\"\n\n    def __init__(self, loss = 'log_loss'):\n        \"\"\"\n     \
  \   Initialize a histogram-based gradient boosting classifier.\n\n        This constructor\
  \ sets up the basic parameters for the HistGradientBoostingClassifier,\n       \
  \ which implements a fast gradient boosting algorithm using histogram-based tree\
  \ growing.\n\n        Parameters\n        ----------\n        loss : {'log_loss'}\
  \ or BaseLoss instance, default='log_loss'\n            The loss function to use\
  \ in the boosting process. For binary classification,\n            'log_loss' uses\
  \ logistic loss (binomial deviance). For multiclass classification,\n          \
  \  'log_loss' uses multinomial deviance (categorical crossentropy). Custom loss\n\
  \            functions can be provided as BaseLoss instances.\n\n        learning_rate\
  \ : float, default=0.1\n            The learning rate (shrinkage factor) applied\
  \ to each tree's contribution.\n            Lower values make the model more robust\
  \ but require more iterations.\n            Must be greater than 0.\n\n        max_iter\
  \ : int, default=100\n            Maximum number of boosting iterations. For binary\
  \ classification, this is\n            the number of trees. For multiclass, n_classes\
  \ trees are built per iteration.\n            Must be at least 1.\n\n        max_leaf_nodes\
  \ : int or None, default=31\n            Maximum number of leaves per tree. Must\
  \ be greater than 1 if specified.\n            If None, no limit is imposed on the\
  \ number of leaves.\n\n        max_depth : int or None, default=None\n         \
  \   Maximum depth of each tree (number of edges from root to deepest leaf).\n  \
  \          If None, depth is not constrained.\n\n        min_samples_leaf : int,\
  \ default=20\n            Minimum number of samples required in each leaf node.\
  \ For small datasets,\n            consider lowering this value to allow deeper\
  \ trees.\n\n        l2_regularization : float, default=0.0\n            L2 regularization\
  \ parameter for leaf values. Higher values provide more\n            regularization.\
  \ Must be non-negative.\n\n        max_features : float, default=1.0\n         \
  \   Proportion of features to consider at each split (0 < max_features <= 1).\n\
  \            Provides regularization by introducing randomness in feature selection.\n\
  \n        max_bins : int, default=255\n            Maximum number of bins for discretizing\
  \ continuous features. Must be\n            between 2 and 255. One additional bin\
  \ is reserved for missing values.\n\n        categorical_features : array-like,\
  \ str or None, default='from_dtype'\n            Specification of categorical features.\
  \ Can be boolean mask, integer indices,\n            feature names, or 'from_dtype'\
  \ to auto-detect from pandas categorical columns.\n\n        monotonic_cst : array-like,\
  \ dict or None, default=None\n            Monotonic constraints for features. Values:\
  \ 1 (increasing), 0 (no constraint),\n            -1 (decreasing). Only supported\
  \ for binary classification.\n\n        interaction_cst : sequence, str or None,\
  \ default=None\n            Interaction constraints specifying which features can\
  \ interact in splits.\n            Can be 'pairwise', 'no_interactions', or custom\
  \ feature groupings.\n\n        warm_start : bool, default=False\n            Whether\
  \ to reuse the previous fit solution and add more estimators.\n            Requires\
  \ fitting on the same data.\n\n        early_stopping : 'auto' or bool, default='auto'\n\
  \            Whether to use early stopping. 'auto' enables it for large datasets\n\
  \            (n_samples > 10000) or when validation data is provided.\n\n      \
  \  scoring : str, callable or None, default='loss'\n            Scoring method for\
  \ early stopping. Can be a scorer name, callable,\n            None (uses accuracy),\
  \ or 'loss' (uses the loss function).\n\n        validation_fraction : float, int\
  \ or None, default=0.1\n            Fraction or absolute size of training data for\
  \ validation in early stopping.\n            Ignored if validation data is explicitly\
  \ provided or early stopping is disabled.\n\n        n_iter_no_change : int, default=10\n\
  \            Number of iterations with no improvement to trigger early stopping.\n\
  \            Must be at least 1.\n\n        tol : float, default=1e-7\n        \
  \    Tolerance for early stopping. Higher values make early stopping more likely.\n\
  \            Must be non-negative.\n\n        verbose : int, default=0\n       \
  \     Verbosity level. 0 for silent, 1 for summary info, 2 for detailed iteration\
  \ info.\n\n        random_state : int, RandomState or None, default=None\n     \
  \       Random state for reproducible results. Controls binning randomness and\n\
  \            train/validation splits.\n\n        class_weight : dict, 'balanced'\
  \ or None, default=None\n            Weights for classes. 'balanced' uses inverse\
  \ class frequencies.\n            Custom weights as {class_label: weight} dictionary.\n\
  \n        Notes\n        -----\n        This initializer only sets the parameters.\
  \ The actual model training occurs\n        when the fit method is called. The classifier\
  \ supports both binary and\n        multiclass classification automatically based\
  \ on the target variable.\n\n        For binary classification, one tree per iteration\
  \ is built. For multiclass\n        classification, one tree per class per iteration\
  \ is built.\n        \"\"\"\n        <your code>\n..."
