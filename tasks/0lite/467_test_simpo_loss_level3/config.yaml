base_image: pb-python310_cu121_torch251-base_08bcbe5c
black_links:
- https://github.com/linkedin/Liger-Kernel/
commit: null
docker_specs:
  run_args:
    cap_add: []
    enable_gpu: true
install: pip install -e ".[dev]"
instance_image: pb-instance_ffbb2d38
library_name: liger-kernel
pip_packages: []
pre_install: []
repo_name: Liger-Kernel
repository: linkedin/Liger-Kernel
task_level: 3
task_name: Liger_Kernel_simpo_loss
task_statement: '**Task: Implement Memory-Efficient Preference-Based Language Model
  Training**


  **Core Functionality:**

  Develop a fused linear layer with SimPO (Simple Preference Optimization) loss for
  training language models on preference data without requiring reference models.


  **Main Features & Requirements:**

  - Combine linear transformation and preference loss computation in a single memory-efficient
  operation

  - Implement SimPO algorithm using length-normalized log probabilities with configurable
  margin and smoothing

  - Support both forward and backward passes with proper gradient computation

  - Handle chunked processing for large sequences

  - Provide configurable hyperparameters (beta, gamma, label smoothing)


  **Key Challenges:**

  - Optimize memory usage by fusing operations rather than computing them separately

  - Correctly implement the SimPO loss formula with sigmoid-based preference comparison

  - Ensure proper gradient flow through custom autograd functions

  - Balance computational efficiency with numerical stability for large-scale training'
technical_docs:
- description: latex source of the paper liger_kernel which implemented many llm operators
    using triton
  path: liger-kernel-tex-source
test_cmd: pytest -rA --timeout=20
test_code1: 'from agent_code.liger_kernel.chunked_loss import LigerFusedLinearSimPOLoss

  from agent_code.liger_kernel.chunked_loss.simpo_loss import LigerFusedLinearSimPOFunction'
test_code_example: from agent_code.liger_kernel.chunked_loss import LigerFusedLinearSimPOLoss
test_code_example_obj: LigerFusedLinearSimPOLoss
test_code_example_path: /testbed/agent_code/liger_kernel/chunked_loss.py
test_description1: Below is **Test Description 1**
timeout: 20
interface_description1: 'Below is **Interface Description 1** for file: src-liger_kernel-chunked_loss-simpo_loss.py


  This file contains 2 top-level interface(s) that need to be implemented.

  '
interface_code1: "class LigerFusedLinearSimPOFunction(LigerFusedLinearPreferenceBase):\n\
  \    \"\"\"\n    A PyTorch autograd function that implements the SimPO (Simple Preference\
  \ Optimization) loss combined with a fused linear layer for efficient preference-based\
  \ model training.\n    \n    This class extends LigerFusedLinearPreferenceBase to\
  \ provide a memory-efficient implementation of SimPO loss, which is used for training\
  \ language models on preference data without requiring a reference model. The SimPO\
  \ loss compares chosen and rejected sequences using their average log probabilities\
  \ and applies a sigmoid-based loss function with configurable margin and smoothing\
  \ parameters.\n    \n    **Key Features:**\n    - Fuses linear layer computation\
  \ with SimPO loss calculation for improved memory efficiency\n    - Implements the\
  \ SimPO algorithm from \"SimPO: Simple Preference Optimization with a Reference-Free\
  \ Reward\" (https://arxiv.org/pdf/2405.14734)\n    - Supports chunked processing\
  \ for handling large sequences\n    - Provides both forward and backward pass implementations\
  \ with gradient computation\n    \n    **Main Methods:**\n    \n    - `preference_loss_fn(chosen_logps,\
  \ rejected_logps, full_target, beta, gamma, label_smoothing)`: \n      Computes\
  \ the core SimPO loss using the formula: L_SimPO(π_θ) = -E [log σ(β/|y_w| log π_θ(y_w|x)\
  \ - β/|y_l| log π_θ(y_l|x) - γ)].\n      Returns the loss value along with chosen\
  \ and rejected rewards.\n    \n    - `forward(cls, ctx, _input, weight, target,\
  \ bias, ignore_index, beta, alpha, label_smoothing, compute_nll_loss, compiled,\
  \ gamma, chunk_size)`:\n      Performs the forward pass by calling the parent class\
  \ implementation with SimPO-specific parameters.\n      Handles the fused linear\
  \ transformation and loss computation in a single operation.\n    \n    - `backward(ctx,\
  \ *grad_output)`:\n      Computes gradients for the input, weight, target, and bias\
  \ tensors during backpropagation.\n      Returns gradients while setting None for\
  \ non-differentiable parameters.\n    \n    **Usage Example:**\n    ```python\n\
  \    # Typically used through LigerFusedLinearSimPOLoss module\n    simpo_loss =\
  \ LigerFusedLinearSimPOLoss(beta=0.1, gamma=0.5, label_smoothing=0.0)\n    loss\
  \ = simpo_loss(linear_weight, input_tensor, target_tensor, bias)\n    \n    # Or\
  \ directly as an autograd function\n    loss = LigerFusedLinearSimPOFunction.apply(\n\
  \        input_tensor, weight, target, bias, -100, 0.1, 1.0, 0.0, False, True, 0.5,\
  \ 1\n    )\n    ```\n    \n    **Parameters:**\n    - `beta`: Controls the strength\
  \ of the preference signal (default: 0.1)\n    - `gamma`: Margin term that affects\
  \ the decision boundary (default: 0.5)  \n    - `label_smoothing`: Smoothing factor\
  \ for the loss function (default: 0.0)\n    - `chunk_size`: Size of chunks for memory-efficient\
  \ processing (default: 1)\n    \"\"\"\n\n    @staticmethod\n    def preference_loss_fn(\n\
  \        chosen_logps,\n        rejected_logps,\n        full_target,\n        beta\
  \ = 0.1,\n        gamma = 0.5,\n        label_smoothing = 0.0\n    ):\n        \"\
  \"\"\n\n                Paper: https://arxiv.org/pdf/2405.14734\n\n            \
  \    Formula:\n                L_SimPO(π_θ) = -E [log σ(β/|y_w| log π_θ(y_w|x) -\
  \ β/|y_l| log π_θ(y_l|x) - γ)]\n\n                Where:\n                - π_θ(y|x):\
  \ Policy (model) probability\n                - y_w: Chosen sequence\n         \
  \       - y_l: Rejected sequence\n                - |y_w|, |y_l|: Sequence lengths\n\
  \                - σ: Sigmoid function\n                - β: beta weight\n     \
  \           - γ: gemma margin term\n\n                Args:\n                  \
  \  chosen_logps (torch.Tensor): Avg log probabilities of chosen tokens. Shape: (batch_size,).\n\
  \                    rejected_logps (torch.Tensor): Avg log probabilities of rejected\
  \ tokens. Shape: (batch_size,).\n                    full_target: Non chunked full\
  \ target tensor\n                    beta (float): beta weight\n               \
  \     gamma (float): gemma margin term\n                    label_smoothing (float):\
  \ Label smoothing factor, will reduce to Equation above when label_smoothing ->\
  \ 0.\n\n        \"\"\"\n        <your code>\n\n    @classmethod\n    def forward(\n\
  \        cls,\n        ctx,\n        _input,\n        weight,\n        target,\n\
  \        bias = None,\n        ignore_index = -100,\n        beta = 0.1,\n     \
  \   alpha = 1.0,\n        label_smoothing = 0.0,\n        compute_nll_loss = False,\n\
  \        compiled = True,\n        gamma = 0.5,\n        chunk_size = 1\n    ):\n\
  \        \"\"\"\n\n                Fused linear layer with SimPO loss.\n       \
  \         Args:\n                    _input (torch.Tensor): Input tensor. Shape:\
  \ (batch_size * seq_len, hidden_size)\n                    weight (torch.Tensor):\
  \ Weight tensor. Shape: (vocab_size, hidden_size)\n                    target (torch.LongTensor):\
  \ Target tensor. Shape: (batch_size * seq_len,)\n                    bias (torch.Tensor,\
  \ optional): Bias tensor. Shape: (vocab_size,)\n                    ignore_index\
  \ (int): Index to ignore in loss computation\n                    beta (float):\
  \ Weight for the odds ratio loss\n                    alpha (float): Weight for\
  \ the alpha parameter\n                    label_smoothing (float): Label smoothing\
  \ factor\n                    compute_nll_loss (bool): Whether to compute the NLL\
  \ loss\n                    compiled (bool): Whether to use torch compile\n    \
  \                gamma (float): Weight for the gamma parameter\n               \
  \     chunk_size (int): Size of chunks for processing\n                Returns:\n\
  \                    torch.Tensor: Computed loss\n\n        \"\"\"\n        <your\
  \ code>\n\n    @staticmethod\n    def backward(ctx, *grad_output):\n        \"\"\
  \"\n        Performs the backward pass for the LigerFusedLinearSimPOFunction.\n\n\
  \        This static method computes gradients with respect to the input parameters\
  \ during backpropagation\n        for the fused linear layer with SimPO (Simple\
  \ Preference Optimization) loss. It delegates the\n        gradient computation\
  \ to the parent class and then filters and pads the results appropriately.\n\n \
  \       Args:\n            ctx: The context object that was saved during the forward\
  \ pass, containing\n                 intermediate values and tensors needed for\
  \ gradient computation.\n            *grad_output: Variable length argument list\
  \ containing the gradients of the loss\n                          with respect to\
  \ the outputs of the forward pass. Typically contains\n                        \
  \  the gradient tensor from the subsequent layer in the computation graph.\n\n \
  \       Returns:\n            tuple: A tuple containing gradients with respect to\
  \ all input parameters of the forward\n                   method, in the same order\
  \ as the forward method signature:\n                   - Gradient w.r.t. _input\
  \ (torch.Tensor or None)\n                   - Gradient w.r.t. weight (torch.Tensor\
  \ or None) \n                   - Gradient w.r.t. target (torch.Tensor or None)\n\
  \                   - Gradient w.r.t. bias (torch.Tensor or None)\n            \
  \       - None for ignore_index (no gradient needed)\n                   - None\
  \ for beta (no gradient needed)\n                   - None for alpha (no gradient\
  \ needed)\n                   - None for label_smoothing (no gradient needed)\n\
  \                   - None for compute_nll_loss (no gradient needed)\n         \
  \          - None for compiled (no gradient needed)\n                   - None for\
  \ gamma (no gradient needed)\n                   - None for chunk_size (no gradient\
  \ needed)\n\n        Notes:\n            - This method is automatically called by\
  \ PyTorch's autograd system during backpropagation\n            - The method only\
  \ computes gradients for the first 4 parameters (input tensors) and returns\n  \
  \            None for all hyperparameters and boolean flags as they don't require\
  \ gradients\n            - The actual gradient computation is handled by the parent\
  \ class LigerFusedLinearPreferenceBase\n        \"\"\"\n        <your code>\n\n\
  class LigerFusedLinearSimPOLoss(torch.nn.Module):\n    \"\"\"\n    \n        Fused\
  \ linear layer with SimPO loss.\n        \n    \"\"\"\n\n    def __init__(\n   \
  \     self,\n        ignore_index: int = -100,\n        beta: float = 0.1,\n   \
  \     alpha: float = 1.0,\n        label_smoothing: float = 0.0,\n        compute_nll_loss:\
  \ bool = True,\n        compiled: bool = True,\n        gamma: float = 0.5,\n  \
  \      chunk_size: int = 1\n    ):\n        \"\"\"\n\n                Args:\n  \
  \                  ignore_index (int): Index to ignore in the loss.\n          \
  \          beta (float): Weight for the odds ratio loss.\n                    alpha\
  \ (float): Weight for the alpha parameter.\n                    label_smoothing\
  \ (float): Label smoothing factor.\n                    compute_nll_loss (bool):\
  \ Whether to compute the NLL loss.\n                    compiled (bool): Whether\
  \ to use the torch compiled kernel.\n                    gamma (float): Weight for\
  \ the gamma parameter.\n                    chunk_size (int): Size of chunks for\
  \ processing.\n\n        \"\"\"\n        <your code>\n\n    def forward(\n     \
  \   self,\n        lin_weight,\n        _input,\n        target,\n        bias =\
  \ None\n    ):\n        \"\"\"\n        Computes the fused linear transformation\
  \ with SimPO (Simple Preference Optimization) loss.\n\n        This method performs\
  \ a forward pass through a linear layer followed by SimPO loss computation,\n  \
  \      which is designed for preference-based training without requiring a reference\
  \ model. The SimPO\n        loss uses length-normalized log probabilities and includes\
  \ a margin term for improved training stability.\n\n        Parameters:\n      \
  \      lin_weight (torch.Tensor): The weight matrix for the linear transformation.\n\
  \                Shape: (vocab_size, hidden_size)\n            _input (torch.Tensor):\
  \ Input tensor containing token embeddings.\n                Shape: (batch_size\
  \ * seq_len, hidden_size)\n            target (torch.LongTensor): Target token indices\
  \ for loss computation.\n                Shape: (batch_size * seq_len,)\n      \
  \      bias (torch.Tensor, optional): Bias vector for the linear transformation.\n\
  \                Shape: (vocab_size,). Defaults to None.\n\n        Returns:\n \
  \           torch.Tensor: The computed SimPO loss value as a scalar tensor.\n\n\
  \        Notes:\n            - This implementation follows the SimPO paper (https://arxiv.org/pdf/2405.14734)\n\
  \            - The loss function uses length-normalized log probabilities: L_SimPO(π_θ)\
  \ = -E [log σ(β/|y_w| log π_θ(y_w|x) - β/|y_l| log π_θ(y_l|x) - γ)]\n          \
  \  - The method leverages the class instance parameters (beta, gamma, alpha, etc.)\
  \ set during initialization\n            - Input tensors are expected to contain\
  \ both chosen and rejected sequences in the batch\n            - The computation\
  \ is performed using a fused kernel for improved efficiency\n            - Gradients\
  \ are automatically computed for backpropagation through the custom autograd function\n\
  \        \"\"\"\n        <your code>\n"
interface_code_example: "class LigerFusedLinearSimPOLoss(torch.nn.Module):\n    \"\
  \"\"\n    \n        Fused linear layer with SimPO loss.\n        \n    \"\"\"\n\n\
  \    def __init__(\n        self,\n        ignore_index: int = -100,\n        beta:\
  \ float = 0.1,\n        alpha: float = 1.0,\n        label_smoothing: float = 0.0,\n\
  \        compute_nll_loss: bool = True,\n        compiled: bool = True,\n      \
  \  gamma: float = 0.5,\n        chunk_size: int = 1\n    ):\n        \"\"\"\n\n\
  \                Args:\n                    ignore_index (int): Index to ignore\
  \ in the loss.\n                    beta (float): Weight for the odds ratio loss.\n\
  \                    alpha (float): Weight for the alpha parameter.\n          \
  \          label_smoothing (float): Label smoothing factor.\n                  \
  \  compute_nll_loss (bool): Whether to compute the NLL loss.\n                 \
  \   compiled (bool): Whether to use the torch compiled kernel.\n               \
  \     gamma (float): Weight for the gamma parameter.\n                    chunk_size\
  \ (int): Size of chunks for processing.\n\n        \"\"\"\n        <your code>\n\
  ..."
