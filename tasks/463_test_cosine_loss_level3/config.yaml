base_image: pb-python310_cu121_torch251-base_08bcbe5c
black_links:
- https://github.com/linkedin/Liger-Kernel/
commit: null
docker_specs:
  run_args:
    cap_add: []
    enable_gpu: true
install: pip install -e ".[dev]"
instance_image: pb-instance_ffbb2d38
library_name: liger-kernel
pip_packages: []
pre_install: []
repo_name: Liger-Kernel
repository: linkedin/Liger-Kernel
task_level: 3
task_name: Liger_Kernel_cosine_loss
task_statement: '**Task: Implement Fused Linear Layer with Cosine Similarity Knowledge
  Distillation**


  **Core Functionality:**

  Create an efficient PyTorch module that combines linear transformations with cosine
  similarity-based knowledge distillation, enabling a student model to learn from
  a teacher model by matching both ground truth labels and directional patterns in
  the output space.


  **Main Features & Requirements:**

  - Fuse linear layer computation with distillation loss calculation for performance
  optimization

  - Implement cosine similarity loss between normalized student and teacher logits

  - Support configurable weighting between hard loss (cross-entropy) and soft loss
  (cosine similarity)

  - Handle memory-efficient processing through chunking for large vocabularies

  - Provide proper gradient computation for backpropagation training


  **Key Challenges:**

  - Balance computational efficiency with memory usage in fused operations

  - Implement custom autograd functions with correct gradient flow

  - Handle numerical stability in cosine similarity calculations with normalization

  - Manage hyperparameter tuning for loss weighting and temperature scaling

  - Ensure compatibility with chunked processing for scalability'
technical_docs:
- description: latex source of the paper liger_kernel which implemented many llm operators
    using triton
  path: liger-kernel-tex-source
test_cmd: pytest -rA --timeout=20
test_code1: 'from agent_code.liger_kernel.chunked_loss.cosine_similarity_loss import
  LigerFusedLinearCosineSimilarityFunction

  from agent_code.liger_kernel.chunked_loss.cosine_similarity_loss import LigerFusedLinearCosineSimilarityLoss'
test_code_example: from agent_code.liger_kernel.chunked_loss.cosine_similarity_loss
  import LigerFusedLinearCosineSimilarityFunction
test_code_example_obj: LigerFusedLinearCosineSimilarityFunction
test_code_example_path: /testbed/agent_code/liger_kernel/chunked_loss/cosine_similarity_loss.py
test_description1: Below is **Test Description 1**
timeout: 20
interface_description1: 'Below is **Interface Description 1** for file: src-liger_kernel-chunked_loss-cosine_similarity_loss.py


  This file contains 2 top-level interface(s) that need to be implemented.

  '
interface_code1: "class LigerFusedLinearCosineSimilarityFunction(LigerFusedLinearDistillationBase):\n\
  \    \"\"\"\n    A PyTorch autograd function that implements fused linear layer\
  \ computation with cosine similarity-based knowledge distillation loss.\n    \n\
  \    This class extends LigerFusedLinearDistillationBase to provide an efficient\
  \ implementation of knowledge distillation using cosine similarity loss between\
  \ student and teacher model outputs. It combines linear transformations with distillation\
  \ loss computation in a single fused operation for improved performance.\n    \n\
  \    Main Methods:\n        distillation_loss_fn(student_logits, teacher_logits,\
  \ beta=1.0):\n            Computes the cosine similarity loss between normalized\
  \ student and teacher logits.\n            The loss is calculated as beta * (1 -\
  \ cosine_similarity), where cosine similarity\n            measures the alignment\
  \ between the normalized logit vectors.\n    \n        forward(cls, ctx, student_input,\
  \ student_weight, teacher_input, teacher_weight, \n                true_labels,\
  \ student_bias, teacher_bias, **kwargs):\n            Performs the forward pass\
  \ by computing linear transformations for both student\n            and teacher\
  \ networks, then applies the combined hard loss (cross-entropy with\n          \
  \  true labels) and soft loss (cosine similarity distillation loss). Supports\n\
  \            chunked processing for memory efficiency.\n    \n        backward(ctx,\
  \ grad_output):\n            Computes gradients with respect to the input tensors\
  \ during backpropagation.\n            Only propagates gradients for trainable parameters\
  \ (student inputs/weights),\n            while setting gradients to None for hyperparameters\
  \ and teacher bias.\n    \n    Key Parameters:\n        - beta: Controls the strength\
  \ of the cosine similarity loss component\n        - weight_hard_loss/weight_soft_loss:\
  \ Balance between supervised and distillation losses\n        - temperature: Temperature\
  \ scaling for knowledge distillation\n        - chunk_size: Memory optimization\
  \ parameter for processing large tensors\n        - ignore_index: Label index to\
  \ ignore in loss computation\n    \n    Usage Example:\n        ```python\n    \
  \    # Used internally by LigerFusedLinearCosineSimilarityLoss\n        loss_fn\
  \ = LigerFusedLinearCosineSimilarityLoss(\n            weight_hard_loss=0.3,\n \
  \           weight_soft_loss=0.7,\n            beta=1.0,\n            temperature=3.0\n\
  \        )\n        \n        loss = loss_fn(\n            student_input=student_hidden_states,\n\
  \            student_weight=student_lm_head.weight,\n            teacher_input=teacher_hidden_states,\n\
  \            teacher_weight=teacher_lm_head.weight,\n            true_labels=target_labels\n\
  \        )\n        ```\n    \"\"\"\n\n    @staticmethod\n    def distillation_loss_fn(student_logits,\
  \ teacher_logits, beta = 1.0):\n        \"\"\"\n\n                Compute Cosine\
  \ loss (Cosine Similarity Loss).\n                Args:\n                    student_logits\
  \ (torch.Tensor): Logits of student tokens. Shape: (batch_size * seq_len,).\n  \
  \                  teacher_logits (torch.Tensor): Logits of teacher tokens. Shape:\
  \ (batch_size * seq_len,).\n                    beta: Coefficient beta of generalized\
  \ Cosine Similarity in the interval [0, 1]. Default: `1.0` (float): .\n        \
  \        Returns:\n                    torch.Tensor: cosine similarity loss\n\n\
  \        \"\"\"\n        <your code>\n\n    @classmethod\n    def forward(\n   \
  \     cls,\n        ctx,\n        student_input: torch.Tensor,\n        student_weight:\
  \ torch.Tensor,\n        teacher_input: torch.Tensor,\n        teacher_weight: torch.Tensor,\n\
  \        true_labels: torch.LongTensor,\n        student_bias: torch.Tensor,\n \
  \       teacher_bias: torch.Tensor,\n        weight_hard_loss: float = 0.5,\n  \
  \      weight_soft_loss: float = 0.5,\n        beta: float = 0.5,\n        ignore_index:\
  \ int = -100,\n        temperature: float = 1.0,\n        compiled: bool = True,\n\
  \        chunk_size: int = 1024\n    ):\n        \"\"\"\n        Forward pass for\
  \ the Liger Fused Linear Cosine Similarity distillation function.\n\n        This\
  \ classmethod implements the forward pass of a custom autograd function that combines\n\
  \        linear transformations with cosine similarity-based knowledge distillation.\
  \ It computes\n        a combined loss consisting of both hard loss (cross-entropy\
  \ with true labels) and soft\n        loss (cosine similarity between student and\
  \ teacher logits).\n\n        Parameters:\n            ctx: PyTorch autograd context\
  \ for storing information needed in backward pass\n            student_input (torch.Tensor):\
  \ Input tensor for the student model, typically hidden states\n                from\
  \ a transformer layer with shape (batch_size, seq_len, hidden_dim)\n           \
  \ student_weight (torch.Tensor): Weight matrix for the student's linear layer with\
  \ shape\n                (vocab_size, hidden_dim)\n            teacher_input (torch.Tensor):\
  \ Input tensor for the teacher model with same shape as\n                student_input\n\
  \            teacher_weight (torch.Tensor): Weight matrix for the teacher's linear\
  \ layer with shape\n                (vocab_size, hidden_dim)\n            true_labels\
  \ (torch.LongTensor): Ground truth labels for computing hard loss with shape\n \
  \               (batch_size, seq_len)\n            student_bias (torch.Tensor):\
  \ Optional bias tensor for student's linear layer with shape\n                (vocab_size,).\
  \ Can be None if no bias is used\n            teacher_bias (torch.Tensor): Optional\
  \ bias tensor for teacher's linear layer with shape\n                (vocab_size,).\
  \ Can be None if no bias is used\n            weight_hard_loss (float, optional):\
  \ Weight coefficient for the hard loss component.\n                Default: 0.5\n\
  \            weight_soft_loss (float, optional): Weight coefficient for the soft\
  \ loss component.\n                Default: 0.5\n            beta (float, optional):\
  \ Coefficient for the generalized cosine similarity loss in the\n              \
  \  interval [0, 1]. Default: 0.5\n            ignore_index (int, optional): Index\
  \ to ignore when computing the loss, typically used\n                for padding\
  \ tokens. Default: -100\n            temperature (float, optional): Temperature\
  \ parameter for softmax scaling in distillation.\n                Default: 1.0\n\
  \            compiled (bool, optional): Whether to use compiled operations for better\
  \ performance.\n                Default: True\n            chunk_size (int, optional):\
  \ Size of chunks for memory-efficient processing.\n                Default: 1024\n\
  \n        Returns:\n            torch.Tensor: Combined distillation loss scalar\
  \ value computed as a weighted sum of\n                hard loss (cross-entropy)\
  \ and soft loss (cosine similarity)\n\n        Important Notes:\n            - This\
  \ function delegates the actual computation to the parent class's forward method\n\
  \            - The cosine similarity loss encourages the student model's output\
  \ distribution to\n              align with the teacher's output distribution in\
  \ terms of directional similarity\n            - Memory usage is optimized through\
  \ chunked processing to handle large vocabulary sizes\n            - The function\
  \ is part of a custom autograd function and should be called via the\n         \
  \     apply() method rather than directly\n        \"\"\"\n        <your code>\n\
  \n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n     \
  \   Compute gradients for the fused linear cosine similarity distillation loss function.\n\
  \n        This static method implements the backward pass for the LigerFusedLinearCosineSimilarityFunction,\n\
  \        computing gradients with respect to the input parameters during backpropagation.\
  \ It delegates\n        the gradient computation to the parent class's backward\
  \ method and then formats the return\n        values to match the expected signature\
  \ by setting non-differentiable parameters to None.\n\n        Args:\n         \
  \   ctx: The context object that stores information from the forward pass, including\n\
  \                 saved tensors and other data needed for gradient computation.\n\
  \            grad_output (torch.Tensor): The gradient of the loss with respect to\
  \ the output\n                                       of the forward pass. This gradient\
  \ flows backward from\n                                       the subsequent layers\
  \ in the computation graph.\n\n        Returns:\n            tuple: A tuple containing\
  \ gradients for all input parameters of the forward method:\n                - Gradient\
  \ w.r.t. student_input (torch.Tensor or None)\n                - Gradient w.r.t.\
  \ student_weight (torch.Tensor or None)  \n                - Gradient w.r.t. teacher_input\
  \ (torch.Tensor or None)\n                - Gradient w.r.t. teacher_weight (torch.Tensor\
  \ or None)\n                - Gradient w.r.t. true_labels (None, as labels are not\
  \ differentiable)\n                - Gradient w.r.t. student_bias (torch.Tensor\
  \ or None)\n                - None for teacher_bias (non-differentiable parameter)\n\
  \                - None for weight_hard_loss (non-differentiable parameter)\n  \
  \              - None for weight_soft_loss (non-differentiable parameter)\n    \
  \            - None for beta (non-differentiable parameter)\n                - None\
  \ for ignore_index (non-differentiable parameter)\n                - None for temperature\
  \ (non-differentiable parameter)\n                - None for compiled (non-differentiable\
  \ parameter)\n                - None for chunk_size (non-differentiable parameter)\n\
  \n        Notes:\n            - This method is part of PyTorch's autograd system\
  \ and should not be called directly\n            - Only the first 6 gradients from\
  \ the parent class backward method are used\n            - All hyperparameters and\
  \ configuration options return None as they are not differentiable\n           \
  \ - The method assumes the parent class LigerFusedLinearDistillationBase properly\
  \ implements\n              the gradient computation for the core distillation loss\
  \ components\n        \"\"\"\n        <your code>\n\nclass LigerFusedLinearCosineSimilarityLoss(torch.nn.Module):\n\
  \    \"\"\"\n    A PyTorch neural network module that implements a fused linear\
  \ layer with cosine similarity-based distillation loss for knowledge distillation\
  \ tasks.\n    \n    This loss function combines hard loss (cross-entropy with true\
  \ labels) and soft loss (cosine similarity between student and teacher logits) to\
  \ enable efficient knowledge transfer from a teacher model to a student model. The\
  \ cosine similarity loss measures the angular similarity between normalized student\
  \ and teacher representations.\n    \n    Attributes:\n        weight_hard_loss\
  \ (float): Weight coefficient for the hard loss component (cross-entropy with true\
  \ labels). Default: 0.5.\n        weight_soft_loss (float): Weight coefficient for\
  \ the soft loss component (cosine similarity loss). Default: 0.5.\n        beta\
  \ (float): Coefficient for generalized cosine similarity loss in the interval [0,\
  \ 1]. Controls the strength of the cosine similarity penalty. Default: 0.5.\n  \
  \      ignore_index (int): Index to ignore in loss computation, typically used for\
  \ padding tokens. Default: -100.\n        temperature (float): Temperature parameter\
  \ for softmax scaling in distillation. Must be non-zero. Default: 1.0.\n       \
  \ compiled (bool): Whether to use compiled operations for performance optimization.\
  \ Default: True.\n        chunk_size (int): Size of chunks for memory-efficient\
  \ processing of large tensors. Default: 1024.\n    \n    Methods:\n        __init__:\
  \ Initializes the loss module with specified hyperparameters and validates temperature\
  \ is non-zero.\n        forward: Computes the combined distillation loss by applying\
  \ the fused linear cosine similarity function to student and teacher inputs, weights,\
  \ and true labels.\n    \n    Usage Example:\n        ```python\n        import\
  \ torch\n        \n        # Initialize the loss function\n        loss_fn = LigerFusedLinearCosineSimilarityLoss(\n\
  \            weight_hard_loss=0.7,\n            weight_soft_loss=0.3,\n        \
  \    beta=0.8,\n            temperature=3.0\n        )\n        \n        # Prepare\
  \ inputs\n        batch_size, seq_len, hidden_dim = 2, 10, 768\n        vocab_size\
  \ = 1000\n        \n        student_input = torch.randn(batch_size * seq_len, hidden_dim)\n\
  \        teacher_input = torch.randn(batch_size * seq_len, hidden_dim)\n       \
  \ student_weight = torch.randn(vocab_size, hidden_dim)\n        teacher_weight =\
  \ torch.randn(vocab_size, hidden_dim)\n        true_labels = torch.randint(0, vocab_size,\
  \ (batch_size * seq_len,))\n        \n        # Compute loss\n        loss = loss_fn(\n\
  \            student_input=student_input,\n            student_weight=student_weight,\n\
  \            teacher_input=teacher_input,\n            teacher_weight=teacher_weight,\n\
  \            true_labels=true_labels\n        )\n        \n        # Backpropagate\n\
  \        loss.backward()\n        ```\n    \"\"\"\n\n    def __init__(\n       \
  \ self,\n        weight_hard_loss: float = 0.5,\n        weight_soft_loss: float\
  \ = 0.5,\n        beta: float = 0.5,\n        ignore_index: int = -100,\n      \
  \  temperature: float = 1.0,\n        compiled: bool = True,\n        chunk_size:\
  \ int = 1024\n    ):\n        \"\"\"\n        Initialize the LigerFusedLinearCosineSimilarityLoss\
  \ module.\n\n        This constructor sets up a fused linear layer with cosine similarity-based\
  \ distillation loss,\n        which combines both hard loss (cross-entropy with\
  \ true labels) and soft loss (cosine similarity\n        between student and teacher\
  \ logits) for knowledge distillation in neural networks.\n\n        Parameters:\n\
  \            weight_hard_loss (float, optional): Weight coefficient for the hard\
  \ loss component \n                (cross-entropy loss with true labels). Must be\
  \ in range [0, 1]. Default: 0.5.\n            weight_soft_loss (float, optional):\
  \ Weight coefficient for the soft loss component \n                (cosine similarity\
  \ loss between student and teacher). Must be in range [0, 1]. Default: 0.5.\n  \
  \          beta (float, optional): Coefficient for generalized cosine similarity\
  \ computation \n                in the distillation loss. Controls the strength\
  \ of the cosine similarity penalty. Default: 0.5.\n            ignore_index (int,\
  \ optional): Index to ignore in loss computation, typically used \n            \
  \    for padding tokens. Default: -100.\n            temperature (float, optional):\
  \ Temperature parameter for softmax scaling in knowledge \n                distillation.\
  \ Higher values create softer probability distributions. Must be non-zero. Default:\
  \ 1.0.\n            compiled (bool, optional): Whether to use compiled operations\
  \ for potential performance \n                optimization. Default: True.\n   \
  \         chunk_size (int, optional): Size of chunks for processing large tensors\
  \ to manage \n                memory usage during computation. Default: 1024.\n\n\
  \        Raises:\n            AssertionError: If temperature is set to 0, which\
  \ would cause division by zero errors.\n\n        Notes:\n            - The total\
  \ loss is computed as: weight_hard_loss * hard_loss + weight_soft_loss * soft_loss\n\
  \            - The cosine similarity loss encourages the student model to produce\
  \ logits with similar \n              directional patterns to the teacher model\n\
  \            - Chunked processing helps manage memory for large vocabulary sizes\
  \ or long sequences\n            - The ignore_index parameter is useful for masked\
  \ language modeling where certain positions \n              should not contribute\
  \ to the loss\n        \"\"\"\n        <your code>\n\n    def forward(\n       \
  \ self,\n        student_input: torch.Tensor,\n        student_weight: torch.Tensor,\n\
  \        teacher_input: torch.Tensor,\n        teacher_weight: torch.Tensor,\n \
  \       true_labels: torch.LongTensor,\n        student_bias: torch.Tensor = None,\n\
  \        teacher_bias: torch.Tensor = None\n    ) -> torch.Tensor:\n        \"\"\
  \"\n        Compute the fused linear cosine similarity distillation loss between\
  \ student and teacher models.\n\n        This method performs a forward pass through\
  \ the fused linear cosine similarity loss function,\n        which combines both\
  \ hard loss (cross-entropy with true labels) and soft loss (cosine similarity\n\
  \        between student and teacher logits) for knowledge distillation. The computation\
  \ is optimized\n        using chunked processing to handle large vocabularies efficiently.\n\
  \n        Parameters:\n            student_input (torch.Tensor): Input tensor for\
  \ the student model, typically hidden states\n                from the student network.\
  \ Shape: (batch_size * seq_len, hidden_dim).\n            student_weight (torch.Tensor):\
  \ Weight matrix for the student model's linear layer.\n                Shape: (vocab_size,\
  \ hidden_dim).\n            teacher_input (torch.Tensor): Input tensor for the teacher\
  \ model, typically hidden states\n                from the teacher network. Shape:\
  \ (batch_size * seq_len, hidden_dim).\n            teacher_weight (torch.Tensor):\
  \ Weight matrix for the teacher model's linear layer.\n                Shape: (vocab_size,\
  \ hidden_dim).\n            true_labels (torch.LongTensor): Ground truth labels\
  \ for computing the hard loss.\n                Shape: (batch_size * seq_len,).\
  \ Values should be token indices.\n            student_bias (torch.Tensor, optional):\
  \ Bias vector for the student model's linear layer.\n                Shape: (vocab_size,).\
  \ Defaults to None.\n            teacher_bias (torch.Tensor, optional): Bias vector\
  \ for the teacher model's linear layer.\n                Shape: (vocab_size,). Defaults\
  \ to None.\n\n        Returns:\n            torch.Tensor: A scalar tensor representing\
  \ the combined distillation loss. The loss is\n                computed as a weighted\
  \ combination of hard loss (cross-entropy) and soft loss\n                (cosine\
  \ similarity), where the weights are determined by the instance's\n            \
  \    weight_hard_loss and weight_soft_loss parameters.\n\n        Notes:\n     \
  \       - The function uses the instance's configuration parameters (weight_hard_loss,\n\
  \              weight_soft_loss, beta, ignore_index, temperature, compiled, chunk_size)\
  \ that\n              were set during initialization.\n            - The cosine\
  \ similarity loss is computed using L2-normalized logits from both\n           \
  \   student and teacher models.\n            - Chunked processing is employed to\
  \ manage memory usage when dealing with large\n              vocabulary sizes.\n\
  \            - The temperature parameter is used for softmax temperature scaling\
  \ in the base\n              distillation computation.\n            - Tokens with\
  \ labels equal to ignore_index are excluded from loss computation.\n        \"\"\
  \"\n        <your code>\n"
interface_code_example: "class LigerFusedLinearCosineSimilarityFunction(LigerFusedLinearDistillationBase):\n\
  \    \"\"\"\n    A PyTorch autograd function that implements fused linear layer\
  \ computation with cosine similarity-based knowledge distillation loss.\n    \n\
  \    This class extends LigerFusedLinearDistillationBase to provide an efficient\
  \ implementation of knowledge distillation using cosine similarity loss between\
  \ student and teacher model outputs. It combines linear transformations with distillation\
  \ loss computation in a single fused operation for improved performance.\n    \n\
  \    Main Methods:\n        distillation_loss_fn(student_logits, teacher_logits,\
  \ beta=1.0):\n            Computes the cosine similarity loss between normalized\
  \ student and teacher logits.\n            The loss is calculated as beta * (1 -\
  \ cosine_similarity), where cosine similarity\n            measures the alignment\
  \ between the normalized logit vectors.\n    \n        forward(cls, ctx, student_input,\
  \ student_weight, teacher_input, teacher_weight, \n                true_labels,\
  \ student_bias, teacher_bias, **kwargs):\n            Performs the forward pass\
  \ by computing linear transformations for both student\n            and teacher\
  \ networks, then applies the combined hard loss (cross-entropy with\n          \
  \  true labels) and soft loss (cosine similarity distillation loss). Supports\n\
  \            chunked processing for memory efficiency.\n    \n        backward(ctx,\
  \ grad_output):\n            Computes gradients with respect to the input tensors\
  \ during backpropagation.\n            Only propagates gradients for trainable parameters\
  \ (student inputs/weights),\n            while setting gradients to None for hyperparameters\
  \ and teacher bias.\n    \n    Key Parameters:\n        - beta: Controls the strength\
  \ of the cosine similarity loss component\n        - weight_hard_loss/weight_soft_loss:\
  \ Balance between supervised and distillation losses\n        - temperature: Temperature\
  \ scaling for knowledge distillation\n        - chunk_size: Memory optimization\
  \ parameter for processing large tensors\n        - ignore_index: Label index to\
  \ ignore in loss computation\n    \n    Usage Example:\n        ```python\n    \
  \    # Used internally by LigerFusedLinearCosineSimilarityLoss\n        loss_fn\
  \ = LigerFusedLinearCosineSimilarityLoss(\n            weight_hard_loss=0.3,\n \
  \           weight_soft_loss=0.7,\n            beta=1.0,\n            temperature=3.0\n\
  \        )\n        \n        loss = loss_fn(\n            student_input=student_hidden_states,\n\
  \            student_weight=student_lm_head.weight,\n            teacher_input=teacher_hidden_states,\n\
  \            teacher_weight=teacher_lm_head.weight,\n            true_labels=target_labels\n\
  \        )\n        ```\n    \"\"\"\n\n    @staticmethod\n    def distillation_loss_fn(student_logits,\
  \ teacher_logits, beta = 1.0):\n        \"\"\"\n\n                Compute Cosine\
  \ loss (Cosine Similarity Loss).\n                Args:\n                    student_logits\
  \ (torch.Tensor): Logits of student tokens. Shape: (batch_size * seq_len,).\n  \
  \                  teacher_logits (torch.Tensor): Logits of teacher tokens. Shape:\
  \ (batch_size * seq_len,).\n                    beta: Coefficient beta of generalized\
  \ Cosine Similarity in the interval [0, 1]. Default: `1.0` (float): .\n        \
  \        Returns:\n                    torch.Tensor: cosine similarity loss\n\n\
  \        \"\"\"\n        <your code>\n..."
