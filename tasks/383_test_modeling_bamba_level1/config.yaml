base_image: pb-python310_nvidia-base_a48d8454
black_links:
- https://github.com/huggingface/transformers/
commit: null
docker_specs:
  run_args:
    cap add: []
    cuda_visible_devices: 0,1
    environment:
      PYTHONPATH: /testbed
install: python -m pip install --upgrade pip setuptools wheel && pip install -e '.[testing]'
  && echo 'Transformers环境设置完成'
instance_image: pb-instance_4702c5d8
library_name: transformers
pip_packages:
- numpy>=1.17
- packaging>=20.0
- pyyaml>=5.1
- regex!=2019.12.17
- requests
- tokenizers>=0.19,<0.20
- safetensors>=0.4.1
- huggingface-hub>=0.23.2,<1.0
- filelock
- tqdm>=4.27
- pytest>=7.2.0
- pytest-timeout
- pytest-xdist
- parameterized
- psutil
- Pillow<=15.0
- optuna
- ray[tune]
- sigopt
- timm
- datasets!=2.5.0
- accelerate>=0.21.0
- peft>=0.3.0
- bitsandbytes>0.37.0
python: '3.10'
repo_name: transformers
repository: huggingface/transformers
task_level: 1
task_name: transformers_modeling_bamba
task_statement: '## Task: Implement Hybrid Mamba-Attention Model with Dynamic Caching


  **Core Functionality:**

  Develop a hybrid neural network architecture that combines Mamba (state space model)
  layers with traditional attention layers for causal language modeling, featuring
  a specialized dynamic caching system.


  **Main Features & Requirements:**

  - **Dual Architecture Support**: Handle both attention layers (with sequence-dependent
  key-value caching) and Mamba layers (with fixed-size convolution/SSM state caching)

  - **Dynamic Cache Management**: Implement cache that adapts tensor shapes based
  on layer type - growing cache for attention layers, fixed cache for Mamba layers

  - **Generation Optimization**: Support efficient autoregressive text generation
  with proper cache reordering for beam search

  - **Configuration Flexibility**: Allow configurable layer arrangements (which layers
  use attention vs. Mamba) and model dimensions


  **Key Challenges:**

  - **Heterogeneous Caching**: Managing two fundamentally different cache types (sequence-growing
  vs. fixed-size) within a unified interface

  - **Memory Efficiency**: Balancing cache growth for attention layers while maintaining
  constant memory for Mamba layers

  - **Generation Compatibility**: Ensuring seamless integration with standard text
  generation pipelines and beam search mechanisms

  - **Performance Optimization**: Supporting multiple attention implementations (eager,
  SDPA, Flash Attention) while maintaining cache consistency'
technical_docs: []
test_cmd: pytest --no-header -rA --tb=short -p no:cacheprovider --timeout=50
test_code1: 'from transformers import BambaConfig

  from transformers import BambaForCausalLM

  from transformers import BambaModel

  from transformers.models.bamba.modeling_bamba import HybridMambaAttentionDynamicCache'
test_code_example: from transformers import BambaConfig
test_code_example_obj: BambaConfig
test_code_example_path: /testbed/src/transformers/models/bamba/configuration_bamba.py
test_description1: Below is **Test Description 1**
timeout: 50
interface_description1: 'Below is **Interface Description 1** for file: src-transformers-models-jamba-modeling_jamba.py


  This file contains 1 top-level interface(s) that need to be implemented.

  '
interface_code1: "class HybridMambaAttentionDynamicCache:\n    \"\"\"\n    \n    \
  \    A dynamic cache that can handle both the attention cache (which has a seq_len\
  \ dimension) and the mamba cache\n        (which has a constant shape regardless\
  \ of seq_len).\n    \n        This cache has two sets of lists of tensors: `key_cache`\
  \ and `value_cache` for attention cache and `conv_states`\n        and `ssm_states`\
  \ for mamba cache. Each of these lists has `num_layers` tensors. The expected shape\
  \ for each tensor\n        For attention layers, `key_cache` and `value_cache` have\
  \ a shape of `(batch_size, num_heads, seq_len, head_dim)`,\n        while `conv_states`\
  \ and `ssm_states` have a shape of `(batch_size, 0)` (empty tensors).\n        For\
  \ mamba layers, `key_cache` and `value_cache` have a shape of `(batch_size, 0)`\
  \ (empty tensors),\n        while `conv_states` represents the convolution state\
  \ and has a shape of `(batch_size, d_inner, d_conv)`,\n        and `ssm_states`\
  \ represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n\
  \        \n    \"\"\"\n\n    is_compileable = False\n\n    def __init__(\n     \
  \   self,\n        config,\n        batch_size,\n        dtype = torch.float16,\n\
  \        device = None\n    ):\n        \"\"\"\n        Initialize a HybridMambaAttentionDynamicCache\
  \ instance for handling both attention and Mamba layer caching.\n\n        This\
  \ cache manages two types of states: attention cache (key/value tensors with sequence\
  \ length dimension)\n        and Mamba cache (convolution and SSM states with fixed\
  \ dimensions). The cache structure adapts to the\n        model's layer configuration,\
  \ creating appropriate tensor shapes for each layer type.\n\n        Parameters:\n\
  \            config (JambaConfig): Model configuration containing layer specifications,\
  \ hidden dimensions,\n                and Mamba-specific parameters like expand\
  \ factor, state size, and convolution kernel size.\n            batch_size (int):\
  \ Batch size for initializing cache tensors. All cache tensors will have\n     \
  \           this as their first dimension.\n            dtype (torch.dtype, optional):\
  \ Data type for cache tensors. Defaults to torch.float16.\n                Should\
  \ match the model's computation dtype for optimal performance.\n            device\
  \ (torch.device or str, optional): Device where cache tensors will be allocated.\n\
  \                If None, tensors will be created on CPU. Should typically match\
  \ the model's device.\n\n        Important Notes:\n            - For attention layers:\
  \ Creates empty key_cache and value_cache tensors that will grow\n             \
  \ during generation, while conv_states and ssm_states remain empty placeholders.\n\
  \            - For Mamba layers: Creates fixed-size conv_states and ssm_states tensors\
  \ for maintaining\n              convolution and state space model states, while\
  \ key_cache and value_cache remain empty.\n            - The cache automatically\
  \ determines layer types from config.layers_block_type and\n              initializes\
  \ appropriate tensor shapes based on model architecture parameters.\n          \
  \  - Conv states have shape (batch_size, intermediate_size, conv_kernel_size).\n\
  \            - SSM states have shape (batch_size, intermediate_size, ssm_state_size).\n\
  \            - This cache type is required for Jamba models and is not compatible\
  \ with standard\n              transformer caching mechanisms.\n        \"\"\"\n\
  \        <your code>\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n\
  \        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs:\
  \ Optional[dict[str, Any]] = None\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n\
  \        \"\"\"\n        Updates the key-value cache for attention layers in the\
  \ Jamba hybrid model.\n\n        This method manages the dynamic cache by either\
  \ initializing new cache entries or concatenating\n        new key and value states\
  \ to existing cached states for a specific layer. It handles the\n        attention\
  \ cache component of the HybridMambaAttentionDynamicCache, which stores both attention\n\
  \        cache (with seq_len dimension) and mamba cache (with constant shape).\n\
  \n        Args:\n            key_states (torch.Tensor): New key states to cache.\
  \ Expected shape is \n                (batch_size, num_heads, seq_len, head_dim)\
  \ for attention layers.\n            value_states (torch.Tensor): New value states\
  \ to cache. Expected shape is\n                (batch_size, num_heads, seq_len,\
  \ head_dim) for attention layers.\n            layer_idx (int): Index of the layer\
  \ for which to update the cache. Must be a valid\n                layer index within\
  \ the model's layer range.\n            cache_kwargs (Optional[dict[str, Any]],\
  \ optional): Additional cache-related keyword\n                arguments. Currently\
  \ unused but provided for interface compatibility. Defaults to None.\n\n       \
  \ Returns:\n            tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n\
  \                - Updated key cache tensor for the specified layer\n          \
  \      - Updated value cache tensor for the specified layer\n                Both\
  \ tensors have shape (batch_size, num_heads, total_seq_len, head_dim) where\n  \
  \              total_seq_len includes both previously cached and newly added sequence\
  \ length.\n\n        Notes:\n            - For the first call on a layer, the cache\
  \ is initialized with the provided states\n            - For subsequent calls, new\
  \ states are concatenated along the sequence dimension (dim=2)\n            - This\
  \ method only handles attention layers; mamba layers use separate conv_states and\
  \ ssm_states\n            - The cache maintains the sequence dimension which grows\
  \ with each update\n            - Empty tensors are used as placeholders for non-attention\
  \ layers in the cache structure\n        \"\"\"\n        <your code>\n\n    def\
  \ reorder_cache(\n        self,\n        beam_idx: torch.LongTensor\n    ):\n  \
  \      \"\"\"\n        Reorders the cache for beam search, given the selected beam\
  \ indices.\n        \"\"\"\n        <your code>\n\n    def get_seq_length(\n   \
  \     self,\n        layer_idx: Optional[int] = 0\n    ) -> int:\n        \"\"\"\
  \n        Returns the sequence length of the cached states. A layer index can be\
  \ optionally passed.\n        \"\"\"\n        <your code>\n"
interface_description2: 'Below is **Interface Description 2** for file: src-transformers-models-bamba-configuration_bamba.py


  This file contains 1 top-level interface(s) that need to be implemented.

  '
interface_code2: "class BambaConfig(PretrainedConfig):\n    \"\"\"\n    \n       \
  \ This is the configuration class to store the configuration of a [`BambaModel`].\
  \ It is used to instantiate a\n        BambaModel model according to the specified\
  \ arguments, defining the model architecture. Instantiating a configuration\n  \
  \      with defaults taken from [ibm-fms/Bamba-9.8b-2.2T-hf](https://huggingface.co/ibm-fms/Bamba-9.8b-2.2T-hf).\n\
  \    \n        The BambaModel is a hybrid [mamba2](https://github.com/state-spaces/mamba)\
  \ architecture with SwiGLU.\n        The checkpoints are  jointly trained by IBM,\
  \ Princeton, and UIUC.\n    \n        Configuration objects inherit from [`PretrainedConfig`]\
  \ and can be used to control the model outputs. Read the\n        documentation\
  \ from [`PretrainedConfig`] for more information.\n    \n        Args:\n       \
  \     vocab_size (`int`, *optional*, defaults to 128000):\n                Vocabulary\
  \ size of the Bamba model. Defines the number of different tokens that can be represented\
  \ by the\n                `inputs_ids` passed when calling [`BambaModel`]\n    \
  \        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n      \
  \          Whether the model's input and output word embeddings should be tied.\
  \ Note that this is only relevant if the\n                model has an output word\
  \ embedding layer.\n            hidden_size (`int`, *optional*, defaults to 4096):\n\
  \                Dimension of the hidden representations.\n            intermediate_size\
  \ (`int`, *optional*, defaults to 14336):\n                Dimension of the MLP\
  \ representations.\n            num_hidden_layers (`int`, *optional*, defaults to\
  \ 32):\n                Number of hidden layers in the Transformer encoder.\n  \
  \          num_attention_heads (`int`, *optional*, defaults to 32):\n          \
  \      Number of attention heads for each attention layer in the Transformer encoder.\n\
  \            num_key_value_heads (`int`, *optional*, defaults to 8):\n         \
  \       This is the number of key_value heads that should be used to implement Grouped\
  \ Query Attention. If\n                `num_key_value_heads=num_attention_heads`,\
  \ the model will use Multi Head Attention (MHA), if\n                `num_key_value_heads=1`\
  \ the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n \
  \               converting a multi-head checkpoint to a GQA checkpoint, each group\
  \ key and value head should be constructed\n                by meanpooling all the\
  \ original heads within that group. For more details, check out [this\n        \
  \        paper](https://huggingface.co/papers/2305.13245). If it is not specified,\
  \ will default to `8`.\n            hidden_act (`str` or `function`, *optional*,\
  \ defaults to `\"silu\"`):\n                The non-linear activation function (function\
  \ or string) in the decoder.\n            initializer_range (`float`, *optional*,\
  \ defaults to 0.02):\n                The standard deviation of the truncated_normal_initializer\
  \ for initializing all weight matrices.\n            rms_norm_eps (`float`, *optional*,\
  \ defaults to 1e-05):\n                The epsilon used by the rms normalization\
  \ layers.\n            use_cache (`bool`, *optional*, defaults to `True`):\n   \
  \             Whether or not the model should return the last key/values attentions\
  \ (not used by all models). Only\n                relevant if `config.is_decoder=True`.\n\
  \            num_logits_to_keep (`int` or `None`, *optional*, defaults to 1):\n\
  \                Number of prompt logits to calculate during generation. If `None`,\
  \ all logits will be calculated. If an\n                integer value, only last\
  \ `num_logits_to_keep` logits will be calculated. Default is 1 because only the\n\
  \                logits of the last prompt token are needed for generation. For\
  \ long sequences, the logits for the entire\n                sequence may use a\
  \ lot of memory so, setting `num_logits_to_keep=1` will reduce memory footprint\n\
  \                significantly.\n            pad_token_id (`int`, *optional*, defaults\
  \ to 0):\n                The id of the padding token.\n            bos_token_id\
  \ (`int`, *optional*, defaults to 1):\n                The id of the \"beginning-of-sequence\"\
  \ token.\n            eos_token_id (`int`, *optional*, defaults to 2):\n       \
  \         The id of the \"end-of-sequence\" token.\n            max_position_embeddings\
  \ (`int`, *optional*, defaults to 262144):\n                Max cached sequence\
  \ length for the model\n            attention_dropout (`float`, *optional*, defaults\
  \ to 0.0):\n                The dropout ratio for the attention probabilities.\n\
  \            attn_layer_indices (`list`, *optional*):\n                Specifies\
  \ the layer indices that will have full attention. Must contain values at most num_hidden_layers.\n\
  \            mamba_n_heads (`int`, *optional*, defaults to 128):\n             \
  \   The number of mamba heads used in the v2 implementation.\n            mamba_d_head\
  \ (`int`, *optional*, defaults to `\"auto\"`):\n                Head embedding dimension\
  \ size\n            mamba_n_groups (`int`, *optional*, defaults to 1):\n       \
  \         The number of the mamba groups used in the v2 implementation.\n      \
  \      mamba_d_state (`int`, *optional*, defaults to 256):\n                The\
  \ dimension the mamba state space latents\n            mamba_d_conv (`int`, *optional*,\
  \ defaults to 4):\n                The size of the mamba convolution kernel\n  \
  \          mamba_expand (`int`, *optional*, defaults to 2):\n                Expanding\
  \ factor (relative to hidden_size) used to determine the mamba intermediate size\n\
  \            mamba_chunk_size (`int`, *optional*, defaults to 256):\n          \
  \      The chunks in which to break the sequence when doing prefill/training\n \
  \           mamba_conv_bias (`bool`, *optional*, defaults to `True`):\n        \
  \        Flag indicating whether or not to use bias in the convolution layer of\
  \ the mamba mixer block.\n            mamba_proj_bias (`bool`, *optional*, defaults\
  \ to `False`):\n                Flag indicating whether or not to use bias in the\
  \ input and output projections ([\"in_proj\", \"out_proj\"]) of the mamba mixer\
  \ block\n            z_loss_coefficient (`float`, *optional*, defaults to 0.0):\n\
  \                Coefficient for auxiliary z-loss used to control logit growth during\
  \ training\n    \n        \n    \"\"\"\n\n    model_type = \"bamba\"\n    keys_to_ignore_at_inference\
  \ = ['past_key_values']\n\n    def __init__(\n        self,\n        vocab_size\
  \ = 128000,\n        tie_word_embeddings = False,\n        hidden_size = 4096,\n\
  \        intermediate_size = 14336,\n        num_hidden_layers = 32,\n        num_attention_heads\
  \ = 32,\n        num_key_value_heads = 8,\n        hidden_act = 'silu',\n      \
  \  initializer_range = 0.02,\n        rms_norm_eps = 1e-05,\n        use_cache =\
  \ True,\n        num_logits_to_keep = 1,\n        pad_token_id = 0,\n        bos_token_id\
  \ = 1,\n        eos_token_id = 2,\n        max_position_embeddings = 262144,\n \
  \       attention_dropout = 0.0,\n        attn_layer_indices = None,\n        mamba_n_heads\
  \ = 128,\n        mamba_d_head = 'auto',\n        mamba_n_groups = 1,\n        mamba_d_state\
  \ = 256,\n        mamba_d_conv = 4,\n        mamba_expand = 2,\n        mamba_chunk_size\
  \ = 256,\n        mamba_conv_bias = True,\n        mamba_proj_bias = False,\n  \
  \      z_loss_coefficient = 0.0,\n        **kwargs\n    ):\n        \"\"\"\n   \
  \     Initialize a BambaConfig instance for configuring a Bamba model.\n\n     \
  \   This constructor sets up the configuration parameters for a Bamba model, which\
  \ is a hybrid\n        mamba2 architecture with SwiGLU. The configuration defines\
  \ the model architecture including\n        vocabulary size, hidden dimensions,\
  \ attention mechanisms, and mamba-specific parameters.\n\n        Parameters:\n\
  \            vocab_size (int, optional): Vocabulary size of the Bamba model. Defines\
  \ the number of \n                different tokens that can be represented by the\
  \ inputs_ids. Defaults to 128000.\n            tie_word_embeddings (bool, optional):\
  \ Whether the model's input and output word \n                embeddings should\
  \ be tied. Only relevant if the model has an output word embedding \n          \
  \      layer. Defaults to False.\n            hidden_size (int, optional): Dimension\
  \ of the hidden representations. Defaults to 4096.\n            intermediate_size\
  \ (int, optional): Dimension of the MLP representations. Defaults to 14336.\n  \
  \          num_hidden_layers (int, optional): Number of hidden layers in the Transformer\
  \ encoder. \n                Defaults to 32.\n            num_attention_heads (int,\
  \ optional): Number of attention heads for each attention layer \n             \
  \   in the Transformer encoder. Defaults to 32.\n            num_key_value_heads\
  \ (int, optional): Number of key_value heads for Grouped Query Attention.\n    \
  \            If equal to num_attention_heads, uses Multi Head Attention (MHA). If\
  \ 1, uses Multi \n                Query Attention (MQA). Otherwise uses GQA. Defaults\
  \ to 8.\n            hidden_act (str or function, optional): The non-linear activation\
  \ function in the decoder. \n                Defaults to 'silu'.\n            initializer_range\
  \ (float, optional): Standard deviation of the truncated_normal_initializer \n \
  \               for initializing all weight matrices. Defaults to 0.02.\n      \
  \      rms_norm_eps (float, optional): Epsilon used by the RMS normalization layers.\
  \ \n                Defaults to 1e-05.\n            use_cache (bool, optional):\
  \ Whether the model should return the last key/values attentions.\n            \
  \    Only relevant if config.is_decoder=True. Defaults to True.\n            num_logits_to_keep\
  \ (int or None, optional): Number of prompt logits to calculate during \n      \
  \          generation. If None, all logits calculated. If int, only last num_logits_to_keep\
  \ \n                logits calculated. Defaults to 1.\n            pad_token_id\
  \ (int, optional): ID of the padding token. Defaults to 0.\n            bos_token_id\
  \ (int, optional): ID of the \"beginning-of-sequence\" token. Defaults to 1.\n \
  \           eos_token_id (int, optional): ID of the \"end-of-sequence\" token. Defaults\
  \ to 2.\n            max_position_embeddings (int, optional): Maximum cached sequence\
  \ length for the model. \n                Defaults to 262144.\n            attention_dropout\
  \ (float, optional): Dropout ratio for attention probabilities. \n             \
  \   Defaults to 0.0.\n            attn_layer_indices (list, optional): Layer indices\
  \ that will have full attention. \n                Values must be at most num_hidden_layers.\
  \ Defaults to None.\n            mamba_n_heads (int, optional): Number of mamba\
  \ heads used in the v2 implementation. \n                Defaults to 128.\n    \
  \        mamba_d_head (int or str, optional): Head embedding dimension size. If\
  \ 'auto', \n                calculated as mamba_intermediate // mamba_n_heads. Defaults\
  \ to 'auto'.\n            mamba_n_groups (int, optional): Number of mamba groups\
  \ used in the v2 implementation. \n                Defaults to 1.\n            mamba_d_state\
  \ (int, optional): Dimension of the mamba state space latents. Defaults to 256.\n\
  \            mamba_d_conv (int, optional): Size of the mamba convolution kernel.\
  \ Defaults to 4.\n            mamba_expand (int, optional): Expanding factor (relative\
  \ to hidden_size) used to \n                determine the mamba intermediate size.\
  \ Defaults to 2.\n            mamba_chunk_size (int, optional): Chunks to break\
  \ the sequence when doing prefill/training. \n                Defaults to 256.\n\
  \            mamba_conv_bias (bool, optional): Whether to use bias in the convolution\
  \ layer of the \n                mamba mixer block. Defaults to True.\n        \
  \    mamba_proj_bias (bool, optional): Whether to use bias in the input and output\
  \ projections \n                of the mamba mixer block. Defaults to False.\n \
  \           z_loss_coefficient (float, optional): Coefficient for auxiliary z-loss\
  \ used to control \n                logit growth during training. Defaults to 0.0.\n\
  \            **kwargs: Additional keyword arguments passed to the parent PretrainedConfig\
  \ class.\n\n        Raises:\n            ValueError: If mamba_n_heads does not divide\
  \ (mamba_expand * hidden_size).\n            ValueError: If mamba_d_head * mamba_n_heads\
  \ does not equal mamba_intermediate.\n\n        Notes:\n            - The configuration\
  \ defaults are taken from ibm-fms/Bamba-9.8b-2.2T-hf model.\n            - For backward\
  \ compatibility, if num_key_value_heads is None, it defaults to num_attention_heads.\n\
  \            - When mamba_d_head is 'auto', it is automatically calculated as mamba_intermediate\
  \ // mamba_n_heads.\n            - The model uses RoPE (Rotary Position Embedding)\
  \ with theta=10000.0 and partial_rotary_factor=0.5.\n        \"\"\"\n        <your\
  \ code>\n\n    @property\n    def layers_block_type(self):\n        \"\"\"\n   \
  \     Get the block type for each layer in the Bamba model architecture.\n\n   \
  \     This property determines whether each layer in the model uses attention or\
  \ mamba blocks\n        based on the configuration. The Bamba model is a hybrid\
  \ architecture that can contain\n        both attention layers and mamba layers\
  \ at different positions.\n\n        Returns:\n            list[str]: A list of\
  \ strings with length equal to `num_hidden_layers`, where each\n               \
  \ element is either \"attention\" or \"mamba\". The value at index i indicates the\n\
  \                block type for layer i. If `attn_layer_indices` is specified and\
  \ contains\n                index i, then layer i will be \"attention\", otherwise\
  \ it will be \"mamba\".\n\n        Notes:\n            - This property is computed\
  \ dynamically based on the `attn_layer_indices` and\n              `num_hidden_layers`\
  \ configuration parameters\n            - If `attn_layer_indices` is None or empty,\
  \ all layers will be \"mamba\" type\n            - The `attn_layer_indices` should\
  \ contain values that are valid layer indices\n              (i.e., between 0 and\
  \ `num_hidden_layers - 1`)\n        \"\"\"\n        <your code>\n"
interface_description3: 'Below is **Interface Description 3** for file: src-transformers-models-bamba-modeling_bamba.py


  This file contains 3 top-level interface(s) that need to be implemented.

  '
interface_code3: "class HybridMambaAttentionDynamicCache:\n    \"\"\"\n    \n    \
  \    A dynamic cache that can handle both the attention cache (which has a seq_len\
  \ dimension) and the mamba cache\n        (which has a constant shape regardless\
  \ of seq_len).\n    \n        This cache has two sets of lists of tensors: `key_cache`\
  \ and `value_cache` for attention cache and `conv_states`\n        and `ssm_states`\
  \ for mamba cache. Each of these lists has `num_layers` tensors. The expected shape\
  \ for each tensor\n        For attention layers, `key_cache` and `value_cache` have\
  \ a shape of `(batch_size, num_heads, seq_len, head_dim)`,\n        while `conv_states`\
  \ and `ssm_states` have a shape of `(batch_size, 0)` (empty tensors).\n        For\
  \ mamba layers, `key_cache` and `value_cache` have a shape of `(batch_size, 0)`\
  \ (empty tensors),\n        while `conv_states` represents the convolution state\
  \ and has a shape of `(batch_size, d_inner, d_conv)`,\n        and `ssm_states`\
  \ represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n\
  \        \n    \"\"\"\n\n    is_compileable = False\n\n    def __init__(\n     \
  \   self,\n        config: BambaConfig,\n        batch_size,\n        dtype = torch.float16,\n\
  \        device = None\n    ):\n        \"\"\"\n        Initialize a HybridMambaAttentionDynamicCache\
  \ instance for handling both attention and mamba cache states.\n\n        This cache\
  \ manages two types of cache states:\n        - Attention cache: key_cache and value_cache\
  \ tensors with shape (batch_size, num_heads, seq_len, head_dim)\n        - Mamba\
  \ cache: conv_states and ssm_states tensors with fixed shapes regardless of sequence\
  \ length\n\n        For attention layers, the mamba cache tensors are empty, and\
  \ for mamba layers, the attention cache tensors are empty.\n\n        Args:\n  \
  \          config (BambaConfig): The model configuration containing layer specifications\
  \ and dimensions.\n            batch_size (int): The batch size for which to initialize\
  \ the cache tensors.\n            dtype (torch.dtype, optional): The data type for\
  \ cache tensors. Defaults to torch.float16.\n            device (torch.device, optional):\
  \ The device on which to create cache tensors. If None, uses default device.\n\n\
  \        Returns:\n            None: This is an __init__ method that initializes\
  \ the cache instance.\n\n        Notes:\n            - The cache structure depends\
  \ on config.layers_block_type which specifies whether each layer is \"mamba\" or\
  \ \"attention\"\n            - For mamba layers: conv_states shape is (batch_size,\
  \ expanded_hidden_size + 2*n_groups*state_size, conv_kernel_size)\n            \
  \                   ssm_states shape is (batch_size, n_heads, d_head, state_size)\n\
  \            - For attention layers: conv_states and ssm_states are empty tensors\n\
  \            - The has_previous_state flag is used internally by mamba layers to\
  \ track cache state\n            - All cache tensors are initialized with zeros\
  \ for mamba layers and empty tensors for attention layers\n        \"\"\"\n    \
  \    <your code>\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n\
  \        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs:\
  \ Optional[dict[str, Any]] = None\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n\
  \        \"\"\"\n        Updates the key and value cache for attention layers in\
  \ the hybrid Mamba-Attention dynamic cache.\n\n        This method manages the caching\
  \ mechanism for attention layers by either initializing the cache\n        with\
  \ new key-value states or concatenating new states to existing cached states along\
  \ the\n        sequence dimension.\n\n        Parameters:\n            key_states\
  \ (torch.Tensor): The key states tensor to be cached. Expected shape is\n      \
  \          (batch_size, num_heads, seq_len, head_dim) for attention layers.\n  \
  \          value_states (torch.Tensor): The value states tensor to be cached. Expected\
  \ shape is\n                (batch_size, num_heads, seq_len, head_dim) for attention\
  \ layers.\n            layer_idx (int): The index of the layer for which the cache\
  \ is being updated. Used to\n                access the correct cache entry in the\
  \ layer-specific cache lists.\n            cache_kwargs (Optional[dict[str, Any]],\
  \ optional): Additional cache-related keyword\n                arguments. Currently\
  \ unused but maintained for interface compatibility. Defaults to None.\n\n     \
  \   Returns:\n            tuple[torch.Tensor, torch.Tensor]: A tuple containing\
  \ the updated key and value cache\n                tensors for the specified layer.\
  \ Both tensors have shape \n                (batch_size, num_heads, total_seq_len,\
  \ head_dim) where total_seq_len includes\n                both previously cached\
  \ and newly added sequence tokens.\n\n        Important Notes:\n            - This\
  \ method is specifically designed for attention layers in the hybrid cache system\n\
  \            - For Mamba layers, the key_cache and value_cache contain empty tensors\n\
  \            - The method performs in-place updates to the cache lists\n       \
  \     - New key-value states are concatenated along dimension 2 (sequence length\
  \ dimension)\n            - If the cache is empty (shape[-1] == 0), it initializes\
  \ with the provided states\n            - The cache grows dynamically as new tokens\
  \ are processed during generation\n        \"\"\"\n        <your code>\n\n    def\
  \ reorder_cache(\n        self,\n        beam_idx: torch.LongTensor\n    ):\n  \
  \      \"\"\"\n        Reorders the cache for beam search, given the selected beam\
  \ indices.\n        \"\"\"\n        <your code>\n\n    def get_seq_length(\n   \
  \     self,\n        layer_idx: Optional[int] = 0\n    ) -> int:\n        \"\"\"\
  \n        Returns the sequence length of the cached states. A layer index can be\
  \ optionally passed.\n        \"\"\"\n        <your code>\n\n@auto_docstring\nclass\
  \ BambaModel(BambaPreTrainedModel):\n    \"\"\"\n    \"\"\"\n    A hybrid transformer-mamba\
  \ model that combines attention and mamba (state space model) layers.\n    \n  \
  \  BambaModel is the core model class that implements a hybrid architecture alternating\
  \ between\n    attention layers and mamba layers based on the configuration. It\
  \ processes input sequences\n    through embedding, multiple decoder layers, and\
  \ final normalization to produce contextualized\n    hidden representations.\n \
  \   \n    Main Attributes:\n        embed_tokens (nn.Embedding): Token embedding\
  \ layer that converts input IDs to dense vectors\n        layers (nn.ModuleList):\
  \ List of decoder layers, each being either attention or mamba type\n        final_layernorm\
  \ (BambaRMSNorm): RMS normalization applied to final hidden states\n        rotary_emb\
  \ (BambaRotaryEmbedding): Rotary position embeddings shared across attention layers\n\
  \        padding_idx (int): Index used for padding tokens\n        vocab_size (int):\
  \ Size of the vocabulary\n    \n    Main Methods:\n        forward: Main forward\
  \ pass that processes input through all layers and returns model outputs.\n    \
  \        Handles both training and inference modes, supports caching for efficient\
  \ generation,\n            and can output attention weights and hidden states from\
  \ intermediate layers.\n        \n        _update_causal_mask: Creates and updates\
  \ 4D causal attention masks for attention layers,\n            handling different\
  \ attention implementations (eager, SDPA, flash attention).\n        \n        _update_mamba_mask:\
  \ Updates 2D attention masks specifically for mamba layers, with\n            optimizations\
  \ for cached forward passes and full attention scenarios.\n        \n        _prepare_4d_causal_attention_mask_with_cache_position:\
  \ Static method that converts 2D\n            attention masks to 4D causal masks\
  \ with proper cache position handling.\n    \n    Usage Examples:\n        ```python\n\
  \        from transformers import BambaModel, BambaConfig\n        \n        # Initialize\
  \ model with hybrid attention-mamba architecture\n        config = BambaConfig(\n\
  \            hidden_size=768,\n            num_hidden_layers=12,\n            layers_block_type=[\"\
  attention\", \"mamba\"] * 6  # Alternating layers\n        )\n        model = BambaModel(config)\n\
  \        \n        # Forward pass\n        input_ids = torch.randint(0, 1000, (2,\
  \ 10))  # batch_size=2, seq_len=10\n        outputs = model(input_ids)\n       \
  \ hidden_states = outputs.last_hidden_state\n        \n        # With caching for\
  \ generation\n        from transformers.models.bamba.modeling_bamba import HybridMambaAttentionDynamicCache\n\
  \        cache = HybridMambaAttentionDynamicCache(config, batch_size=2, device=model.device)\n\
  \        outputs = model(input_ids, past_key_values=cache, use_cache=True)\n   \
  \     ```\n    \n    The model supports advanced features like gradient checkpointing,\
  \ different attention implementations\n    (eager, SDPA, flash attention), and specialized\
  \ caching mechanisms for efficient autoregressive generation.\n    \"\"\"\n    \"\
  \"\"\n\n    def __init__(self, config: BambaConfig):\n        \"\"\"\n        Initialize\
  \ a Bamba model with the given configuration.\n\n        This constructor sets up\
  \ all the necessary components for a Bamba model, including:\n        - Model configuration\
  \ and basic parameters (padding index, vocabulary size)\n        - Token embedding\
  \ layer for converting input token IDs to embeddings\n        - Decoder layers with\
  \ hybrid Mamba/Attention architecture based on the layer configuration\n       \
  \ - Final layer normalization and rotary position embeddings\n        - Gradient\
  \ checkpointing support\n\n        Args:\n            config (BambaConfig): Configuration\
  \ object containing all model hyperparameters\n                and architectural\
  \ settings. This includes:\n                - Model dimensions (hidden_size, vocab_size,\
  \ num_hidden_layers)\n                - Layer type configuration (layers_block_type)\
  \ specifying which layers\n                  use Mamba vs attention mechanisms\n\
  \                - Normalization parameters (rms_norm_eps)\n                - Attention\
  \ implementation settings (_attn_implementation)\n                - Token padding\
  \ configuration (pad_token_id)\n\n        Returns:\n            None: This is a\
  \ constructor method that initializes the model instance.\n\n        Important Notes:\n\
  \            - The model uses a hybrid architecture where each layer can be either\
  \ a Mamba\n              layer or an attention layer, as specified in config.layers_block_type\n\
  \            - Rotary position embeddings are initialized for attention layers\n\
  \            - The model supports gradient checkpointing for memory-efficient training\n\
  \            - Post-initialization processing (weight initialization) is automatically\n\
  \              called after construction\n            - The attention implementation\
  \ can be configured via config._attn_implementation\n              to use different\
  \ backends (eager, flash_attention_2, sdpa)\n        \"\"\"\n        <your code>\n\
  \n    @can_return_tuple\n    @auto_docstring\n    def forward(\n        self,\n\
  \        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask:\
  \ Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor]\
  \ = None,\n        past_key_values: Optional[HybridMambaAttentionDynamicCache] =\
  \ None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache:\
  \ Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n  \
  \      output_hidden_states: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor]\
  \ = None,\n        **kwargs: Unpack[BambaFlashAttentionKwargs]\n    ) -> BaseModelOutputWithPast:\n\
  \        \"\"\"\n        Performs a forward pass through the Bamba model, processing\
  \ input sequences through hybrid Mamba-Attention layers.\n\n        This method\
  \ handles the core forward computation of the Bamba model, which combines Mamba\
  \ (selective state space) layers and traditional attention layers in a hybrid architecture.\
  \ It processes input tokens through embedding, applies positional encodings, and\
  \ passes through decoder layers to produce contextualized representations.\n\n \
  \       Args:\n            input_ids (torch.LongTensor, optional): \n          \
  \      Indices of input sequence tokens in the vocabulary of shape `(batch_size,\
  \ sequence_length)`. \n                Cannot be provided simultaneously with `inputs_embeds`.\n\
  \            attention_mask (torch.Tensor, optional): \n                Mask to\
  \ avoid performing attention on padding token indices of shape `(batch_size, sequence_length)`.\
  \ \n                Mask values are in `[0, 1]`: 1 for tokens that are NOT MASKED,\
  \ 0 for MASKED tokens.\n            position_ids (torch.LongTensor, optional): \n\
  \                Indices of positions of each input sequence token in the position\
  \ embeddings of shape \n                `(batch_size, sequence_length)`. If None,\
  \ will be automatically generated.\n            past_key_values (HybridMambaAttentionDynamicCache,\
  \ optional): \n                Pre-computed hidden states (keys, values, conv states,\
  \ and SSM states) that can be used to speed up \n                sequential decoding.\
  \ Contains both attention cache and Mamba cache components.\n            inputs_embeds\
  \ (torch.FloatTensor, optional): \n                Embedded representation of input\
  \ tokens of shape `(batch_size, sequence_length, hidden_size)`. \n             \
  \   Cannot be provided simultaneously with `input_ids`.\n            use_cache (bool,\
  \ optional): \n                If set to True, past_key_values are returned and\
  \ can be used to speed up decoding. \n                Defaults to model configuration\
  \ value.\n            output_attentions (bool, optional): \n                Whether\
  \ to return attention weights from attention layers. Note that Mamba layers do not\
  \ produce \n                attention weights. Defaults to model configuration value.\n\
  \            output_hidden_states (bool, optional): \n                Whether to\
  \ return hidden states from all layers. Defaults to model configuration value.\n\
  \            cache_position (torch.LongTensor, optional): \n                Indices\
  \ depicting the position of input sequence tokens in the sequence of shape `(sequence_length,)`.\
  \ \n                Used for caching and positional embeddings.\n            **kwargs:\
  \ \n                Additional keyword arguments of type `BambaFlashAttentionKwargs`\
  \ for advanced Flash Attention usage, \n                including `cu_seq_lens_q`,\
  \ `cu_seq_lens_k`, `max_length_q`, `max_length_k`, and `seq_idx` for \n        \
  \        padding-free training and optimized kernel usage.\n\n        Returns:\n\
  \            BaseModelOutputWithPast: A dataclass containing:\n                -\
  \ last_hidden_state (torch.FloatTensor): Hidden states from the last layer of shape\
  \ \n                  `(batch_size, sequence_length, hidden_size)`\n           \
  \     - past_key_values (HybridMambaAttentionDynamicCache): Updated cache containing\
  \ attention and Mamba states\n                - hidden_states (tuple of torch.FloatTensor,\
  \ optional): Hidden states from all layers if \n                  `output_hidden_states=True`\n\
  \                - attentions (tuple of torch.FloatTensor, optional): Attention\
  \ weights from attention layers if \n                  `output_attentions=True`\n\
  \n        Raises:\n            ValueError: If both `input_ids` and `inputs_embeds`\
  \ are provided or if neither is provided.\n\n        Notes:\n            - The model\
  \ uses a hybrid architecture combining Mamba (selective state space) and attention\
  \ layers\n            - Mamba layers use a specialized cache structure different\
  \ from traditional attention caches\n            - When using gradient checkpointing\
  \ during training, `use_cache` will be automatically set to False\n            -\
  \ The model automatically handles different attention implementations (eager, flash_attention_2,\
  \ sdpa)\n            - For Mamba layers, a 2D attention mask is used, while attention\
  \ layers use a 4D causal mask\n        \"\"\"\n        <your code>\n\n    def _update_causal_mask(\n\
  \        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n\
  \        cache_position: torch.Tensor,\n        past_key_values: HybridMambaAttentionDynamicCache,\n\
  \        output_attentions: bool\n    ):\n        \"\"\"\n        Updates the causal\
  \ attention mask for attention layers in the hybrid Bamba model.\n\n        This\
  \ method generates and processes the appropriate attention mask based on the attention\
  \ implementation\n        being used (eager, SDPA, or Flash Attention 2). It handles\
  \ different scenarios including cached\n        generation, training vs inference,\
  \ and various mask formats.\n\n        Parameters:\n            attention_mask (torch.Tensor):\
  \ The input attention mask of shape (batch_size, sequence_length)\n            \
  \    where padding elements are indicated by 0. Can be None if no masking is needed.\n\
  \            input_tensor (torch.Tensor): The input tensor to the model, used to\
  \ determine batch size,\n                sequence length, and dtype for mask generation.\n\
  \            cache_position (torch.Tensor): Tensor containing indices depicting\
  \ the position of input\n                sequence tokens in the sequence, used for\
  \ cache-aware mask generation.\n            past_key_values (HybridMambaAttentionDynamicCache):\
  \ The dynamic cache containing both\n                attention cache (key/value\
  \ states) and mamba cache (conv/ssm states). Used to determine\n               \
  \ the number of past tokens seen.\n            output_attentions (bool): Whether\
  \ attention weights will be returned. Affects SDPA\n                implementation\
  \ choice and mask processing.\n\n        Returns:\n            torch.Tensor or None:\
  \ The processed causal attention mask of shape \n                (batch_size, 1,\
  \ query_length, key_value_length) for eager/SDPA implementations,\n            \
  \    or the original 2D mask for Flash Attention 2, or None if no mask is needed.\n\
  \n        Important notes:\n            - For Flash Attention 2: Returns None unless\
  \ the attention_mask contains padding (0.0 values)\n            - For SDPA: May\
  \ return None when causal masking can be handled implicitly via is_causal argument\n\
  \            - For eager implementation: Always returns a 4D causal mask\n     \
  \       - The method handles both 2D input masks (converted to 4D causal) and pre-existing\
  \ 4D masks\n            - Applies special handling for SDPA on CUDA/XPU/NPU devices\
  \ to ensure memory-efficient attention compatibility\n            - The mask values\
  \ use the minimum representable value for the tensor dtype to indicate masked positions\n\
  \        \"\"\"\n        <your code>\n\n    @staticmethod\n    def _prepare_4d_causal_attention_mask_with_cache_position(\n\
  \        attention_mask: torch.Tensor,\n        sequence_length: int,\n        target_length:\
  \ int,\n        dtype: torch.dtype,\n        cache_position: torch.Tensor,\n   \
  \     batch_size: int,\n        **kwargs\n    ):\n        \"\"\"\n\n           \
  \     Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)`\
  \ from a 2D mask of shape\n                `(batch_size, key_value_length)`, or\
  \ if the input `attention_mask` is already 4D, do nothing.\n\n                Args:\n\
  \                    attention_mask (`torch.Tensor`):\n                        A\
  \ 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention\
  \ mask of shape\n                        `(batch_size, 1, query_length, key_value_length)`.\n\
  \                    sequence_length (`int`):\n                        The sequence\
  \ length being processed.\n                    target_length (`int`):\n        \
  \                The target length: when generating with static cache, the mask\
  \ should be as long as the static cache,\n                        to account for\
  \ the 0 padding, the part of the cache that is not filled yet.\n               \
  \     dtype (`torch.dtype`):\n                        The dtype to use for the 4D\
  \ attention mask.\n                    cache_position (`torch.Tensor`):\n      \
  \                  Indices depicting the position of the input sequence tokens in\
  \ the sequence.\n                    batch_size (`torch.Tensor`):\n            \
  \            Batch size.\n\n        \"\"\"\n        <your code>\n\n    def _update_mamba_mask(self,\
  \ attention_mask, cache_position):\n        \"\"\"\n\n                No need for\
  \ zeroing states when\n                    1. Cached forward\n                 \
  \   2. Attending to all inputs\n\n        \"\"\"\n        <your code>\n\n@auto_docstring\n\
  class BambaForCausalLM(BambaPreTrainedModel, GenerationMixin):\n    \"\"\"\n   \
  \ A hybrid transformer model that combines Mamba (state space model) and attention\
  \ mechanisms for causal language modeling tasks. This model implements the Bamba\
  \ architecture which alternates between Mamba layers for efficient sequence processing\
  \ and attention layers for capturing long-range dependencies.\n    \n    BambaForCausalLM\
  \ is designed for autoregressive text generation and can be used for tasks such\
  \ as language modeling, text completion, and conversational AI. The model leverages\
  \ a specialized hybrid cache system (HybridMambaAttentionDynamicCache) to efficiently\
  \ handle the different caching requirements of Mamba and attention layers.\n   \
  \ \n    Args:\n        config (BambaConfig): Model configuration containing hyperparameters\
  \ and architecture settings.\n    \n    Attributes:\n        model (BambaModel):\
  \ The core Bamba model containing embedding layers, decoder layers, and normalization.\n\
  \        vocab_size (int): Size of the vocabulary.\n        lm_head (nn.Linear):\
  \ Linear layer that projects hidden states to vocabulary logits for next token prediction.\n\
  \        z_loss_coefficient (float): Coefficient for auxiliary z-loss regularization\
  \ to improve training stability.\n    \n    Methods:\n        forward: Performs\
  \ forward pass through the model, computing logits and optionally loss for training.\n\
  \            Handles both training (with labels) and inference modes. Supports various\
  \ output options\n            including hidden states and attention weights.\n \
  \       \n        prepare_inputs_for_generation: Prepares model inputs for text\
  \ generation, handling cache management\n            and input slicing for efficient\
  \ autoregressive generation. Creates and manages the hybrid\n            cache system\
  \ required for the mixed Mamba-attention architecture.\n    \n    Example:\n   \
  \     ```python\n        from transformers import AutoTokenizer, BambaForCausalLM\n\
  \        \n        # Load model and tokenizer\n        model = BambaForCausalLM.from_pretrained(\"\
  bamba-model-name\")\n        tokenizer = AutoTokenizer.from_pretrained(\"bamba-model-name\"\
  )\n        \n        # Text generation\n        prompt = \"The future of AI is\"\
  \n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        \n        #\
  \ Generate text\n        with torch.no_grad():\n            outputs = model.generate(\n\
  \                inputs.input_ids,\n                max_length=50,\n           \
  \     temperature=0.7,\n                do_sample=True\n            )\n        \n\
  \        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\
  \        print(generated_text)\n        \n        # Training mode with labels\n\
  \        labels = inputs.input_ids.clone()\n        outputs = model(input_ids=inputs.input_ids,\
  \ labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\
  \        ```\n    \n    Note:\n        This model requires specific dependencies\
  \ (mamba_ssm, causal_conv1d) for optimal performance.\n        When these are available\
  \ and running on GPU, the model uses optimized CUDA kernels.\n        Otherwise,\
  \ it falls back to a PyTorch implementation with reduced performance.\n    \"\"\"\
  \n\n    _tied_weights_keys = ['lm_head.weight']\n    _tp_plan = {'lm_head': 'colwise_rep'}\n\
  \    _pp_plan = {'lm_head': (['hidden_states'], ['logits'])}\n\n    def __init__(self,\
  \ config):\n        \"\"\"\n        Initialize a Bamba model configuration instance.\n\
  \n        This constructor sets up the configuration object that defines the architecture\
  \ and hyperparameters\n        for a Bamba model, which is a hybrid architecture\
  \ combining Mamba (selective state space) layers\n        and traditional attention\
  \ layers.\n\n        Parameters:\n            config (BambaConfig): Configuration\
  \ object containing all the hyperparameters and settings\n                for the\
  \ Bamba model. This includes parameters such as:\n                - Model dimensions\
  \ (hidden_size, vocab_size, num_hidden_layers)\n                - Attention configuration\
  \ (num_attention_heads, num_key_value_heads)\n                - Mamba-specific parameters\
  \ (mamba_d_state, mamba_d_conv, mamba_expand, etc.)\n                - Layer type\
  \ specification (layers_block_type)\n                - Normalization and activation\
  \ settings\n                - Training-specific parameters (dropout rates, bias\
  \ settings)\n\n        Return value:\n            None: This is a constructor method\
  \ that initializes the instance.\n\n        Important notes:\n            - This\
  \ is an __init__ method for what appears to be a Bamba model class\n           \
  \ - The config parameter must be a valid BambaConfig instance with all required\
  \ attributes\n            - This file is auto-generated from modular_bamba.py and\
  \ should not be edited directly\n            - The Bamba architecture requires specific\
  \ dependencies (mamba_ssm, causal_conv1d) for optimal performance\n            -\
  \ The configuration will determine whether layers use Mamba or attention mechanisms\
  \ based on layers_block_type\n        \"\"\"\n        <your code>\n\n    @can_return_tuple\n\
  \    @auto_docstring\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor]\
  \ = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids:\
  \ Optional[torch.LongTensor] = None,\n        past_key_values: Optional[HybridMambaAttentionDynamicCache]\
  \ = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels:\
  \ Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n\
  \        output_attentions: Optional[bool] = None,\n        output_hidden_states:\
  \ Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n\
  \        logits_to_keep: Union[int, torch.Tensor] = 0,\n        **kwargs\n    )\
  \ -> CausalLMOutputWithPast:\n        \"\"\"\n\n                labels (`torch.LongTensor`\
  \ of shape `(batch_size, sequence_length)`, *optional*):\n                    Labels\
  \ for computing the masked language modeling loss. Indices should either be in `[0,\
  \ ...,\n                    config.vocab_size]` or -100 (see `input_ids` docstring).\
  \ Tokens with indices set to `-100` are ignored\n                    (masked), the\
  \ loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\
  \n                Example:\n\n                ```python\n                >>> from\
  \ transformers import AutoTokenizer, BambaForCausalLM\n\n                >>> model\
  \ = BambaForCausalLM.from_pretrained(\"...\")\n                >>> tokenizer = AutoTokenizer.from_pretrained(\"\
  ...\")\n\n                >>> prompt = \"Hey, are you conscious? Can you talk to\
  \ me?\"\n                >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\
  \n                >>> # Generate\n                >>> generate_ids = model.generate(inputs.input_ids,\
  \ max_length=30)\n                >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\
  \ clean_up_tokenization_spaces=False)[0]\n                \"Hey, are you conscious?\
  \ Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n          \
  \      ```\n        \"\"\"\n        <your code>\n\n    def prepare_inputs_for_generation(\n\
  \        self,\n        input_ids,\n        past_key_values = None,\n        attention_mask\
  \ = None,\n        inputs_embeds = None,\n        cache_position = None,\n     \
  \   position_ids = None,\n        use_cache = True,\n        **kwargs\n    ):\n\
  \        \"\"\"\n        Prepares inputs for text generation by processing and formatting\
  \ various input parameters.\n\n        This method handles the preparation of input\
  \ tensors for generation tasks, including managing\n        cached states, attention\
  \ masks, position IDs, and input embeddings. It performs special handling\n    \
  \    for the HybridMambaAttentionDynamicCache used by the Bamba model architecture.\n\
  \n        Parameters:\n            input_ids (torch.LongTensor, optional): \n  \
  \              Input token IDs of shape (batch_size, sequence_length). Contains\
  \ the tokenized input text.\n            past_key_values (HybridMambaAttentionDynamicCache,\
  \ optional): \n                Cached key-value states from previous generation\
  \ steps. If None, a new cache will be \n                initialized for the current\
  \ batch.\n            attention_mask (torch.Tensor, optional): \n              \
  \  Attention mask of shape (batch_size, sequence_length) where 1 indicates tokens\
  \ to attend \n                to and 0 indicates padding tokens to ignore.\n   \
  \         inputs_embeds (torch.FloatTensor, optional): \n                Pre-computed\
  \ input embeddings of shape (batch_size, sequence_length, hidden_size). \n     \
  \           If provided, will be used instead of input_ids for the first generation\
  \ step.\n            cache_position (torch.LongTensor, optional): \n           \
  \     Position indices indicating which tokens in the sequence are being processed\
  \ in the \n                current forward pass.\n            position_ids (torch.LongTensor,\
  \ optional): \n                Position indices for each token in the input sequence.\
  \ If None and attention_mask is \n                provided, position_ids will be\
  \ automatically generated.\n            use_cache (bool, optional): \n         \
  \       Whether to use caching for faster generation. Defaults to True.\n      \
  \      **kwargs: \n                Additional keyword arguments passed to the method.\n\
  \n        Returns:\n            dict: A dictionary containing the prepared model\
  \ inputs with the following keys:\n                - 'input_ids' or 'inputs_embeds':\
  \ The processed input tokens or embeddings\n                - 'position_ids': Position\
  \ indices for the input tokens\n                - 'past_key_values': The cache object\
  \ (newly created if was None)\n                - 'use_cache': Boolean flag for cache\
  \ usage\n                - 'attention_mask': The attention mask tensor\n       \
  \         - 'logits_to_keep': Number of logits to keep from config\n           \
  \     - 'cache_position': Position indices for caching\n\n        Important Notes:\n\
  \            - This method performs intelligent slicing of input_ids when cache\
  \ is present to only \n              process unprocessed tokens\n            - Creates\
  \ a new HybridMambaAttentionDynamicCache if none is provided\n            - Handles\
  \ special cases for input_embeds usage and cache position boundaries\n         \
  \   - Automatically generates position_ids from attention_mask when not provided\n\
  \            - The returned input_ids tensor is made contiguous for compilation\
  \ compatibility\n        \"\"\"\n        <your code>\n"
interface_code_example: "class BambaConfig(PretrainedConfig):\n    \"\"\"\n    \n\
  \        This is the configuration class to store the configuration of a [`BambaModel`].\
  \ It is used to instantiate a\n        BambaModel model according to the specified\
  \ arguments, defining the model architecture. Instantiating a configuration\n  \
  \      with defaults taken from [ibm-fms/Bamba-9.8b-2.2T-hf](https://huggingface.co/ibm-fms/Bamba-9.8b-2.2T-hf).\n\
  \    \n        The BambaModel is a hybrid [mamba2](https://github.com/state-spaces/mamba)\
  \ architecture with SwiGLU.\n        The checkpoints are  jointly trained by IBM,\
  \ Princeton, and UIUC.\n    \n        Configuration objects inherit from [`PretrainedConfig`]\
  \ and can be used to control the model outputs. Read the\n        documentation\
  \ from [`PretrainedConfig`] for more information.\n    \n        Args:\n       \
  \     vocab_size (`int`, *optional*, defaults to 128000):\n                Vocabulary\
  \ size of the Bamba model. Defines the number of different tokens that can be represented\
  \ by the\n                `inputs_ids` passed when calling [`BambaModel`]\n    \
  \        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n      \
  \          Whether the model's input and output word embeddings should be tied.\
  \ Note that this is only relevant if the\n                model has an output word\
  \ embedding layer.\n            hidden_size (`int`, *optional*, defaults to 4096):\n\
  \                Dimension of the hidden representations.\n            intermediate_size\
  \ (`int`, *optional*, defaults to 14336):\n                Dimension of the MLP\
  \ representations.\n            num_hidden_layers (`int`, *optional*, defaults to\
  \ 32):\n                Number of hidden layers in the Transformer encoder.\n  \
  \          num_attention_heads (`int`, *optional*, defaults to 32):\n          \
  \      Number of attention heads for each attention layer in the Transformer encoder.\n\
  \            num_key_value_heads (`int`, *optional*, defaults to 8):\n         \
  \       This is the number of key_value heads that should be used to implement Grouped\
  \ Query Attention. If\n                `num_key_value_heads=num_attention_heads`,\
  \ the model will use Multi Head Attention (MHA), if\n                `num_key_value_heads=1`\
  \ the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n \
  \               converting a multi-head checkpoint to a GQA checkpoint, each group\
  \ key and value head should be constructed\n                by meanpooling all the\
  \ original heads within that group. For more details, check out [this\n        \
  \        paper](https://huggingface.co/papers/2305.13245). If it is not specified,\
  \ will default to `8`.\n            hidden_act (`str` or `function`, *optional*,\
  \ defaults to `\"silu\"`):\n                The non-linear activation function (function\
  \ or string) in the decoder.\n            initializer_range (`float`, *optional*,\
  \ defaults to 0.02):\n                The standard deviation of the truncated_normal_initializer\
  \ for initializing all weight matrices.\n            rms_norm_eps (`float`, *optional*,\
  \ defaults to 1e-05):\n                The epsilon used by the rms normalization\
  \ layers.\n            use_cache (`bool`, *optional*, defaults to `True`):\n   \
  \             Whether or not the model should return the last key/values attentions\
  \ (not used by all models). Only\n                relevant if `config.is_decoder=True`.\n\
  \            num_logits_to_keep (`int` or `None`, *optional*, defaults to 1):\n\
  \                Number of prompt logits to calculate during generation. If `None`,\
  \ all logits will be calculated. If an\n                integer value, only last\
  \ `num_logits_to_keep` logits will be calculated. Default is 1 because only the\n\
  \                logits of the last prompt token are needed for generation. For\
  \ long sequences, the logits for the entire\n                sequence may use a\
  \ lot of memory so, setting `num_logits_to_keep=1` will reduce memory footprint\n\
  \                significantly.\n            pad_token_id (`int`, *optional*, defaults\
  \ to 0):\n                The id of the padding token.\n            bos_token_id\
  \ (`int`, *optional*, defaults to 1):\n                The id of the \"beginning-of-sequence\"\
  \ token.\n            eos_token_id (`int`, *optional*, defaults to 2):\n       \
  \         The id of the \"end-of-sequence\" token.\n            max_position_embeddings\
  \ (`int`, *optional*, defaults to 262144):\n                Max cached sequence\
  \ length for the model\n            attention_dropout (`float`, *optional*, defaults\
  \ to 0.0):\n                The dropout ratio for the attention probabilities.\n\
  \            attn_layer_indices (`list`, *optional*):\n                Specifies\
  \ the layer indices that will have full attention. Must contain values at most num_hidden_layers.\n\
  \            mamba_n_heads (`int`, *optional*, defaults to 128):\n             \
  \   The number of mamba heads used in the v2 implementation.\n            mamba_d_head\
  \ (`int`, *optional*, defaults to `\"auto\"`):\n                Head embedding dimension\
  \ size\n            mamba_n_groups (`int`, *optional*, defaults to 1):\n       \
  \         The number of the mamba groups used in the v2 implementation.\n      \
  \      mamba_d_state (`int`, *optional*, defaults to 256):\n                The\
  \ dimension the mamba state space latents\n            mamba_d_conv (`int`, *optional*,\
  \ defaults to 4):\n                The size of the mamba convolution kernel\n  \
  \          mamba_expand (`int`, *optional*, defaults to 2):\n                Expanding\
  \ factor (relative to hidden_size) used to determine the mamba intermediate size\n\
  \            mamba_chunk_size (`int`, *optional*, defaults to 256):\n          \
  \      The chunks in which to break the sequence when doing prefill/training\n \
  \           mamba_conv_bias (`bool`, *optional*, defaults to `True`):\n        \
  \        Flag indicating whether or not to use bias in the convolution layer of\
  \ the mamba mixer block.\n            mamba_proj_bias (`bool`, *optional*, defaults\
  \ to `False`):\n                Flag indicating whether or not to use bias in the\
  \ input and output projections ([\"in_proj\", \"out_proj\"]) of the mamba mixer\
  \ block\n            z_loss_coefficient (`float`, *optional*, defaults to 0.0):\n\
  \                Coefficient for auxiliary z-loss used to control logit growth during\
  \ training\n    \n        \n    \"\"\"\n\n    model_type = \"bamba\"\n    keys_to_ignore_at_inference\
  \ = ['past_key_values']\n\n    def __init__(\n        self,\n        vocab_size\
  \ = 128000,\n        tie_word_embeddings = False,\n        hidden_size = 4096,\n\
  \        intermediate_size = 14336,\n        num_hidden_layers = 32,\n        num_attention_heads\
  \ = 32,\n        num_key_value_heads = 8,\n        hidden_act = 'silu',\n      \
  \  initializer_range = 0.02,\n        rms_norm_eps = 1e-05,\n        use_cache =\
  \ True,\n        num_logits_to_keep = 1,\n        pad_token_id = 0,\n        bos_token_id\
  \ = 1,\n        eos_token_id = 2,\n        max_position_embeddings = 262144,\n \
  \       attention_dropout = 0.0,\n        attn_layer_indices = None,\n        mamba_n_heads\
  \ = 128,\n        mamba_d_head = 'auto',\n        mamba_n_groups = 1,\n        mamba_d_state\
  \ = 256,\n        mamba_d_conv = 4,\n        mamba_expand = 2,\n        mamba_chunk_size\
  \ = 256,\n        mamba_conv_bias = True,\n        mamba_proj_bias = False,\n  \
  \      z_loss_coefficient = 0.0,\n        **kwargs\n    ):\n        \"\"\"\n   \
  \     Initialize a BambaConfig instance for configuring a Bamba model.\n\n     \
  \   This constructor sets up the configuration parameters for a Bamba model, which\
  \ is a hybrid\n        mamba2 architecture with SwiGLU. The configuration defines\
  \ the model architecture including\n        vocabulary size, hidden dimensions,\
  \ attention mechanisms, and mamba-specific parameters.\n\n        Parameters:\n\
  \            vocab_size (int, optional): Vocabulary size of the Bamba model. Defines\
  \ the number of \n                different tokens that can be represented by the\
  \ inputs_ids. Defaults to 128000.\n            tie_word_embeddings (bool, optional):\
  \ Whether the model's input and output word \n                embeddings should\
  \ be tied. Only relevant if the model has an output word embedding \n          \
  \      layer. Defaults to False.\n            hidden_size (int, optional): Dimension\
  \ of the hidden representations. Defaults to 4096.\n            intermediate_size\
  \ (int, optional): Dimension of the MLP representations. Defaults to 14336.\n  \
  \          num_hidden_layers (int, optional): Number of hidden layers in the Transformer\
  \ encoder. \n                Defaults to 32.\n            num_attention_heads (int,\
  \ optional): Number of attention heads for each attention layer \n             \
  \   in the Transformer encoder. Defaults to 32.\n            num_key_value_heads\
  \ (int, optional): Number of key_value heads for Grouped Query Attention.\n    \
  \            If equal to num_attention_heads, uses Multi Head Attention (MHA). If\
  \ 1, uses Multi \n                Query Attention (MQA). Otherwise uses GQA. Defaults\
  \ to 8.\n            hidden_act (str or function, optional): The non-linear activation\
  \ function in the decoder. \n                Defaults to 'silu'.\n            initializer_range\
  \ (float, optional): Standard deviation of the truncated_normal_initializer \n \
  \               for initializing all weight matrices. Defaults to 0.02.\n      \
  \      rms_norm_eps (float, optional): Epsilon used by the RMS normalization layers.\
  \ \n                Defaults to 1e-05.\n            use_cache (bool, optional):\
  \ Whether the model should return the last key/values attentions.\n            \
  \    Only relevant if config.is_decoder=True. Defaults to True.\n            num_logits_to_keep\
  \ (int or None, optional): Number of prompt logits to calculate during \n      \
  \          generation. If None, all logits calculated. If int, only last num_logits_to_keep\
  \ \n                logits calculated. Defaults to 1.\n            pad_token_id\
  \ (int, optional): ID of the padding token. Defaults to 0.\n            bos_token_id\
  \ (int, optional): ID of the \"beginning-of-sequence\" token. Defaults to 1.\n \
  \           eos_token_id (int, optional): ID of the \"end-of-sequence\" token. Defaults\
  \ to 2.\n            max_position_embeddings (int, optional): Maximum cached sequence\
  \ length for the model. \n                Defaults to 262144.\n            attention_dropout\
  \ (float, optional): Dropout ratio for attention probabilities. \n             \
  \   Defaults to 0.0.\n            attn_layer_indices (list, optional): Layer indices\
  \ that will have full attention. \n                Values must be at most num_hidden_layers.\
  \ Defaults to None.\n            mamba_n_heads (int, optional): Number of mamba\
  \ heads used in the v2 implementation. \n                Defaults to 128.\n    \
  \        mamba_d_head (int or str, optional): Head embedding dimension size. If\
  \ 'auto', \n                calculated as mamba_intermediate // mamba_n_heads. Defaults\
  \ to 'auto'.\n            mamba_n_groups (int, optional): Number of mamba groups\
  \ used in the v2 implementation. \n                Defaults to 1.\n            mamba_d_state\
  \ (int, optional): Dimension of the mamba state space latents. Defaults to 256.\n\
  \            mamba_d_conv (int, optional): Size of the mamba convolution kernel.\
  \ Defaults to 4.\n            mamba_expand (int, optional): Expanding factor (relative\
  \ to hidden_size) used to \n                determine the mamba intermediate size.\
  \ Defaults to 2.\n            mamba_chunk_size (int, optional): Chunks to break\
  \ the sequence when doing prefill/training. \n                Defaults to 256.\n\
  \            mamba_conv_bias (bool, optional): Whether to use bias in the convolution\
  \ layer of the \n                mamba mixer block. Defaults to True.\n        \
  \    mamba_proj_bias (bool, optional): Whether to use bias in the input and output\
  \ projections \n                of the mamba mixer block. Defaults to False.\n \
  \           z_loss_coefficient (float, optional): Coefficient for auxiliary z-loss\
  \ used to control \n                logit growth during training. Defaults to 0.0.\n\
  \            **kwargs: Additional keyword arguments passed to the parent PretrainedConfig\
  \ class.\n\n        Raises:\n            ValueError: If mamba_n_heads does not divide\
  \ (mamba_expand * hidden_size).\n            ValueError: If mamba_d_head * mamba_n_heads\
  \ does not equal mamba_intermediate.\n\n        Notes:\n            - The configuration\
  \ defaults are taken from ibm-fms/Bamba-9.8b-2.2T-hf model.\n            - For backward\
  \ compatibility, if num_key_value_heads is None, it defaults to num_attention_heads.\n\
  \            - When mamba_d_head is 'auto', it is automatically calculated as mamba_intermediate\
  \ // mamba_n_heads.\n            - The model uses RoPE (Rotary Position Embedding)\
  \ with theta=10000.0 and partial_rotary_factor=0.5.\n        \"\"\"\n        <your\
  \ code>\n..."
