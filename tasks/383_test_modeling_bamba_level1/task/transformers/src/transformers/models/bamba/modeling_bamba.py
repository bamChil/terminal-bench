#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/bamba/modular_bamba.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_bamba.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, Callable, Optional, TypedDict, Union

import torch
from torch import nn

from transformers.activations import ACT2FN

from ...cache_utils import Cache
from ...generation import GenerationMixin
from ...integrations import use_kernel_forward_from_hub
from ...modeling_attn_mask_utils import AttentionMaskConverter
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging
from ...utils.deprecation import deprecate_kwarg
from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available
from .configuration_bamba import BambaConfig


if is_mamba_2_ssm_available():
    from mamba_ssm.ops.triton.selective_state_update import selective_state_update
    from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined
else:
    selective_state_update = None

if is_causal_conv1d_available():
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
else:
    causal_conv1d_update, causal_conv1d_fn = None, None


logger = logging.get_logger(__name__)




class HybridMambaAttentionDynamicCache:
    """

        A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache
        (which has a constant shape regardless of seq_len).

        This cache has two sets of lists of tensors: `key_cache` and `value_cache` for attention cache and `conv_states`
        and `ssm_states` for mamba cache. Each of these lists has `num_layers` tensors. The expected shape for each tensor
        For attention layers, `key_cache` and `value_cache` have a shape of `(batch_size, num_heads, seq_len, head_dim)`,
        while `conv_states` and `ssm_states` have a shape of `(batch_size, 0)` (empty tensors).
        For mamba layers, `key_cache` and `value_cache` have a shape of `(batch_size, 0)` (empty tensors),
        while `conv_states` represents the convolution state and has a shape of `(batch_size, d_inner, d_conv)`,
        and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.

    """

    is_compileable = False

    def __init__(
            self,
            config: BambaConfig,
            batch_size,
            dtype = torch.float16,
            device = None
        ):
        raise NotImplementedError('This function has been masked for testing')

    def update(
            self,
            key_states: torch.Tensor,
            value_states: torch.Tensor,
            layer_idx: int,
            cache_kwargs: Optional[dict[str, Any]] = None
        ) -> tuple[torch.Tensor, torch.Tensor]:
        raise NotImplementedError('This function has been masked for testing')

    def reorder_cache(
            self,
            beam_idx: torch.LongTensor
        ):
        """
        Reorders the cache for beam search, given the selected beam indices.
        """
        raise NotImplementedError('This function has been masked for testing')

    def get_seq_length(
            self,
            layer_idx: Optional[int] = 0
        ) -> int:
        """
        Returns the sequence length of the cached states. A layer index can be optionally passed.
        """
        raise NotImplementedError('This function has been masked for testing')










# Adapted from transformers.models.glm.modular_glm.apply_rotary_pos_emb






# Helper methods for segment sum computation








is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))




# Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer










@auto_docstring
class BambaModel(BambaPreTrainedModel):

    def __init__(self, config: BambaConfig):
        raise NotImplementedError('This function has been masked for testing')

    @can_return_tuple
    @auto_docstring
    def forward(
            self,
            input_ids: Optional[torch.LongTensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,
            inputs_embeds: Optional[torch.FloatTensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            cache_position: Optional[torch.LongTensor] = None,
            **kwargs: Unpack[BambaFlashAttentionKwargs]
        ) -> BaseModelOutputWithPast:
        raise NotImplementedError('This function has been masked for testing')

    def _update_causal_mask(
            self,
            attention_mask: torch.Tensor,
            input_tensor: torch.Tensor,
            cache_position: torch.Tensor,
            past_key_values: HybridMambaAttentionDynamicCache,
            output_attentions: bool
        ):
        raise NotImplementedError('This function has been masked for testing')

    @staticmethod
    def _prepare_4d_causal_attention_mask_with_cache_position(
            attention_mask: torch.Tensor,
            sequence_length: int,
            target_length: int,
            dtype: torch.dtype,
            cache_position: torch.Tensor,
            batch_size: int,
            **kwargs
        ):
        """

                Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
                `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

                Args:
                    attention_mask (`torch.Tensor`):
                        A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape
                        `(batch_size, 1, query_length, key_value_length)`.
                    sequence_length (`int`):
                        The sequence length being processed.
                    target_length (`int`):
                        The target length: when generating with static cache, the mask should be as long as the static cache,
                        to account for the 0 padding, the part of the cache that is not filled yet.
                    dtype (`torch.dtype`):
                        The dtype to use for the 4D attention mask.
                    cache_position (`torch.Tensor`):
                        Indices depicting the position of the input sequence tokens in the sequence.
                    batch_size (`torch.Tensor`):
                        Batch size.

        """
        raise NotImplementedError('This function has been masked for testing')

    def _update_mamba_mask(self, attention_mask, cache_position):
        """

                No need for zeroing states when
                    1. Cached forward
                    2. Attending to all inputs

        """
        raise NotImplementedError('This function has been masked for testing')


@auto_docstring
class BambaForCausalLM(BambaPreTrainedModel, GenerationMixin):

    _tied_weights_keys = ['lm_head.weight']
    _tp_plan = {'lm_head': 'colwise_rep'}
    _pp_plan = {'lm_head': (['hidden_states'], ['logits'])}

    def __init__(self, config):
        raise NotImplementedError('This function has been masked for testing')

    @can_return_tuple
    @auto_docstring
    def forward(
            self,
            input_ids: Optional[torch.LongTensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,
            inputs_embeds: Optional[torch.FloatTensor] = None,
            labels: Optional[torch.LongTensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            cache_position: Optional[torch.LongTensor] = None,
            logits_to_keep: Union[int, torch.Tensor] = 0,
            **kwargs
        ) -> CausalLMOutputWithPast:
        """

                labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                    Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                    config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                    (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

                Example:

                ```python
                >>> from transformers import AutoTokenizer, BambaForCausalLM

                >>> model = BambaForCausalLM.from_pretrained("...")
                >>> tokenizer = AutoTokenizer.from_pretrained("...")

                >>> prompt = "Hey, are you conscious? Can you talk to me?"
                >>> inputs = tokenizer(prompt, return_tensors="pt")

                >>> # Generate
                >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
                >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
                "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
                ```
        """
        raise NotImplementedError('This function has been masked for testing')

    def prepare_inputs_for_generation(
            self,
            input_ids,
            past_key_values = None,
            attention_mask = None,
            inputs_embeds = None,
            cache_position = None,
            position_ids = None,
            use_cache = True,
            **kwargs
        ):
        raise NotImplementedError('This function has been masked for testing')


__all__ = ["BambaModel", "BambaForCausalLM", "BambaPreTrainedModel"]